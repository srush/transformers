

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>AutoModels &mdash; transformers 2.6.0 documentation</title>
  

  
  
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/huggingface.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/code-snippets.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="BERT" href="bert.html" />
    <link rel="prev" title="Installation" href="../installation.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> transformers
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pretrained_models.html">Pretrained models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../usage.html">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_sharing.html">Model Sharing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks.html">Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../serialization.html">Loading Google AI or OpenAI pre-trained weights or PyTorch dump</a></li>
<li class="toctree-l1"><a class="reference internal" href="../serialization.html#serialization-best-practices">Serialization best-practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../converting_tensorflow_models.html">Converting Tensorflow Checkpoints</a></li>
<li class="toctree-l1"><a class="reference internal" href="../torchscript.html">Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../multilingual.html">Multi-lingual models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../benchmarks.html">Benchmarks</a></li>
</ul>
<p class="caption"><span class="caption-text">Core Classes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/configuration.html">Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/model.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/tokenizer.html">Tokenizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/pipelines.html">Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/optimizer_schedules.html">Optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/optimizer_schedules.html#schedules">Schedules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/optimizer_schedules.html#gradient-strategies">Gradient Strategies</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/processors.html">Processors</a></li>
</ul>
<p class="caption"><span class="caption-text">Features</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../migration.html">Migrating from pytorch-pretrained-bert</a></li>
<li class="toctree-l1"><a class="reference internal" href="../glossary.html">Glossary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../bertology.html">BERTology</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
</ul>
<p class="caption"><span class="caption-text">Models</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">AutoModels</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#autoconfig"><code class="docutils literal notranslate"><span class="pre">AutoConfig</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#autotokenizer"><code class="docutils literal notranslate"><span class="pre">AutoTokenizer</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#automodel"><code class="docutils literal notranslate"><span class="pre">AutoModel</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#automodelforpretraining"><code class="docutils literal notranslate"><span class="pre">AutoModelForPreTraining</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#automodelwithlmhead"><code class="docutils literal notranslate"><span class="pre">AutoModelWithLMHead</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#automodelforsequenceclassification"><code class="docutils literal notranslate"><span class="pre">AutoModelForSequenceClassification</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#automodelforquestionanswering"><code class="docutils literal notranslate"><span class="pre">AutoModelForQuestionAnswering</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#automodelfortokenclassification"><code class="docutils literal notranslate"><span class="pre">AutoModelForTokenClassification</span></code></a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="bert.html">BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpt.html">OpenAI GPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="transformerxl.html">Transformer XL</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpt2.html">OpenAI GPT2</a></li>
<li class="toctree-l1"><a class="reference internal" href="xlm.html">XLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="xlnet.html">XLNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="roberta.html">RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="distilbert.html">DistilBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="ctrl.html">CTRL</a></li>
<li class="toctree-l1"><a class="reference internal" href="camembert.html">CamemBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="albert.html">ALBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="xlmroberta.html">XLM-RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="flaubert.html">FlauBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="bart.html">Bart</a></li>
<li class="toctree-l1"><a class="reference internal" href="t5.html">T5</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">transformers</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>AutoModels</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/model_doc/auto.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="automodels">
<h1>AutoModels<a class="headerlink" href="#automodels" title="Permalink to this headline">Â¶</a></h1>
<p>In many cases, the architecture you want to use can be guessed from the name or the path of the pretrained model you are supplying to the <code class="docutils literal notranslate"><span class="pre">from_pretrained</span></code> method.</p>
<p>AutoClasses are here to do this job for you so that you automatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary:</p>
<p>Instantiating one of <code class="docutils literal notranslate"><span class="pre">AutoModel</span></code>, <code class="docutils literal notranslate"><span class="pre">AutoConfig</span></code> and <code class="docutils literal notranslate"><span class="pre">AutoTokenizer</span></code> will directly create a class of the relevant architecture (ex: <code class="docutils literal notranslate"><span class="pre">model</span> <span class="pre">=</span> <span class="pre">AutoModel.from_pretrained('bert-base-cased')</span></code> will create a instance of <code class="docutils literal notranslate"><span class="pre">BertModel</span></code>).</p>
<div class="section" id="autoconfig">
<h2><code class="docutils literal notranslate"><span class="pre">AutoConfig</span></code><a class="headerlink" href="#autoconfig" title="Permalink to this headline">Â¶</a></h2>
<dl class="class">
<dt id="transformers.AutoConfig">
<em class="property">class </em><code class="descclassname">transformers.</code><code class="descname">AutoConfig</code><a class="reference internal" href="../_modules/transformers/configuration_auto.html#AutoConfig"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoConfig" title="Permalink to this definition">Â¶</a></dt>
<dd><p><a class="reference internal" href="#transformers.AutoConfig" title="transformers.AutoConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">AutoConfig</span></code></a> is a generic configuration class
that will be instantiated as one of the configuration classes of the library
when created with the <a class="reference internal" href="#transformers.AutoConfig.from_pretrained" title="transformers.AutoConfig.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> class method.</p>
<p>The <a class="reference internal" href="#transformers.AutoConfig.from_pretrained" title="transformers.AutoConfig.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> method takes care of returning the correct model class instance
based on the <cite>model_type</cite> property of the config object, or when itâs missing,
falling back to using pattern matching on the <cite>pretrained_model_name_or_path</cite> string.</p>
<dl class="classmethod">
<dt id="transformers.AutoConfig.from_pretrained">
<em class="property">classmethod </em><code class="descname">from_pretrained</code><span class="sig-paren">(</span><em>pretrained_model_name_or_path</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/configuration_auto.html#AutoConfig.from_pretrained"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoConfig.from_pretrained" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiates one of the configuration classes of the library
from a pre-trained model configuration.</p>
<p>The configuration class to instantiate is selected
based on the <cite>model_type</cite> property of the config object, or when itâs missing,
falling back to using pattern matching on the <cite>pretrained_model_name_or_path</cite> string.</p>
<blockquote>
<div><ul class="simple">
<li>contains <cite>t5</cite>: <a class="reference internal" href="t5.html#transformers.T5Config" title="transformers.T5Config"><code class="xref py py-class docutils literal notranslate"><span class="pre">T5Config</span></code></a> (T5 model)</li>
<li>contains <cite>distilbert</cite>: <a class="reference internal" href="distilbert.html#transformers.DistilBertConfig" title="transformers.DistilBertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertConfig</span></code></a> (DistilBERT model)</li>
<li>contains <cite>albert</cite>: <a class="reference internal" href="albert.html#transformers.AlbertConfig" title="transformers.AlbertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">AlbertConfig</span></code></a> (ALBERT model)</li>
<li>contains <cite>camembert</cite>: <a class="reference internal" href="camembert.html#transformers.CamembertConfig" title="transformers.CamembertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">CamembertConfig</span></code></a> (CamemBERT model)</li>
<li>contains <cite>xlm-roberta</cite>: <a class="reference internal" href="xlmroberta.html#transformers.XLMRobertaConfig" title="transformers.XLMRobertaConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMRobertaConfig</span></code></a> (XLM-RoBERTa model)</li>
<li>contains <cite>roberta</cite>: <a class="reference internal" href="roberta.html#transformers.RobertaConfig" title="transformers.RobertaConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaConfig</span></code></a> (RoBERTa model)</li>
<li>contains <cite>bert</cite>: <a class="reference internal" href="bert.html#transformers.BertConfig" title="transformers.BertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertConfig</span></code></a> (Bert model)</li>
<li>contains <cite>openai-gpt</cite>: <a class="reference internal" href="gpt.html#transformers.OpenAIGPTConfig" title="transformers.OpenAIGPTConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">OpenAIGPTConfig</span></code></a> (OpenAI GPT model)</li>
<li>contains <cite>gpt2</cite>: <a class="reference internal" href="gpt2.html#transformers.GPT2Config" title="transformers.GPT2Config"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPT2Config</span></code></a> (OpenAI GPT-2 model)</li>
<li>contains <cite>transfo-xl</cite>: <a class="reference internal" href="transformerxl.html#transformers.TransfoXLConfig" title="transformers.TransfoXLConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">TransfoXLConfig</span></code></a> (Transformer-XL model)</li>
<li>contains <cite>xlnet</cite>: <a class="reference internal" href="xlnet.html#transformers.XLNetConfig" title="transformers.XLNetConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetConfig</span></code></a> (XLNet model)</li>
<li>contains <cite>xlm</cite>: <a class="reference internal" href="xlm.html#transformers.XLMConfig" title="transformers.XLMConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMConfig</span></code></a> (XLM model)</li>
<li>contains <cite>ctrl</cite> : <a class="reference internal" href="ctrl.html#transformers.CTRLConfig" title="transformers.CTRLConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">CTRLConfig</span></code></a> (CTRL model)</li>
<li>contains <cite>flaubert</cite> : <a class="reference internal" href="flaubert.html#transformers.FlaubertConfig" title="transformers.FlaubertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">FlaubertConfig</span></code></a> (Flaubert model)</li>
</ul>
</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>pretrained_model_name_or_path</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">string</span></code>) â <dl class="docutils">
<dt>Is either: </dt>
<dd><ul class="first last">
<li>a string with the <cite>shortcut name</cite> of a pre-trained model configuration to load from cache or download, e.g.: <code class="docutils literal notranslate"><span class="pre">bert-base-uncased</span></code>.</li>
<li>a string with the <cite>identifier name</cite> of a pre-trained model configuration that was user-uploaded to our S3, e.g.: <code class="docutils literal notranslate"><span class="pre">dbmdz/bert-base-german-cased</span></code>.</li>
<li>a path to a <cite>directory</cite> containing a configuration file saved using the <a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig.save_pretrained" title="transformers.PretrainedConfig.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> method, e.g.: <code class="docutils literal notranslate"><span class="pre">./my_model_directory/</span></code>.</li>
<li>a path or url to a saved configuration JSON <cite>file</cite>, e.g.: <code class="docutils literal notranslate"><span class="pre">./my_model_directory/configuration.json</span></code>.</li>
</ul>
</dd>
</dl>
</li>
<li><strong>cache_dir</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">string</span></code>, optional, defaults to <cite>None</cite>) â Path to a directory in which a downloaded pre-trained model
configuration should be cached if the standard cache should not be used.</li>
<li><strong>force_download</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">boolean</span></code>, optional, defaults to <cite>False</cite>) â Force to (re-)download the model weights and configuration files and override the cached versions if they exist.</li>
<li><strong>resume_download</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">boolean</span></code>, optional, defaults to <cite>False</cite>) â Do not delete incompletely received file. Attempt to resume the download if such a file exists.</li>
<li><strong>proxies</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">str]</span></code>, optional, defaults to <cite>None</cite>) â A dictionary of proxy servers to use by protocol or endpoint, e.g.: <code class="xref py py-obj docutils literal notranslate"><span class="pre">{'http':</span> <span class="pre">'foo.bar:3128',</span> <span class="pre">'http://hostname':</span> <span class="pre">'foo.bar:4012'}</span></code>.
The proxies are used on each request. See <a class="reference external" href="https://requests.readthedocs.io/en/master/user/advanced/#proxies">the requests documentation</a> for usage.</li>
<li><strong>return_unused_kwargs</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">boolean</span></code>, optional, defaults to <cite>False</cite>) â <ul>
<li>If False, then this function returns just the final configuration object.</li>
<li>If True, then this functions returns a tuple <cite>(config, unused_kwargs)</cite> where <cite>unused_kwargs</cite> is a dictionary consisting of the key/value pairs whose keys are not configuration attributes: ie the part of kwargs which has not been used to update <cite>config</cite> and is otherwise ignored.</li>
</ul>
</li>
<li><strong>kwargs</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">any]</span></code>, optional, defaults to <cite>{}</cite>) â key/value pairs with which to update the configuration object after loading.
- The values in kwargs of any keys which are configuration attributes will be used to override the loaded values.
- Behavior concerning key/value pairs whose keys are <em>not</em> configuration attributes is controlled by the <cite>return_unused_kwargs</cite> keyword parameter.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>  <span class="c1"># Download configuration from S3 and cache.</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;./test/bert_saved_model/&#39;</span><span class="p">)</span>  <span class="c1"># E.g. config (or model) was saved using `save_pretrained(&#39;./test/saved_model/&#39;)`</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;./test/bert_saved_model/my_configuration.json&#39;</span><span class="p">)</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">,</span> <span class="n">output_attention</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">foo</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">config</span><span class="o">.</span><span class="n">output_attention</span> <span class="o">==</span> <span class="kc">True</span>
<span class="n">config</span><span class="p">,</span> <span class="n">unused_kwargs</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">,</span> <span class="n">output_attention</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                                   <span class="n">foo</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_unused_kwargs</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">config</span><span class="o">.</span><span class="n">output_attention</span> <span class="o">==</span> <span class="kc">True</span>
<span class="k">assert</span> <span class="n">unused_kwargs</span> <span class="o">==</span> <span class="p">{</span><span class="s1">&#39;foo&#39;</span><span class="p">:</span> <span class="kc">False</span><span class="p">}</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="autotokenizer">
<h2><code class="docutils literal notranslate"><span class="pre">AutoTokenizer</span></code><a class="headerlink" href="#autotokenizer" title="Permalink to this headline">Â¶</a></h2>
<dl class="class">
<dt id="transformers.AutoTokenizer">
<em class="property">class </em><code class="descclassname">transformers.</code><code class="descname">AutoTokenizer</code><a class="reference internal" href="../_modules/transformers/tokenization_auto.html#AutoTokenizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoTokenizer" title="Permalink to this definition">Â¶</a></dt>
<dd><p><a class="reference internal" href="#transformers.AutoTokenizer" title="transformers.AutoTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">AutoTokenizer</span></code></a> is a generic tokenizer class
that will be instantiated as one of the tokenizer classes of the library
when created with the <cite>AutoTokenizer.from_pretrained(pretrained_model_name_or_path)</cite>
class method.</p>
<p>The <cite>from_pretrained()</cite> method take care of returning the correct tokenizer class instance
based on the <cite>model_type</cite> property of the config object, or when itâs missing,
falling back to using pattern matching on the <cite>pretrained_model_name_or_path</cite> string.</p>
<p>The tokenizer class to instantiate is selected as the first pattern matching
in the <cite>pretrained_model_name_or_path</cite> string (in the following order):</p>
<blockquote>
<div><ul class="simple">
<li>contains <cite>t5</cite>: T5Tokenizer (T5 model)</li>
<li>contains <cite>distilbert</cite>: DistilBertTokenizer (DistilBert model)</li>
<li>contains <cite>albert</cite>: AlbertTokenizer (ALBERT model)</li>
<li>contains <cite>camembert</cite>: CamembertTokenizer (CamemBERT model)</li>
<li>contains <cite>xlm-roberta</cite>: XLMRobertaTokenizer (XLM-RoBERTa model)</li>
<li>contains <cite>roberta</cite>: RobertaTokenizer (RoBERTa model)</li>
<li>contains <cite>bert</cite>: BertTokenizer (Bert model)</li>
<li>contains <cite>openai-gpt</cite>: OpenAIGPTTokenizer (OpenAI GPT model)</li>
<li>contains <cite>gpt2</cite>: GPT2Tokenizer (OpenAI GPT-2 model)</li>
<li>contains <cite>transfo-xl</cite>: TransfoXLTokenizer (Transformer-XL model)</li>
<li>contains <cite>xlnet</cite>: XLNetTokenizer (XLNet model)</li>
<li>contains <cite>xlm</cite>: XLMTokenizer (XLM model)</li>
<li>contains <cite>ctrl</cite>: CTRLTokenizer (Salesforce CTRL model)</li>
</ul>
</div></blockquote>
<p>This class cannot be instantiated using <cite>__init__()</cite> (throw an error).</p>
<dl class="classmethod">
<dt id="transformers.AutoTokenizer.from_pretrained">
<em class="property">classmethod </em><code class="descname">from_pretrained</code><span class="sig-paren">(</span><em>pretrained_model_name_or_path</em>, <em>*inputs</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/tokenization_auto.html#AutoTokenizer.from_pretrained"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoTokenizer.from_pretrained" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiate one of the tokenizer classes of the library
from a pre-trained model vocabulary.</p>
<p>The tokenizer class to instantiate is selected as the first pattern matching
in the <cite>pretrained_model_name_or_path</cite> string (in the following order):</p>
<blockquote>
<div><ul class="simple">
<li>contains <cite>t5</cite>: T5Tokenizer (T5 model)</li>
<li>contains <cite>distilbert</cite>: DistilBertTokenizer (DistilBert model)</li>
<li>contains <cite>albert</cite>: AlbertTokenizer (ALBERT model)</li>
<li>contains <cite>camembert</cite>: CamembertTokenizer (CamemBERT model)</li>
<li>contains <cite>xlm-roberta</cite>: XLMRobertaTokenizer (XLM-RoBERTa model)</li>
<li>contains <cite>roberta</cite>: RobertaTokenizer (RoBERTa model)</li>
<li>contains <cite>bert-base-japanese</cite>: BertJapaneseTokenizer (Bert model)</li>
<li>contains <cite>bert</cite>: BertTokenizer (Bert model)</li>
<li>contains <cite>openai-gpt</cite>: OpenAIGPTTokenizer (OpenAI GPT model)</li>
<li>contains <cite>gpt2</cite>: GPT2Tokenizer (OpenAI GPT-2 model)</li>
<li>contains <cite>transfo-xl</cite>: TransfoXLTokenizer (Transformer-XL model)</li>
<li>contains <cite>xlnet</cite>: XLNetTokenizer (XLNet model)</li>
<li>contains <cite>xlm</cite>: XLMTokenizer (XLM model)</li>
<li>contains <cite>ctrl</cite>: CTRLTokenizer (Salesforce CTRL model)</li>
</ul>
</div></blockquote>
<dl class="docutils">
<dt>Params:</dt>
<dd><p class="first">pretrained_model_name_or_path: either:</p>
<blockquote>
<div><ul class="simple">
<li>a string with the <cite>shortcut name</cite> of a predefined tokenizer to load from cache or download, e.g.: <code class="docutils literal notranslate"><span class="pre">bert-base-uncased</span></code>.</li>
<li>a string with the <cite>identifier name</cite> of a predefined tokenizer that was user-uploaded to our S3, e.g.: <code class="docutils literal notranslate"><span class="pre">dbmdz/bert-base-german-cased</span></code>.</li>
<li>a path to a <cite>directory</cite> containing vocabulary files required by the tokenizer, for instance saved using the <a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.save_pretrained" title="transformers.PreTrainedTokenizer.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> method, e.g.: <code class="docutils literal notranslate"><span class="pre">./my_model_directory/</span></code>.</li>
<li>(not applicable to all derived classes) a path or url to a single saved vocabulary file if and only if the tokenizer only requires a single vocabulary file (e.g. Bert, XLNet), e.g.: <code class="docutils literal notranslate"><span class="pre">./my_model_directory/vocab.txt</span></code>.</li>
</ul>
</div></blockquote>
<dl class="docutils">
<dt>cache_dir: (<cite>optional</cite>) string:</dt>
<dd>Path to a directory in which a downloaded predefined tokenizer vocabulary files should be cached if the standard cache should not be used.</dd>
<dt>force_download: (<cite>optional</cite>) boolean, default False:</dt>
<dd>Force to (re-)download the vocabulary files and override the cached versions if they exists.</dd>
<dt>resume_download: (<cite>optional</cite>) boolean, default False:</dt>
<dd>Do not delete incompletely recieved file. Attempt to resume the download if such a file exists.</dd>
<dt>proxies: (<cite>optional</cite>) dict, default None:</dt>
<dd>A dictionary of proxy servers to use by protocol or endpoint, e.g.: {âhttpâ: âfoo.bar:3128â, âhttp://hostnameâ: âfoo.bar:4012â}.
The proxies are used on each request.</dd>
<dt>use_fast: (<cite>optional</cite>) boolean, default False:</dt>
<dd>Indicate if transformers should try to load the fast version of the tokenizer (True) or use the Python one (False).</dd>
</dl>
<p>inputs: (<cite>optional</cite>) positional arguments: will be passed to the Tokenizer <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method.</p>
<p class="last">kwargs: (<cite>optional</cite>) keyword arguments: will be passed to the Tokenizer <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method. Can be used to set special tokens like <code class="docutils literal notranslate"><span class="pre">bos_token</span></code>, <code class="docutils literal notranslate"><span class="pre">eos_token</span></code>, <code class="docutils literal notranslate"><span class="pre">unk_token</span></code>, <code class="docutils literal notranslate"><span class="pre">sep_token</span></code>, <code class="docutils literal notranslate"><span class="pre">pad_token</span></code>, <code class="docutils literal notranslate"><span class="pre">cls_token</span></code>, <code class="docutils literal notranslate"><span class="pre">mask_token</span></code>, <code class="docutils literal notranslate"><span class="pre">additional_special_tokens</span></code>. See parameters in the doc string of <a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer" title="transformers.PreTrainedTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizer</span></code></a> for details.</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Download vocabulary from S3 and cache.</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>

<span class="c1"># Download vocabulary from S3 (user-uploaded) and cache.</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;dbmdz/bert-base-german-cased&#39;</span><span class="p">)</span>

<span class="c1"># If vocabulary files are in a directory (e.g. tokenizer was saved using `save_pretrained(&#39;./test/saved_model/&#39;)`)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;./test/bert_saved_model/&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="automodel">
<h2><code class="docutils literal notranslate"><span class="pre">AutoModel</span></code><a class="headerlink" href="#automodel" title="Permalink to this headline">Â¶</a></h2>
<dl class="class">
<dt id="transformers.AutoModel">
<em class="property">class </em><code class="descclassname">transformers.</code><code class="descname">AutoModel</code><a class="reference internal" href="../_modules/transformers/modeling_auto.html#AutoModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModel" title="Permalink to this definition">Â¶</a></dt>
<dd><p><a class="reference internal" href="#transformers.AutoModel" title="transformers.AutoModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">AutoModel</span></code></a> is a generic model class
that will be instantiated as one of the base model classes of the library
when created with the <cite>AutoModel.from_pretrained(pretrained_model_name_or_path)</cite>
or the <cite>AutoModel.from_config(config)</cite> class methods.</p>
<p>This class cannot be instantiated using <cite>__init__()</cite> (throws an error).</p>
<dl class="classmethod">
<dt id="transformers.AutoModel.from_config">
<em class="property">classmethod </em><code class="descname">from_config</code><span class="sig-paren">(</span><em>config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_auto.html#AutoModel.from_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModel.from_config" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiates one of the base model classes of the library
from a configuration.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>config</strong> (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>) â <p>The model class to instantiate is selected based on the configuration class:</p>
<ul class="simple">
<li>isInstance of <cite>distilbert</cite> configuration class: <a class="reference internal" href="distilbert.html#transformers.DistilBertModel" title="transformers.DistilBertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertModel</span></code></a> (DistilBERT model)</li>
<li>isInstance of <cite>roberta</cite> configuration class: <a class="reference internal" href="roberta.html#transformers.RobertaModel" title="transformers.RobertaModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaModel</span></code></a> (RoBERTa model)</li>
<li>isInstance of <cite>bert</cite> configuration class: <a class="reference internal" href="bert.html#transformers.BertModel" title="transformers.BertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertModel</span></code></a> (Bert model)</li>
<li>isInstance of <cite>openai-gpt</cite> configuration class: <a class="reference internal" href="gpt.html#transformers.OpenAIGPTModel" title="transformers.OpenAIGPTModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">OpenAIGPTModel</span></code></a> (OpenAI GPT model)</li>
<li>isInstance of <cite>gpt2</cite> configuration class: <a class="reference internal" href="gpt2.html#transformers.GPT2Model" title="transformers.GPT2Model"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPT2Model</span></code></a> (OpenAI GPT-2 model)</li>
<li>isInstance of <cite>ctrl</cite> configuration class: <a class="reference internal" href="ctrl.html#transformers.CTRLModel" title="transformers.CTRLModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">CTRLModel</span></code></a> (Salesforce CTRL  model)</li>
<li>isInstance of <cite>transfo-xl</cite> configuration class: <a class="reference internal" href="transformerxl.html#transformers.TransfoXLModel" title="transformers.TransfoXLModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TransfoXLModel</span></code></a> (Transformer-XL model)</li>
<li>isInstance of <cite>xlnet</cite> configuration class: <a class="reference internal" href="xlnet.html#transformers.XLNetModel" title="transformers.XLNetModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetModel</span></code></a> (XLNet model)</li>
<li>isInstance of <cite>xlm</cite> configuration class: <a class="reference internal" href="xlm.html#transformers.XLMModel" title="transformers.XLMModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMModel</span></code></a> (XLM model)</li>
<li>isInstance of <cite>flaubert</cite> configuration class: <a class="reference internal" href="flaubert.html#transformers.FlaubertModel" title="transformers.FlaubertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">FlaubertModel</span></code></a> (XLM model)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">config</span> <span class="o">=</span> <span class="n">BertConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>    <span class="c1"># Download configuration from S3 and cache.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="o">.</span><span class="n">from_config</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>  <span class="c1"># E.g. model was saved using `save_pretrained(&#39;./test/saved_model/&#39;)`</span>
</pre></div>
</div>
</dd></dl>

<dl class="classmethod">
<dt id="transformers.AutoModel.from_pretrained">
<em class="property">classmethod </em><code class="descname">from_pretrained</code><span class="sig-paren">(</span><em>pretrained_model_name_or_path</em>, <em>*model_args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_auto.html#AutoModel.from_pretrained"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModel.from_pretrained" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiates one of the base model classes of the library
from a pre-trained model configuration.</p>
<p>The <cite>from_pretrained()</cite> method takes care of returning the correct model class instance
based on the <cite>model_type</cite> property of the config object, or when itâs missing,
falling back to using pattern matching on the <cite>pretrained_model_name_or_path</cite> string.</p>
<p>The base model class to instantiate is selected as the first pattern matching
in the <cite>pretrained_model_name_or_path</cite> string (in the following order):</p>
<blockquote>
<div><ul class="simple">
<li>contains <cite>t5</cite>: <a class="reference internal" href="t5.html#transformers.T5Model" title="transformers.T5Model"><code class="xref py py-class docutils literal notranslate"><span class="pre">T5Model</span></code></a> (T5 model)</li>
<li>contains <cite>distilbert</cite>: <a class="reference internal" href="distilbert.html#transformers.DistilBertModel" title="transformers.DistilBertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertModel</span></code></a> (DistilBERT model)</li>
<li>contains <cite>albert</cite>: <a class="reference internal" href="albert.html#transformers.AlbertModel" title="transformers.AlbertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">AlbertModel</span></code></a> (ALBERT model)</li>
<li>contains <cite>camembert</cite>: <a class="reference internal" href="camembert.html#transformers.CamembertModel" title="transformers.CamembertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">CamembertModel</span></code></a> (CamemBERT model)</li>
<li>contains <cite>xlm-roberta</cite>: <a class="reference internal" href="xlmroberta.html#transformers.XLMRobertaModel" title="transformers.XLMRobertaModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMRobertaModel</span></code></a> (XLM-RoBERTa model)</li>
<li>contains <cite>roberta</cite>: <a class="reference internal" href="roberta.html#transformers.RobertaModel" title="transformers.RobertaModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaModel</span></code></a> (RoBERTa model)</li>
<li>contains <cite>bert</cite>: <a class="reference internal" href="bert.html#transformers.BertModel" title="transformers.BertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertModel</span></code></a> (Bert model)</li>
<li>contains <cite>openai-gpt</cite>: <a class="reference internal" href="gpt.html#transformers.OpenAIGPTModel" title="transformers.OpenAIGPTModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">OpenAIGPTModel</span></code></a> (OpenAI GPT model)</li>
<li>contains <cite>gpt2</cite>: <a class="reference internal" href="gpt2.html#transformers.GPT2Model" title="transformers.GPT2Model"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPT2Model</span></code></a> (OpenAI GPT-2 model)</li>
<li>contains <cite>transfo-xl</cite>: <a class="reference internal" href="transformerxl.html#transformers.TransfoXLModel" title="transformers.TransfoXLModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TransfoXLModel</span></code></a> (Transformer-XL model)</li>
<li>contains <cite>xlnet</cite>: <a class="reference internal" href="xlnet.html#transformers.XLNetModel" title="transformers.XLNetModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetModel</span></code></a> (XLNet model)</li>
<li>contains <cite>xlm</cite>: <a class="reference internal" href="xlm.html#transformers.XLMModel" title="transformers.XLMModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMModel</span></code></a> (XLM model)</li>
<li>contains <cite>ctrl</cite>: <a class="reference internal" href="ctrl.html#transformers.CTRLModel" title="transformers.CTRLModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">CTRLModel</span></code></a> (Salesforce CTRL  model)</li>
<li>contains <cite>flaubert</cite>: <code class="xref py py-class docutils literal notranslate"><span class="pre">Flaubert</span></code> (Flaubert  model)</li>
</ul>
<p>The model is set in evaluation mode by default using <cite>model.eval()</cite> (Dropout modules are deactivated)
To train the model, you should first set it back in training mode with <cite>model.train()</cite></p>
</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>pretrained_model_name_or_path</strong> â <p>either:</p>
<ul>
<li>a string with the <cite>shortcut name</cite> of a pre-trained model to load from cache or download, e.g.: <code class="docutils literal notranslate"><span class="pre">bert-base-uncased</span></code>.</li>
<li>a string with the <cite>identifier name</cite> of a pre-trained model that was user-uploaded to our S3, e.g.: <code class="docutils literal notranslate"><span class="pre">dbmdz/bert-base-german-cased</span></code>.</li>
<li>a path to a <cite>directory</cite> containing model weights saved using <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a>, e.g.: <code class="docutils literal notranslate"><span class="pre">./my_model_directory/</span></code>.</li>
<li>a path or url to a <cite>tensorflow index checkpoint file</cite> (e.g. <cite>./tf_model/model.ckpt.index</cite>). In this case, <code class="docutils literal notranslate"><span class="pre">from_tf</span></code> should be set to True and a configuration object should be provided as <code class="docutils literal notranslate"><span class="pre">config</span></code> argument. This loading path is slower than converting the TensorFlow checkpoint in a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>
</li>
<li><strong>model_args</strong> â (<cite>optional</cite>) Sequence of positional arguments:
All remaning positional arguments will be passed to the underlying modelâs <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method</li>
<li><strong>config</strong> â <p>(<cite>optional</cite>) instance of a class derived from <a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>:
Configuration for the model to use instead of an automatically loaded configuation. Configuration can be automatically loaded when:</p>
<ul>
<li>the model is a model provided by the library (loaded with the <code class="docutils literal notranslate"><span class="pre">shortcut-name</span></code> string of a pretrained model), or</li>
<li>the model was saved using <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and is reloaded by suppling the save directory.</li>
<li>the model is loaded by suppling a local directory as <code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> and a configuration JSON file named <cite>config.json</cite> is found in the directory.</li>
</ul>
</li>
<li><strong>state_dict</strong> â (<cite>optional</cite>) dict:
an optional state dictionnary for the model to use instead of a state dictionary loaded from saved weights file.
This option can be used if you want to create a model from a pretrained configuration but load your own weights.
In this case though, you should check if using <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> is not a simpler option.</li>
<li><strong>cache_dir</strong> â (<cite>optional</cite>) string:
Path to a directory in which a downloaded pre-trained model
configuration should be cached if the standard cache should not be used.</li>
<li><strong>force_download</strong> â (<cite>optional</cite>) boolean, default False:
Force to (re-)download the model weights and configuration files and override the cached versions if they exists.</li>
<li><strong>resume_download</strong> â (<cite>optional</cite>) boolean, default False:
Do not delete incompletely recieved file. Attempt to resume the download if such a file exists.</li>
<li><strong>proxies</strong> â (<cite>optional</cite>) dict, default None:
A dictionary of proxy servers to use by protocol or endpoint, e.g.: {âhttpâ: âfoo.bar:3128â, âhttp://hostnameâ: âfoo.bar:4012â}.
The proxies are used on each request.</li>
<li><strong>output_loading_info</strong> â (<cite>optional</cite>) boolean:
Set to <code class="docutils literal notranslate"><span class="pre">True</span></code> to also return a dictionnary containing missing keys, unexpected keys and error messages.</li>
<li><strong>kwargs</strong> â (<cite>optional</cite>) Remaining dictionary of keyword arguments:
These arguments will be passed to the configuration and the model.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>    <span class="c1"># Download model and configuration from S3 and cache.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;./test/bert_model/&#39;</span><span class="p">)</span>  <span class="c1"># E.g. model was saved using `save_pretrained(&#39;./test/saved_model/&#39;)`</span>
<span class="k">assert</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attention</span> <span class="o">==</span> <span class="kc">True</span>
<span class="c1"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_json_file</span><span class="p">(</span><span class="s1">&#39;./tf_model/bert_tf_model_config.json&#39;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;./tf_model/bert_tf_checkpoint.ckpt.index&#39;</span><span class="p">,</span> <span class="n">from_tf</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="automodelforpretraining">
<h2><code class="docutils literal notranslate"><span class="pre">AutoModelForPreTraining</span></code><a class="headerlink" href="#automodelforpretraining" title="Permalink to this headline">Â¶</a></h2>
<dl class="class">
<dt id="transformers.AutoModelForPreTraining">
<em class="property">class </em><code class="descclassname">transformers.</code><code class="descname">AutoModelForPreTraining</code><a class="reference internal" href="../_modules/transformers/modeling_auto.html#AutoModelForPreTraining"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModelForPreTraining" title="Permalink to this definition">Â¶</a></dt>
<dd><p><a class="reference internal" href="#transformers.AutoModelForPreTraining" title="transformers.AutoModelForPreTraining"><code class="xref py py-class docutils literal notranslate"><span class="pre">AutoModelForPreTraining</span></code></a> is a generic model class
that will be instantiated as one of the model classes of the library -with the architecture used for pretraining this modelâ when created with the <cite>AutoModelForPreTraining.from_pretrained(pretrained_model_name_or_path)</cite>
class method.</p>
<p>This class cannot be instantiated using <cite>__init__()</cite> (throws an error).</p>
<dl class="classmethod">
<dt id="transformers.AutoModelForPreTraining.from_config">
<em class="property">classmethod </em><code class="descname">from_config</code><span class="sig-paren">(</span><em>config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_auto.html#AutoModelForPreTraining.from_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModelForPreTraining.from_config" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiates one of the base model classes of the library
from a configuration.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>config</strong> (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>) â <p>The model class to instantiate is selected based on the configuration class:</p>
<ul class="simple">
<li>isInstance of <cite>distilbert</cite> configuration class: <a class="reference internal" href="distilbert.html#transformers.DistilBertForMaskedLM" title="transformers.DistilBertForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertForMaskedLM</span></code></a> (DistilBERT model)</li>
<li>isInstance of <cite>roberta</cite> configuration class: <a class="reference internal" href="roberta.html#transformers.RobertaForMaskedLM" title="transformers.RobertaForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaForMaskedLM</span></code></a> (RoBERTa model)</li>
<li>isInstance of <cite>bert</cite> configuration class: <a class="reference internal" href="bert.html#transformers.BertForPreTraining" title="transformers.BertForPreTraining"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertForPreTraining</span></code></a> (Bert model)</li>
<li>isInstance of <cite>openai-gpt</cite> configuration class: <a class="reference internal" href="gpt.html#transformers.OpenAIGPTLMHeadModel" title="transformers.OpenAIGPTLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">OpenAIGPTLMHeadModel</span></code></a> (OpenAI GPT model)</li>
<li>isInstance of <cite>gpt2</cite> configuration class: <a class="reference internal" href="gpt2.html#transformers.GPT2LMHeadModel" title="transformers.GPT2LMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPT2LMHeadModel</span></code></a> (OpenAI GPT-2 model)</li>
<li>isInstance of <cite>ctrl</cite> configuration class: <a class="reference internal" href="ctrl.html#transformers.CTRLLMHeadModel" title="transformers.CTRLLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">CTRLLMHeadModel</span></code></a> (Salesforce CTRL  model)</li>
<li>isInstance of <cite>transfo-xl</cite> configuration class: <a class="reference internal" href="transformerxl.html#transformers.TransfoXLLMHeadModel" title="transformers.TransfoXLLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TransfoXLLMHeadModel</span></code></a> (Transformer-XL model)</li>
<li>isInstance of <cite>xlnet</cite> configuration class: <a class="reference internal" href="xlnet.html#transformers.XLNetLMHeadModel" title="transformers.XLNetLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetLMHeadModel</span></code></a> (XLNet model)</li>
<li>isInstance of <cite>xlm</cite> configuration class: <a class="reference internal" href="xlm.html#transformers.XLMWithLMHeadModel" title="transformers.XLMWithLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMWithLMHeadModel</span></code></a> (XLM model)</li>
<li>isInstance of <cite>flaubert</cite> configuration class: <a class="reference internal" href="flaubert.html#transformers.FlaubertWithLMHeadModel" title="transformers.FlaubertWithLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">FlaubertWithLMHeadModel</span></code></a> (Flaubert model)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">config</span> <span class="o">=</span> <span class="n">BertConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>    <span class="c1"># Download configuration from S3 and cache.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForPreTraining</span><span class="o">.</span><span class="n">from_config</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>  <span class="c1"># E.g. model was saved using `save_pretrained(&#39;./test/saved_model/&#39;)`</span>
</pre></div>
</div>
</dd></dl>

<dl class="classmethod">
<dt id="transformers.AutoModelForPreTraining.from_pretrained">
<em class="property">classmethod </em><code class="descname">from_pretrained</code><span class="sig-paren">(</span><em>pretrained_model_name_or_path</em>, <em>*model_args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_auto.html#AutoModelForPreTraining.from_pretrained"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModelForPreTraining.from_pretrained" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiates one of the model classes of the library -with the architecture used for pretraining this modelâ from a pre-trained model configuration.</p>
<p>The <cite>from_pretrained()</cite> method takes care of returning the correct model class instance
based on the <cite>model_type</cite> property of the config object, or when itâs missing,
falling back to using pattern matching on the <cite>pretrained_model_name_or_path</cite> string.</p>
<p>The model class to instantiate is selected as the first pattern matching
in the <cite>pretrained_model_name_or_path</cite> string (in the following order):</p>
<blockquote>
<div><ul class="simple">
<li>contains <cite>t5</cite>: <code class="xref py py-class docutils literal notranslate"><span class="pre">T5ModelWithLMHead</span></code> (T5 model)</li>
<li>contains <cite>distilbert</cite>: <a class="reference internal" href="distilbert.html#transformers.DistilBertForMaskedLM" title="transformers.DistilBertForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertForMaskedLM</span></code></a> (DistilBERT model)</li>
<li>contains <cite>albert</cite>: <a class="reference internal" href="albert.html#transformers.AlbertForMaskedLM" title="transformers.AlbertForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">AlbertForMaskedLM</span></code></a> (ALBERT model)</li>
<li>contains <cite>camembert</cite>: <a class="reference internal" href="camembert.html#transformers.CamembertForMaskedLM" title="transformers.CamembertForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">CamembertForMaskedLM</span></code></a> (CamemBERT model)</li>
<li>contains <cite>xlm-roberta</cite>: <a class="reference internal" href="xlmroberta.html#transformers.XLMRobertaForMaskedLM" title="transformers.XLMRobertaForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMRobertaForMaskedLM</span></code></a> (XLM-RoBERTa model)</li>
<li>contains <cite>roberta</cite>: <a class="reference internal" href="roberta.html#transformers.RobertaForMaskedLM" title="transformers.RobertaForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaForMaskedLM</span></code></a> (RoBERTa model)</li>
<li>contains <cite>bert</cite>: <a class="reference internal" href="bert.html#transformers.BertForPreTraining" title="transformers.BertForPreTraining"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertForPreTraining</span></code></a> (Bert model)</li>
<li>contains <cite>openai-gpt</cite>: <a class="reference internal" href="gpt.html#transformers.OpenAIGPTLMHeadModel" title="transformers.OpenAIGPTLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">OpenAIGPTLMHeadModel</span></code></a> (OpenAI GPT model)</li>
<li>contains <cite>gpt2</cite>: <a class="reference internal" href="gpt2.html#transformers.GPT2LMHeadModel" title="transformers.GPT2LMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPT2LMHeadModel</span></code></a> (OpenAI GPT-2 model)</li>
<li>contains <cite>transfo-xl</cite>: <a class="reference internal" href="transformerxl.html#transformers.TransfoXLLMHeadModel" title="transformers.TransfoXLLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TransfoXLLMHeadModel</span></code></a> (Transformer-XL model)</li>
<li>contains <cite>xlnet</cite>: <a class="reference internal" href="xlnet.html#transformers.XLNetLMHeadModel" title="transformers.XLNetLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetLMHeadModel</span></code></a> (XLNet model)</li>
<li>contains <cite>xlm</cite>: <a class="reference internal" href="xlm.html#transformers.XLMWithLMHeadModel" title="transformers.XLMWithLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMWithLMHeadModel</span></code></a> (XLM model)</li>
<li>contains <cite>ctrl</cite>: <a class="reference internal" href="ctrl.html#transformers.CTRLLMHeadModel" title="transformers.CTRLLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">CTRLLMHeadModel</span></code></a> (Salesforce CTRL model)</li>
<li>contains <cite>flaubert</cite>: <a class="reference internal" href="flaubert.html#transformers.FlaubertWithLMHeadModel" title="transformers.FlaubertWithLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">FlaubertWithLMHeadModel</span></code></a> (Flaubert model)</li>
</ul>
</div></blockquote>
<p>The model is set in evaluation mode by default using <cite>model.eval()</cite> (Dropout modules are deactivated)
To train the model, you should first set it back in training mode with <cite>model.train()</cite></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>pretrained_model_name_or_path</strong> â <p>Either:</p>
<ul>
<li>a string with the <cite>shortcut name</cite> of a pre-trained model to load from cache or download, e.g.: <code class="docutils literal notranslate"><span class="pre">bert-base-uncased</span></code>.</li>
<li>a string with the <cite>identifier name</cite> of a pre-trained model that was user-uploaded to our S3, e.g.: <code class="docutils literal notranslate"><span class="pre">dbmdz/bert-base-german-cased</span></code>.</li>
<li>a path to a <cite>directory</cite> containing model weights saved using <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a>, e.g.: <code class="docutils literal notranslate"><span class="pre">./my_model_directory/</span></code>.</li>
<li>a path or url to a <cite>tensorflow index checkpoint file</cite> (e.g. <cite>./tf_model/model.ckpt.index</cite>). In this case, <code class="docutils literal notranslate"><span class="pre">from_tf</span></code> should be set to True and a configuration object should be provided as <code class="docutils literal notranslate"><span class="pre">config</span></code> argument. This loading path is slower than converting the TensorFlow checkpoint in a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>
</li>
<li><strong>model_args</strong> â (<cite>optional</cite>) Sequence of positional arguments:
All remaning positional arguments will be passed to the underlying modelâs <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method</li>
<li><strong>config</strong> â <p>(<cite>optional</cite>) instance of a class derived from <a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>:
Configuration for the model to use instead of an automatically loaded configuation. Configuration can be automatically loaded when:</p>
<ul>
<li>the model is a model provided by the library (loaded with the <code class="docutils literal notranslate"><span class="pre">shortcut-name</span></code> string of a pretrained model), or</li>
<li>the model was saved using <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and is reloaded by suppling the save directory.</li>
<li>the model is loaded by suppling a local directory as <code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> and a configuration JSON file named <cite>config.json</cite> is found in the directory.</li>
</ul>
</li>
<li><strong>state_dict</strong> â (<cite>optional</cite>) dict:
an optional state dictionnary for the model to use instead of a state dictionary loaded from saved weights file.
This option can be used if you want to create a model from a pretrained configuration but load your own weights.
In this case though, you should check if using <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> is not a simpler option.</li>
<li><strong>cache_dir</strong> â (<cite>optional</cite>) string:
Path to a directory in which a downloaded pre-trained model
configuration should be cached if the standard cache should not be used.</li>
<li><strong>force_download</strong> â (<cite>optional</cite>) boolean, default False:
Force to (re-)download the model weights and configuration files and override the cached versions if they exists.</li>
<li><strong>resume_download</strong> â (<cite>optional</cite>) boolean, default False:
Do not delete incompletely received file. Attempt to resume the download if such a file exists.</li>
<li><strong>proxies</strong> â (<cite>optional</cite>) dict, default None:
A dictionary of proxy servers to use by protocol or endpoint, e.g.: {âhttpâ: âfoo.bar:3128â, âhttp://hostnameâ: âfoo.bar:4012â}.
The proxies are used on each request.</li>
<li><strong>output_loading_info</strong> â (<cite>optional</cite>) boolean:
Set to <code class="docutils literal notranslate"><span class="pre">True</span></code> to also return a dictionnary containing missing keys, unexpected keys and error messages.</li>
<li><strong>kwargs</strong> â (<cite>optional</cite>) Remaining dictionary of keyword arguments:
These arguments will be passed to the configuration and the model.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForPreTraining</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>    <span class="c1"># Download model and configuration from S3 and cache.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForPreTraining</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;./test/bert_model/&#39;</span><span class="p">)</span>  <span class="c1"># E.g. model was saved using `save_pretrained(&#39;./test/saved_model/&#39;)`</span>
<span class="k">assert</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attention</span> <span class="o">==</span> <span class="kc">True</span>
<span class="c1"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_json_file</span><span class="p">(</span><span class="s1">&#39;./tf_model/bert_tf_model_config.json&#39;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForPreTraining</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;./tf_model/bert_tf_checkpoint.ckpt.index&#39;</span><span class="p">,</span> <span class="n">from_tf</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="automodelwithlmhead">
<h2><code class="docutils literal notranslate"><span class="pre">AutoModelWithLMHead</span></code><a class="headerlink" href="#automodelwithlmhead" title="Permalink to this headline">Â¶</a></h2>
<dl class="class">
<dt id="transformers.AutoModelWithLMHead">
<em class="property">class </em><code class="descclassname">transformers.</code><code class="descname">AutoModelWithLMHead</code><a class="reference internal" href="../_modules/transformers/modeling_auto.html#AutoModelWithLMHead"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModelWithLMHead" title="Permalink to this definition">Â¶</a></dt>
<dd><p><a class="reference internal" href="#transformers.AutoModelWithLMHead" title="transformers.AutoModelWithLMHead"><code class="xref py py-class docutils literal notranslate"><span class="pre">AutoModelWithLMHead</span></code></a> is a generic model class
that will be instantiated as one of the language modeling model classes of the library
when created with the <cite>AutoModelWithLMHead.from_pretrained(pretrained_model_name_or_path)</cite>
class method.</p>
<p>This class cannot be instantiated using <cite>__init__()</cite> (throws an error).</p>
<dl class="classmethod">
<dt id="transformers.AutoModelWithLMHead.from_config">
<em class="property">classmethod </em><code class="descname">from_config</code><span class="sig-paren">(</span><em>config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_auto.html#AutoModelWithLMHead.from_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModelWithLMHead.from_config" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiates one of the base model classes of the library
from a configuration.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>config</strong> (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>) â <p>The model class to instantiate is selected based on the configuration class:</p>
<ul class="simple">
<li>isInstance of <cite>distilbert</cite> configuration class: <a class="reference internal" href="distilbert.html#transformers.DistilBertForMaskedLM" title="transformers.DistilBertForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertForMaskedLM</span></code></a> (DistilBERT model)</li>
<li>isInstance of <cite>roberta</cite> configuration class: <a class="reference internal" href="roberta.html#transformers.RobertaForMaskedLM" title="transformers.RobertaForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaForMaskedLM</span></code></a> (RoBERTa model)</li>
<li>isInstance of <cite>bert</cite> configuration class: <a class="reference internal" href="bert.html#transformers.BertForMaskedLM" title="transformers.BertForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertForMaskedLM</span></code></a> (Bert model)</li>
<li>isInstance of <cite>openai-gpt</cite> configuration class: <a class="reference internal" href="gpt.html#transformers.OpenAIGPTLMHeadModel" title="transformers.OpenAIGPTLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">OpenAIGPTLMHeadModel</span></code></a> (OpenAI GPT model)</li>
<li>isInstance of <cite>gpt2</cite> configuration class: <a class="reference internal" href="gpt2.html#transformers.GPT2LMHeadModel" title="transformers.GPT2LMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPT2LMHeadModel</span></code></a> (OpenAI GPT-2 model)</li>
<li>isInstance of <cite>ctrl</cite> configuration class: <a class="reference internal" href="ctrl.html#transformers.CTRLLMHeadModel" title="transformers.CTRLLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">CTRLLMHeadModel</span></code></a> (Salesforce CTRL  model)</li>
<li>isInstance of <cite>transfo-xl</cite> configuration class: <a class="reference internal" href="transformerxl.html#transformers.TransfoXLLMHeadModel" title="transformers.TransfoXLLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TransfoXLLMHeadModel</span></code></a> (Transformer-XL model)</li>
<li>isInstance of <cite>xlnet</cite> configuration class: <a class="reference internal" href="xlnet.html#transformers.XLNetLMHeadModel" title="transformers.XLNetLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetLMHeadModel</span></code></a> (XLNet model)</li>
<li>isInstance of <cite>xlm</cite> configuration class: <a class="reference internal" href="xlm.html#transformers.XLMWithLMHeadModel" title="transformers.XLMWithLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMWithLMHeadModel</span></code></a> (XLM model)</li>
<li>isInstance of <cite>flaubert</cite> configuration class: <a class="reference internal" href="flaubert.html#transformers.FlaubertWithLMHeadModel" title="transformers.FlaubertWithLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">FlaubertWithLMHeadModel</span></code></a> (Flaubert model)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">config</span> <span class="o">=</span> <span class="n">BertConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>    <span class="c1"># Download configuration from S3 and cache.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelWithLMHead</span><span class="o">.</span><span class="n">from_config</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>  <span class="c1"># E.g. model was saved using `save_pretrained(&#39;./test/saved_model/&#39;)`</span>
</pre></div>
</div>
</dd></dl>

<dl class="classmethod">
<dt id="transformers.AutoModelWithLMHead.from_pretrained">
<em class="property">classmethod </em><code class="descname">from_pretrained</code><span class="sig-paren">(</span><em>pretrained_model_name_or_path</em>, <em>*model_args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_auto.html#AutoModelWithLMHead.from_pretrained"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModelWithLMHead.from_pretrained" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiates one of the language modeling model classes of the library
from a pre-trained model configuration.</p>
<p>The <cite>from_pretrained()</cite> method takes care of returning the correct model class instance
based on the <cite>model_type</cite> property of the config object, or when itâs missing,
falling back to using pattern matching on the <cite>pretrained_model_name_or_path</cite> string.</p>
<p>The model class to instantiate is selected as the first pattern matching
in the <cite>pretrained_model_name_or_path</cite> string (in the following order):</p>
<blockquote>
<div><ul class="simple">
<li>contains <cite>t5</cite>: <code class="xref py py-class docutils literal notranslate"><span class="pre">T5ModelWithLMHead</span></code> (T5 model)</li>
<li>contains <cite>distilbert</cite>: <a class="reference internal" href="distilbert.html#transformers.DistilBertForMaskedLM" title="transformers.DistilBertForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertForMaskedLM</span></code></a> (DistilBERT model)</li>
<li>contains <cite>albert</cite>: <a class="reference internal" href="albert.html#transformers.AlbertForMaskedLM" title="transformers.AlbertForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">AlbertForMaskedLM</span></code></a> (ALBERT model)</li>
<li>contains <cite>camembert</cite>: <a class="reference internal" href="camembert.html#transformers.CamembertForMaskedLM" title="transformers.CamembertForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">CamembertForMaskedLM</span></code></a> (CamemBERT model)</li>
<li>contains <cite>xlm-roberta</cite>: <a class="reference internal" href="xlmroberta.html#transformers.XLMRobertaForMaskedLM" title="transformers.XLMRobertaForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMRobertaForMaskedLM</span></code></a> (XLM-RoBERTa model)</li>
<li>contains <cite>roberta</cite>: <a class="reference internal" href="roberta.html#transformers.RobertaForMaskedLM" title="transformers.RobertaForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaForMaskedLM</span></code></a> (RoBERTa model)</li>
<li>contains <cite>bert</cite>: <a class="reference internal" href="bert.html#transformers.BertForMaskedLM" title="transformers.BertForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertForMaskedLM</span></code></a> (Bert model)</li>
<li>contains <cite>openai-gpt</cite>: <a class="reference internal" href="gpt.html#transformers.OpenAIGPTLMHeadModel" title="transformers.OpenAIGPTLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">OpenAIGPTLMHeadModel</span></code></a> (OpenAI GPT model)</li>
<li>contains <cite>gpt2</cite>: <a class="reference internal" href="gpt2.html#transformers.GPT2LMHeadModel" title="transformers.GPT2LMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPT2LMHeadModel</span></code></a> (OpenAI GPT-2 model)</li>
<li>contains <cite>transfo-xl</cite>: <a class="reference internal" href="transformerxl.html#transformers.TransfoXLLMHeadModel" title="transformers.TransfoXLLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TransfoXLLMHeadModel</span></code></a> (Transformer-XL model)</li>
<li>contains <cite>xlnet</cite>: <a class="reference internal" href="xlnet.html#transformers.XLNetLMHeadModel" title="transformers.XLNetLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetLMHeadModel</span></code></a> (XLNet model)</li>
<li>contains <cite>xlm</cite>: <a class="reference internal" href="xlm.html#transformers.XLMWithLMHeadModel" title="transformers.XLMWithLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMWithLMHeadModel</span></code></a> (XLM model)</li>
<li>contains <cite>ctrl</cite>: <a class="reference internal" href="ctrl.html#transformers.CTRLLMHeadModel" title="transformers.CTRLLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">CTRLLMHeadModel</span></code></a> (Salesforce CTRL model)</li>
<li>contains <cite>flaubert</cite>: <a class="reference internal" href="flaubert.html#transformers.FlaubertWithLMHeadModel" title="transformers.FlaubertWithLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">FlaubertWithLMHeadModel</span></code></a> (Flaubert model)</li>
</ul>
</div></blockquote>
<p>The model is set in evaluation mode by default using <cite>model.eval()</cite> (Dropout modules are deactivated)
To train the model, you should first set it back in training mode with <cite>model.train()</cite></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>pretrained_model_name_or_path</strong> â <p>Either:</p>
<ul>
<li>a string with the <cite>shortcut name</cite> of a pre-trained model to load from cache or download, e.g.: <code class="docutils literal notranslate"><span class="pre">bert-base-uncased</span></code>.</li>
<li>a string with the <cite>identifier name</cite> of a pre-trained model that was user-uploaded to our S3, e.g.: <code class="docutils literal notranslate"><span class="pre">dbmdz/bert-base-german-cased</span></code>.</li>
<li>a path to a <cite>directory</cite> containing model weights saved using <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a>, e.g.: <code class="docutils literal notranslate"><span class="pre">./my_model_directory/</span></code>.</li>
<li>a path or url to a <cite>tensorflow index checkpoint file</cite> (e.g. <cite>./tf_model/model.ckpt.index</cite>). In this case, <code class="docutils literal notranslate"><span class="pre">from_tf</span></code> should be set to True and a configuration object should be provided as <code class="docutils literal notranslate"><span class="pre">config</span></code> argument. This loading path is slower than converting the TensorFlow checkpoint in a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>
</li>
<li><strong>model_args</strong> â (<cite>optional</cite>) Sequence of positional arguments:
All remaning positional arguments will be passed to the underlying modelâs <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method</li>
<li><strong>config</strong> â <p>(<cite>optional</cite>) instance of a class derived from <a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>:
Configuration for the model to use instead of an automatically loaded configuation. Configuration can be automatically loaded when:</p>
<ul>
<li>the model is a model provided by the library (loaded with the <code class="docutils literal notranslate"><span class="pre">shortcut-name</span></code> string of a pretrained model), or</li>
<li>the model was saved using <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and is reloaded by suppling the save directory.</li>
<li>the model is loaded by suppling a local directory as <code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> and a configuration JSON file named <cite>config.json</cite> is found in the directory.</li>
</ul>
</li>
<li><strong>state_dict</strong> â (<cite>optional</cite>) dict:
an optional state dictionnary for the model to use instead of a state dictionary loaded from saved weights file.
This option can be used if you want to create a model from a pretrained configuration but load your own weights.
In this case though, you should check if using <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> is not a simpler option.</li>
<li><strong>cache_dir</strong> â (<cite>optional</cite>) string:
Path to a directory in which a downloaded pre-trained model
configuration should be cached if the standard cache should not be used.</li>
<li><strong>force_download</strong> â (<cite>optional</cite>) boolean, default False:
Force to (re-)download the model weights and configuration files and override the cached versions if they exists.</li>
<li><strong>resume_download</strong> â (<cite>optional</cite>) boolean, default False:
Do not delete incompletely received file. Attempt to resume the download if such a file exists.</li>
<li><strong>proxies</strong> â (<cite>optional</cite>) dict, default None:
A dictionary of proxy servers to use by protocol or endpoint, e.g.: {âhttpâ: âfoo.bar:3128â, âhttp://hostnameâ: âfoo.bar:4012â}.
The proxies are used on each request.</li>
<li><strong>output_loading_info</strong> â (<cite>optional</cite>) boolean:
Set to <code class="docutils literal notranslate"><span class="pre">True</span></code> to also return a dictionnary containing missing keys, unexpected keys and error messages.</li>
<li><strong>kwargs</strong> â (<cite>optional</cite>) Remaining dictionary of keyword arguments:
These arguments will be passed to the configuration and the model.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelWithLMHead</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>    <span class="c1"># Download model and configuration from S3 and cache.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelWithLMHead</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;./test/bert_model/&#39;</span><span class="p">)</span>  <span class="c1"># E.g. model was saved using `save_pretrained(&#39;./test/saved_model/&#39;)`</span>
<span class="k">assert</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attention</span> <span class="o">==</span> <span class="kc">True</span>
<span class="c1"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_json_file</span><span class="p">(</span><span class="s1">&#39;./tf_model/bert_tf_model_config.json&#39;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelWithLMHead</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;./tf_model/bert_tf_checkpoint.ckpt.index&#39;</span><span class="p">,</span> <span class="n">from_tf</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="automodelforsequenceclassification">
<h2><code class="docutils literal notranslate"><span class="pre">AutoModelForSequenceClassification</span></code><a class="headerlink" href="#automodelforsequenceclassification" title="Permalink to this headline">Â¶</a></h2>
<dl class="class">
<dt id="transformers.AutoModelForSequenceClassification">
<em class="property">class </em><code class="descclassname">transformers.</code><code class="descname">AutoModelForSequenceClassification</code><a class="reference internal" href="../_modules/transformers/modeling_auto.html#AutoModelForSequenceClassification"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModelForSequenceClassification" title="Permalink to this definition">Â¶</a></dt>
<dd><p><a class="reference internal" href="#transformers.AutoModelForSequenceClassification" title="transformers.AutoModelForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">AutoModelForSequenceClassification</span></code></a> is a generic model class
that will be instantiated as one of the sequence classification model classes of the library
when created with the <cite>AutoModelForSequenceClassification.from_pretrained(pretrained_model_name_or_path)</cite>
class method.</p>
<p>This class cannot be instantiated using <cite>__init__()</cite> (throws an error).</p>
<dl class="classmethod">
<dt id="transformers.AutoModelForSequenceClassification.from_config">
<em class="property">classmethod </em><code class="descname">from_config</code><span class="sig-paren">(</span><em>config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_auto.html#AutoModelForSequenceClassification.from_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModelForSequenceClassification.from_config" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiates one of the base model classes of the library
from a configuration.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>config</strong> (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>) â <p>The model class to instantiate is selected based on the configuration class:</p>
<ul class="simple">
<li>isInstance of <cite>distilbert</cite> configuration class: <a class="reference internal" href="distilbert.html#transformers.DistilBertForSequenceClassification" title="transformers.DistilBertForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertForSequenceClassification</span></code></a> (DistilBERT model)</li>
<li>isInstance of <cite>albert</cite> configuration class: <a class="reference internal" href="albert.html#transformers.AlbertForSequenceClassification" title="transformers.AlbertForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">AlbertForSequenceClassification</span></code></a> (ALBERT model)</li>
<li>isInstance of <cite>camembert</cite> configuration class: <a class="reference internal" href="camembert.html#transformers.CamembertForSequenceClassification" title="transformers.CamembertForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">CamembertForSequenceClassification</span></code></a> (CamemBERT model)</li>
<li>isInstance of <cite>xlm roberta</cite> configuration class: <a class="reference internal" href="xlmroberta.html#transformers.XLMRobertaForSequenceClassification" title="transformers.XLMRobertaForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMRobertaForSequenceClassification</span></code></a> (XLM-RoBERTa model)</li>
<li>isInstance of <cite>roberta</cite> configuration class: <a class="reference internal" href="roberta.html#transformers.RobertaForSequenceClassification" title="transformers.RobertaForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaForSequenceClassification</span></code></a> (RoBERTa model)</li>
<li>isInstance of <cite>bert</cite> configuration class: <a class="reference internal" href="bert.html#transformers.BertForSequenceClassification" title="transformers.BertForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertForSequenceClassification</span></code></a> (Bert model)</li>
<li>isInstance of <cite>xlnet</cite> configuration class: <a class="reference internal" href="xlnet.html#transformers.XLNetForSequenceClassification" title="transformers.XLNetForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetForSequenceClassification</span></code></a> (XLNet model)</li>
<li>isInstance of <cite>xlm</cite> configuration class: <a class="reference internal" href="xlm.html#transformers.XLMForSequenceClassification" title="transformers.XLMForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMForSequenceClassification</span></code></a> (XLM model)</li>
<li>isInstance of <cite>flaubert</cite> configuration class: <a class="reference internal" href="flaubert.html#transformers.FlaubertForSequenceClassification" title="transformers.FlaubertForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">FlaubertForSequenceClassification</span></code></a> (Flaubert model)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">config</span> <span class="o">=</span> <span class="n">BertConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>    <span class="c1"># Download configuration from S3 and cache.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_config</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>  <span class="c1"># E.g. model was saved using `save_pretrained(&#39;./test/saved_model/&#39;)`</span>
</pre></div>
</div>
</dd></dl>

<dl class="classmethod">
<dt id="transformers.AutoModelForSequenceClassification.from_pretrained">
<em class="property">classmethod </em><code class="descname">from_pretrained</code><span class="sig-paren">(</span><em>pretrained_model_name_or_path</em>, <em>*model_args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_auto.html#AutoModelForSequenceClassification.from_pretrained"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModelForSequenceClassification.from_pretrained" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiates one of the sequence classification model classes of the library
from a pre-trained model configuration.</p>
<p>The <cite>from_pretrained()</cite> method takes care of returning the correct model class instance
based on the <cite>model_type</cite> property of the config object, or when itâs missing,
falling back to using pattern matching on the <cite>pretrained_model_name_or_path</cite> string.</p>
<p>The model class to instantiate is selected as the first pattern matching
in the <cite>pretrained_model_name_or_path</cite> string (in the following order):</p>
<blockquote>
<div><ul class="simple">
<li>contains <cite>distilbert</cite>: <a class="reference internal" href="distilbert.html#transformers.DistilBertForSequenceClassification" title="transformers.DistilBertForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertForSequenceClassification</span></code></a> (DistilBERT model)</li>
<li>contains <cite>albert</cite>: <a class="reference internal" href="albert.html#transformers.AlbertForSequenceClassification" title="transformers.AlbertForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">AlbertForSequenceClassification</span></code></a> (ALBERT model)</li>
<li>contains <cite>camembert</cite>: <a class="reference internal" href="camembert.html#transformers.CamembertForSequenceClassification" title="transformers.CamembertForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">CamembertForSequenceClassification</span></code></a> (CamemBERT model)</li>
<li>contains <cite>xlm-roberta</cite>: <a class="reference internal" href="xlmroberta.html#transformers.XLMRobertaForSequenceClassification" title="transformers.XLMRobertaForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMRobertaForSequenceClassification</span></code></a> (XLM-RoBERTa model)</li>
<li>contains <cite>roberta</cite>: <a class="reference internal" href="roberta.html#transformers.RobertaForSequenceClassification" title="transformers.RobertaForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaForSequenceClassification</span></code></a> (RoBERTa model)</li>
<li>contains <cite>bert</cite>: <a class="reference internal" href="bert.html#transformers.BertForSequenceClassification" title="transformers.BertForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertForSequenceClassification</span></code></a> (Bert model)</li>
<li>contains <cite>xlnet</cite>: <a class="reference internal" href="xlnet.html#transformers.XLNetForSequenceClassification" title="transformers.XLNetForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetForSequenceClassification</span></code></a> (XLNet model)</li>
<li>contains <cite>flaubert</cite>: <a class="reference internal" href="flaubert.html#transformers.FlaubertForSequenceClassification" title="transformers.FlaubertForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">FlaubertForSequenceClassification</span></code></a> (Flaubert model)</li>
</ul>
</div></blockquote>
<p>The model is set in evaluation mode by default using <cite>model.eval()</cite> (Dropout modules are deactivated)
To train the model, you should first set it back in training mode with <cite>model.train()</cite></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>pretrained_model_name_or_path</strong> â <p>either:</p>
<ul>
<li>a string with the <cite>shortcut name</cite> of a pre-trained model to load from cache or download, e.g.: <code class="docutils literal notranslate"><span class="pre">bert-base-uncased</span></code>.</li>
<li>a string with the <cite>identifier name</cite> of a pre-trained model that was user-uploaded to our S3, e.g.: <code class="docutils literal notranslate"><span class="pre">dbmdz/bert-base-german-cased</span></code>.</li>
<li>a path to a <cite>directory</cite> containing model weights saved using <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a>, e.g.: <code class="docutils literal notranslate"><span class="pre">./my_model_directory/</span></code>.</li>
<li>a path or url to a <cite>tensorflow index checkpoint file</cite> (e.g. <cite>./tf_model/model.ckpt.index</cite>). In this case, <code class="docutils literal notranslate"><span class="pre">from_tf</span></code> should be set to True and a configuration object should be provided as <code class="docutils literal notranslate"><span class="pre">config</span></code> argument. This loading path is slower than converting the TensorFlow checkpoint in a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>
</li>
<li><strong>model_args</strong> â (<cite>optional</cite>) Sequence of positional arguments:
All remaining positional arguments will be passed to the underlying modelâs <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method</li>
<li><strong>config</strong> â <p>(<cite>optional</cite>) instance of a class derived from <a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>:
Configuration for the model to use instead of an automatically loaded configuation. Configuration can be automatically loaded when:</p>
<ul>
<li>the model is a model provided by the library (loaded with the <code class="docutils literal notranslate"><span class="pre">shortcut-name</span></code> string of a pretrained model), or</li>
<li>the model was saved using <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and is reloaded by suppling the save directory.</li>
<li>the model is loaded by suppling a local directory as <code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> and a configuration JSON file named <cite>config.json</cite> is found in the directory.</li>
</ul>
</li>
<li><strong>state_dict</strong> â (<cite>optional</cite>) dict:
an optional state dictionnary for the model to use instead of a state dictionary loaded from saved weights file.
This option can be used if you want to create a model from a pretrained configuration but load your own weights.
In this case though, you should check if using <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> is not a simpler option.</li>
<li><strong>cache_dir</strong> â (<cite>optional</cite>) string:
Path to a directory in which a downloaded pre-trained model
configuration should be cached if the standard cache should not be used.</li>
<li><strong>force_download</strong> â (<cite>optional</cite>) boolean, default False:
Force to (re-)download the model weights and configuration files and override the cached versions if they exists.</li>
<li><strong>resume_download</strong> â (<cite>optional</cite>) boolean, default False:
Do not delete incompletely recieved file. Attempt to resume the download if such a file exists.</li>
<li><strong>proxies</strong> â (<cite>optional</cite>) dict, default None:
A dictionary of proxy servers to use by protocol or endpoint, e.g.: {âhttpâ: âfoo.bar:3128â, âhttp://hostnameâ: âfoo.bar:4012â}.
The proxies are used on each request.</li>
<li><strong>output_loading_info</strong> â (<cite>optional</cite>) boolean:
Set to <code class="docutils literal notranslate"><span class="pre">True</span></code> to also return a dictionnary containing missing keys, unexpected keys and error messages.</li>
<li><strong>kwargs</strong> â (<cite>optional</cite>) Remaining dictionary of keyword arguments:
These arguments will be passed to the configuration and the model.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>    <span class="c1"># Download model and configuration from S3 and cache.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;./test/bert_model/&#39;</span><span class="p">)</span>  <span class="c1"># E.g. model was saved using `save_pretrained(&#39;./test/saved_model/&#39;)`</span>
<span class="k">assert</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attention</span> <span class="o">==</span> <span class="kc">True</span>
<span class="c1"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_json_file</span><span class="p">(</span><span class="s1">&#39;./tf_model/bert_tf_model_config.json&#39;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;./tf_model/bert_tf_checkpoint.ckpt.index&#39;</span><span class="p">,</span> <span class="n">from_tf</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="automodelforquestionanswering">
<h2><code class="docutils literal notranslate"><span class="pre">AutoModelForQuestionAnswering</span></code><a class="headerlink" href="#automodelforquestionanswering" title="Permalink to this headline">Â¶</a></h2>
<dl class="class">
<dt id="transformers.AutoModelForQuestionAnswering">
<em class="property">class </em><code class="descclassname">transformers.</code><code class="descname">AutoModelForQuestionAnswering</code><a class="reference internal" href="../_modules/transformers/modeling_auto.html#AutoModelForQuestionAnswering"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModelForQuestionAnswering" title="Permalink to this definition">Â¶</a></dt>
<dd><p><a class="reference internal" href="#transformers.AutoModelForQuestionAnswering" title="transformers.AutoModelForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">AutoModelForQuestionAnswering</span></code></a> is a generic model class
that will be instantiated as one of the question answering model classes of the library
when created with the <cite>AutoModelForQuestionAnswering.from_pretrained(pretrained_model_name_or_path)</cite>
class method.</p>
<p>This class cannot be instantiated using <cite>__init__()</cite> (throws an error).</p>
<dl class="classmethod">
<dt id="transformers.AutoModelForQuestionAnswering.from_config">
<em class="property">classmethod </em><code class="descname">from_config</code><span class="sig-paren">(</span><em>config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_auto.html#AutoModelForQuestionAnswering.from_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModelForQuestionAnswering.from_config" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiates one of the base model classes of the library
from a configuration.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>config</strong> (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>) â <p>The model class to instantiate is selected based on the configuration class:</p>
<ul class="simple">
<li>isInstance of <cite>distilbert</cite> configuration class: <a class="reference internal" href="distilbert.html#transformers.DistilBertForQuestionAnswering" title="transformers.DistilBertForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertForQuestionAnswering</span></code></a> (DistilBERT model)</li>
<li>isInstance of <cite>albert</cite> configuration class: <a class="reference internal" href="albert.html#transformers.AlbertForQuestionAnswering" title="transformers.AlbertForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">AlbertForQuestionAnswering</span></code></a> (ALBERT model)</li>
<li>isInstance of <cite>bert</cite> configuration class: <code class="xref py py-class docutils literal notranslate"><span class="pre">BertModelForQuestionAnswering</span></code> (Bert model)</li>
<li>isInstance of <cite>xlnet</cite> configuration class: <a class="reference internal" href="xlnet.html#transformers.XLNetForQuestionAnswering" title="transformers.XLNetForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetForQuestionAnswering</span></code></a> (XLNet model)</li>
<li>isInstance of <cite>xlm</cite> configuration class: <a class="reference internal" href="xlm.html#transformers.XLMForQuestionAnswering" title="transformers.XLMForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMForQuestionAnswering</span></code></a> (XLM model)</li>
<li>isInstance of <cite>flaubert</cite> configuration class: <a class="reference internal" href="flaubert.html#transformers.FlaubertForQuestionAnswering" title="transformers.FlaubertForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">FlaubertForQuestionAnswering</span></code></a> (XLM model)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">config</span> <span class="o">=</span> <span class="n">BertConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>    <span class="c1"># Download configuration from S3 and cache.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_config</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>  <span class="c1"># E.g. model was saved using `save_pretrained(&#39;./test/saved_model/&#39;)`</span>
</pre></div>
</div>
</dd></dl>

<dl class="classmethod">
<dt id="transformers.AutoModelForQuestionAnswering.from_pretrained">
<em class="property">classmethod </em><code class="descname">from_pretrained</code><span class="sig-paren">(</span><em>pretrained_model_name_or_path</em>, <em>*model_args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_auto.html#AutoModelForQuestionAnswering.from_pretrained"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModelForQuestionAnswering.from_pretrained" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiates one of the question answering model classes of the library
from a pre-trained model configuration.</p>
<p>The <cite>from_pretrained()</cite> method takes care of returning the correct model class instance
based on the <cite>model_type</cite> property of the config object, or when itâs missing,
falling back to using pattern matching on the <cite>pretrained_model_name_or_path</cite> string.</p>
<p>The model class to instantiate is selected as the first pattern matching
in the <cite>pretrained_model_name_or_path</cite> string (in the following order):</p>
<blockquote>
<div><ul class="simple">
<li>contains <cite>distilbert</cite>: <a class="reference internal" href="distilbert.html#transformers.DistilBertForQuestionAnswering" title="transformers.DistilBertForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertForQuestionAnswering</span></code></a> (DistilBERT model)</li>
<li>contains <cite>albert</cite>: <a class="reference internal" href="albert.html#transformers.AlbertForQuestionAnswering" title="transformers.AlbertForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">AlbertForQuestionAnswering</span></code></a> (ALBERT model)</li>
<li>contains <cite>bert</cite>: <a class="reference internal" href="bert.html#transformers.BertForQuestionAnswering" title="transformers.BertForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertForQuestionAnswering</span></code></a> (Bert model)</li>
<li>contains <cite>xlnet</cite>: <a class="reference internal" href="xlnet.html#transformers.XLNetForQuestionAnswering" title="transformers.XLNetForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetForQuestionAnswering</span></code></a> (XLNet model)</li>
<li>contains <cite>xlm</cite>: <a class="reference internal" href="xlm.html#transformers.XLMForQuestionAnswering" title="transformers.XLMForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMForQuestionAnswering</span></code></a> (XLM model)</li>
<li>contains <cite>flaubert</cite>: <a class="reference internal" href="flaubert.html#transformers.FlaubertForQuestionAnswering" title="transformers.FlaubertForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">FlaubertForQuestionAnswering</span></code></a> (XLM model)</li>
</ul>
</div></blockquote>
<p>The model is set in evaluation mode by default using <cite>model.eval()</cite> (Dropout modules are deactivated)
To train the model, you should first set it back in training mode with <cite>model.train()</cite></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>pretrained_model_name_or_path</strong> â <p>either:</p>
<ul>
<li>a string with the <cite>shortcut name</cite> of a pre-trained model to load from cache or download, e.g.: <code class="docutils literal notranslate"><span class="pre">bert-base-uncased</span></code>.</li>
<li>a string with the <cite>identifier name</cite> of a pre-trained model that was user-uploaded to our S3, e.g.: <code class="docutils literal notranslate"><span class="pre">dbmdz/bert-base-german-cased</span></code>.</li>
<li>a path to a <cite>directory</cite> containing model weights saved using <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a>, e.g.: <code class="docutils literal notranslate"><span class="pre">./my_model_directory/</span></code>.</li>
<li>a path or url to a <cite>tensorflow index checkpoint file</cite> (e.g. <cite>./tf_model/model.ckpt.index</cite>). In this case, <code class="docutils literal notranslate"><span class="pre">from_tf</span></code> should be set to True and a configuration object should be provided as <code class="docutils literal notranslate"><span class="pre">config</span></code> argument. This loading path is slower than converting the TensorFlow checkpoint in a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>
</li>
<li><strong>model_args</strong> â (<cite>optional</cite>) Sequence of positional arguments:
All remaning positional arguments will be passed to the underlying modelâs <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method</li>
<li><strong>config</strong> â <p>(<cite>optional</cite>) instance of a class derived from <a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>:
Configuration for the model to use instead of an automatically loaded configuation. Configuration can be automatically loaded when:</p>
<ul>
<li>the model is a model provided by the library (loaded with the <code class="docutils literal notranslate"><span class="pre">shortcut-name</span></code> string of a pretrained model), or</li>
<li>the model was saved using <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and is reloaded by suppling the save directory.</li>
<li>the model is loaded by suppling a local directory as <code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> and a configuration JSON file named <cite>config.json</cite> is found in the directory.</li>
</ul>
</li>
<li><strong>state_dict</strong> â (<cite>optional</cite>) dict:
an optional state dictionnary for the model to use instead of a state dictionary loaded from saved weights file.
This option can be used if you want to create a model from a pretrained configuration but load your own weights.
In this case though, you should check if using <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> is not a simpler option.</li>
<li><strong>cache_dir</strong> â (<cite>optional</cite>) string:
Path to a directory in which a downloaded pre-trained model
configuration should be cached if the standard cache should not be used.</li>
<li><strong>force_download</strong> â (<cite>optional</cite>) boolean, default False:
Force to (re-)download the model weights and configuration files and override the cached versions if they exists.</li>
<li><strong>proxies</strong> â (<cite>optional</cite>) dict, default None:
A dictionary of proxy servers to use by protocol or endpoint, e.g.: {âhttpâ: âfoo.bar:3128â, âhttp://hostnameâ: âfoo.bar:4012â}.
The proxies are used on each request.</li>
<li><strong>output_loading_info</strong> â (<cite>optional</cite>) boolean:
Set to <code class="docutils literal notranslate"><span class="pre">True</span></code> to also return a dictionnary containing missing keys, unexpected keys and error messages.</li>
<li><strong>kwargs</strong> â (<cite>optional</cite>) Remaining dictionary of keyword arguments:
These arguments will be passed to the configuration and the model.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForQuestionAnswering</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>    <span class="c1"># Download model and configuration from S3 and cache.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForQuestionAnswering</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;./test/bert_model/&#39;</span><span class="p">)</span>  <span class="c1"># E.g. model was saved using `save_pretrained(&#39;./test/saved_model/&#39;)`</span>
<span class="k">assert</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attention</span> <span class="o">==</span> <span class="kc">True</span>
<span class="c1"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_json_file</span><span class="p">(</span><span class="s1">&#39;./tf_model/bert_tf_model_config.json&#39;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForQuestionAnswering</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;./tf_model/bert_tf_checkpoint.ckpt.index&#39;</span><span class="p">,</span> <span class="n">from_tf</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="automodelfortokenclassification">
<h2><code class="docutils literal notranslate"><span class="pre">AutoModelForTokenClassification</span></code><a class="headerlink" href="#automodelfortokenclassification" title="Permalink to this headline">Â¶</a></h2>
<dl class="class">
<dt id="transformers.AutoModelForTokenClassification">
<em class="property">class </em><code class="descclassname">transformers.</code><code class="descname">AutoModelForTokenClassification</code><a class="reference internal" href="../_modules/transformers/modeling_auto.html#AutoModelForTokenClassification"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModelForTokenClassification" title="Permalink to this definition">Â¶</a></dt>
<dd><p><a class="reference internal" href="#transformers.AutoModelForTokenClassification" title="transformers.AutoModelForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">AutoModelForTokenClassification</span></code></a> is a generic model class
that will be instantiated as one of the token classification model classes of the library
when created with the <cite>AutoModelForTokenClassification.from_pretrained(pretrained_model_name_or_path)</cite>
class method.</p>
<p>This class cannot be instantiated using <cite>__init__()</cite> (throws an error).</p>
<dl class="classmethod">
<dt id="transformers.AutoModelForTokenClassification.from_config">
<em class="property">classmethod </em><code class="descname">from_config</code><span class="sig-paren">(</span><em>config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_auto.html#AutoModelForTokenClassification.from_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModelForTokenClassification.from_config" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiates one of the base model classes of the library
from a configuration.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>config</strong> (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>) â <p>The model class to instantiate is selected based on the configuration class:</p>
<ul class="simple">
<li>isInstance of <cite>distilbert</cite> configuration class: <code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertModelForTokenClassification</span></code> (DistilBERT model)</li>
<li>isInstance of <cite>xlm</cite> configuration class: <code class="xref py py-class docutils literal notranslate"><span class="pre">XLMForTokenClassification</span></code> (XLM model)</li>
<li>isInstance of <cite>xlm roberta</cite> configuration class: <code class="xref py py-class docutils literal notranslate"><span class="pre">XLMRobertaModelForTokenClassification</span></code> (XLMRoberta model)</li>
<li>isInstance of <cite>bert</cite> configuration class: <code class="xref py py-class docutils literal notranslate"><span class="pre">BertModelForTokenClassification</span></code> (Bert model)</li>
<li>isInstance of <cite>albert</cite> configuration class: <code class="xref py py-class docutils literal notranslate"><span class="pre">AlbertForTokenClassification</span></code> (AlBert model)</li>
<li>isInstance of <cite>xlnet</cite> configuration class: <code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetModelForTokenClassification</span></code> (XLNet model)</li>
<li>isInstance of <cite>camembert</cite> configuration class: <code class="xref py py-class docutils literal notranslate"><span class="pre">CamembertModelForTokenClassification</span></code> (Camembert model)</li>
<li>isInstance of <cite>roberta</cite> configuration class: <code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaModelForTokenClassification</span></code> (Roberta model)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">config</span> <span class="o">=</span> <span class="n">BertConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>    <span class="c1"># Download configuration from S3 and cache.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForTokenClassification</span><span class="o">.</span><span class="n">from_config</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>  <span class="c1"># E.g. model was saved using `save_pretrained(&#39;./test/saved_model/&#39;)`</span>
</pre></div>
</div>
</dd></dl>

<dl class="classmethod">
<dt id="transformers.AutoModelForTokenClassification.from_pretrained">
<em class="property">classmethod </em><code class="descname">from_pretrained</code><span class="sig-paren">(</span><em>pretrained_model_name_or_path</em>, <em>*model_args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_auto.html#AutoModelForTokenClassification.from_pretrained"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModelForTokenClassification.from_pretrained" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiates one of the question answering model classes of the library
from a pre-trained model configuration.</p>
<p>The <cite>from_pretrained()</cite> method takes care of returning the correct model class instance
based on the <cite>model_type</cite> property of the config object, or when itâs missing,
falling back to using pattern matching on the <cite>pretrained_model_name_or_path</cite> string.</p>
<p>The model class to instantiate is selected as the first pattern matching
in the <cite>pretrained_model_name_or_path</cite> string (in the following order):</p>
<blockquote>
<div><ul class="simple">
<li>contains <cite>distilbert</cite>: <code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertForTokenClassification</span></code> (DistilBERT model)</li>
<li>contains <cite>xlm</cite>: <code class="xref py py-class docutils literal notranslate"><span class="pre">XLMForTokenClassification</span></code> (XLM model)</li>
<li>contains <cite>xlm-roberta</cite>: <a class="reference internal" href="xlmroberta.html#transformers.XLMRobertaForTokenClassification" title="transformers.XLMRobertaForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMRobertaForTokenClassification</span></code></a> (XLM-RoBERTa?Para model)</li>
<li>contains <cite>camembert</cite>: <a class="reference internal" href="camembert.html#transformers.CamembertForTokenClassification" title="transformers.CamembertForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">CamembertForTokenClassification</span></code></a> (Camembert model)</li>
<li>contains <cite>bert</cite>: <a class="reference internal" href="bert.html#transformers.BertForTokenClassification" title="transformers.BertForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertForTokenClassification</span></code></a> (Bert model)</li>
<li>contains <cite>xlnet</cite>: <a class="reference internal" href="xlnet.html#transformers.XLNetForTokenClassification" title="transformers.XLNetForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetForTokenClassification</span></code></a> (XLNet model)</li>
<li>contains <cite>roberta</cite>: <a class="reference internal" href="roberta.html#transformers.RobertaForTokenClassification" title="transformers.RobertaForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaForTokenClassification</span></code></a> (Roberta model)</li>
</ul>
</div></blockquote>
<p>The model is set in evaluation mode by default using <cite>model.eval()</cite> (Dropout modules are deactivated)
To train the model, you should first set it back in training mode with <cite>model.train()</cite></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>pretrained_model_name_or_path</strong> â <p>Either:</p>
<ul>
<li>a string with the <cite>shortcut name</cite> of a pre-trained model to load from cache or download, e.g.: <code class="docutils literal notranslate"><span class="pre">bert-base-uncased</span></code>.</li>
<li>a path to a <cite>directory</cite> containing model weights saved using <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a>, e.g.: <code class="docutils literal notranslate"><span class="pre">./my_model_directory/</span></code>.</li>
<li>a path or url to a <cite>tensorflow index checkpoint file</cite> (e.g. <cite>./tf_model/model.ckpt.index</cite>). In this case, <code class="docutils literal notranslate"><span class="pre">from_tf</span></code> should be set to True and a configuration object should be provided as <code class="docutils literal notranslate"><span class="pre">config</span></code> argument. This loading path is slower than converting the TensorFlow checkpoint in a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>
</li>
<li><strong>model_args</strong> â (<cite>optional</cite>) Sequence of positional arguments:
All remaning positional arguments will be passed to the underlying modelâs <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method</li>
<li><strong>config</strong> â <p>(<cite>optional</cite>) instance of a class derived from <a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>:
Configuration for the model to use instead of an automatically loaded configuation. Configuration can be automatically loaded when:</p>
<ul>
<li>the model is a model provided by the library (loaded with the <code class="docutils literal notranslate"><span class="pre">shortcut-name</span></code> string of a pretrained model), or</li>
<li>the model was saved using <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and is reloaded by suppling the save directory.</li>
<li>the model is loaded by suppling a local directory as <code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> and a configuration JSON file named <cite>config.json</cite> is found in the directory.</li>
</ul>
</li>
<li><strong>state_dict</strong> â (<cite>optional</cite>) dict:
an optional state dictionnary for the model to use instead of a state dictionary loaded from saved weights file.
This option can be used if you want to create a model from a pretrained configuration but load your own weights.
In this case though, you should check if using <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> is not a simpler option.</li>
<li><strong>cache_dir</strong> â (<cite>optional</cite>) string:
Path to a directory in which a downloaded pre-trained model
configuration should be cached if the standard cache should not be used.</li>
<li><strong>force_download</strong> â (<cite>optional</cite>) boolean, default False:
Force to (re-)download the model weights and configuration files and override the cached versions if they exists.</li>
<li><strong>proxies</strong> â (<cite>optional</cite>) dict, default None:
A dictionary of proxy servers to use by protocol or endpoint, e.g.: {âhttpâ: âfoo.bar:3128â, âhttp://hostnameâ: âfoo.bar:4012â}.
The proxies are used on each request.</li>
<li><strong>output_loading_info</strong> â (<cite>optional</cite>) boolean:
Set to <code class="docutils literal notranslate"><span class="pre">True</span></code> to also return a dictionnary containing missing keys, unexpected keys and error messages.</li>
<li><strong>kwargs</strong> â (<cite>optional</cite>) Remaining dictionary of keyword arguments:
These arguments will be passed to the configuration and the model.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForTokenClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>    <span class="c1"># Download model and configuration from S3 and cache.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForTokenClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;./test/bert_model/&#39;</span><span class="p">)</span>  <span class="c1"># E.g. model was saved using `save_pretrained(&#39;./test/saved_model/&#39;)`</span>
<span class="k">assert</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attention</span> <span class="o">==</span> <span class="kc">True</span>
<span class="c1"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_json_file</span><span class="p">(</span><span class="s1">&#39;./tf_model/bert_tf_model_config.json&#39;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForTokenClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;./tf_model/bert_tf_checkpoint.ckpt.index&#39;</span><span class="p">,</span> <span class="n">from_tf</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="bert.html" class="btn btn-neutral float-right" title="BERT" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="../installation.html" class="btn btn-neutral" title="Installation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, huggingface

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
        <script type="text/javascript" src="../_static/js/custom.js"></script>
    

  

  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>