

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>BERT &mdash; transformers 2.6.0 documentation</title>
  

  
  
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/huggingface.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/code-snippets.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="OpenAI GPT" href="gpt.html" />
    <link rel="prev" title="AutoModels" href="auto.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> transformers
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pretrained_models.html">Pretrained models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../usage.html">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_sharing.html">Model Sharing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks.html">Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../serialization.html">Loading Google AI or OpenAI pre-trained weights or PyTorch dump</a></li>
<li class="toctree-l1"><a class="reference internal" href="../serialization.html#serialization-best-practices">Serialization best-practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../converting_tensorflow_models.html">Converting Tensorflow Checkpoints</a></li>
<li class="toctree-l1"><a class="reference internal" href="../torchscript.html">Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../multilingual.html">Multi-lingual models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../benchmarks.html">Benchmarks</a></li>
</ul>
<p class="caption"><span class="caption-text">Core Classes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/configuration.html">Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/model.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/tokenizer.html">Tokenizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/pipelines.html">Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/optimizer_schedules.html">Optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/optimizer_schedules.html#schedules">Schedules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/optimizer_schedules.html#gradient-strategies">Gradient Strategies</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/processors.html">Processors</a></li>
</ul>
<p class="caption"><span class="caption-text">Features</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../migration.html">Migrating from pytorch-pretrained-bert</a></li>
<li class="toctree-l1"><a class="reference internal" href="../glossary.html">Glossary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../bertology.html">BERTology</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
</ul>
<p class="caption"><span class="caption-text">Models</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="auto.html">AutoModels</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">BERT</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#bertconfig">BertConfig</a></li>
<li class="toctree-l2"><a class="reference internal" href="#berttokenizer">BertTokenizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="#bertmodel">BertModel</a></li>
<li class="toctree-l2"><a class="reference internal" href="#bertforpretraining">BertForPreTraining</a></li>
<li class="toctree-l2"><a class="reference internal" href="#bertformaskedlm">BertForMaskedLM</a></li>
<li class="toctree-l2"><a class="reference internal" href="#bertfornextsentenceprediction">BertForNextSentencePrediction</a></li>
<li class="toctree-l2"><a class="reference internal" href="#bertforsequenceclassification">BertForSequenceClassification</a></li>
<li class="toctree-l2"><a class="reference internal" href="#bertformultiplechoice">BertForMultipleChoice</a></li>
<li class="toctree-l2"><a class="reference internal" href="#bertfortokenclassification">BertForTokenClassification</a></li>
<li class="toctree-l2"><a class="reference internal" href="#bertforquestionanswering">BertForQuestionAnswering</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tfbertmodel">TFBertModel</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tfbertforpretraining">TFBertForPreTraining</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tfbertformaskedlm">TFBertForMaskedLM</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tfbertfornextsentenceprediction">TFBertForNextSentencePrediction</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tfbertforsequenceclassification">TFBertForSequenceClassification</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tfbertformultiplechoice">TFBertForMultipleChoice</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tfbertfortokenclassification">TFBertForTokenClassification</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tfbertforquestionanswering">TFBertForQuestionAnswering</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="gpt.html">OpenAI GPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="transformerxl.html">Transformer XL</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpt2.html">OpenAI GPT2</a></li>
<li class="toctree-l1"><a class="reference internal" href="xlm.html">XLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="xlnet.html">XLNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="roberta.html">RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="distilbert.html">DistilBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="ctrl.html">CTRL</a></li>
<li class="toctree-l1"><a class="reference internal" href="camembert.html">CamemBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="albert.html">ALBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="xlmroberta.html">XLM-RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="flaubert.html">FlauBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="bart.html">Bart</a></li>
<li class="toctree-l1"><a class="reference internal" href="t5.html">T5</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">transformers</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>BERT</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/model_doc/bert.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="bert">
<h1>BERT<a class="headerlink" href="#bert" title="Permalink to this headline">¶</a></h1>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>The BERT model was proposed in <a class="reference external" href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a>
by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova. It’s a bidirectional transformer
pre-trained using a combination of masked language modeling objective and next sentence prediction
on a large corpus comprising the Toronto Book Corpus and Wikipedia.</p>
<p>The abstract from the paper is the following:</p>
<p><em>We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations
from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional
representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result,
the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models
for a wide range of tasks, such as question answering and language inference, without substantial task-specific
architecture modifications.</em></p>
<p><em>BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural
language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI
accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute
improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).</em></p>
<p>Tips:</p>
<ul class="simple">
<li>BERT is a model with absolute position embeddings so it’s usually advised to pad the inputs on
the right rather than the left.</li>
<li>BERT was trained with a masked language modeling (MLM) objective. It is therefore efficient at predicting masked
tokens and at NLU in general, but is not optimal for text generation. Models trained with a causal language
modeling (CLM) objective are better in that regard.</li>
<li>Alongside MLM, BERT was trained using a next sentence prediction (NSP) objective using the [CLS] token as a sequence
approximate. The user may use this token (the first token in a sequence built with special tokens) to get a sequence
prediction rather than a token prediction. However, averaging over the sequence may yield better results than using
the [CLS] token.</li>
</ul>
</div>
<div class="section" id="bertconfig">
<h2>BertConfig<a class="headerlink" href="#bertconfig" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="transformers.BertConfig">
<em class="property">class </em><code class="descclassname">transformers.</code><code class="descname">BertConfig</code><span class="sig-paren">(</span><em>vocab_size=30522</em>, <em>hidden_size=768</em>, <em>num_hidden_layers=12</em>, <em>num_attention_heads=12</em>, <em>intermediate_size=3072</em>, <em>hidden_act='gelu'</em>, <em>hidden_dropout_prob=0.1</em>, <em>attention_probs_dropout_prob=0.1</em>, <em>max_position_embeddings=512</em>, <em>type_vocab_size=2</em>, <em>initializer_range=0.02</em>, <em>layer_norm_eps=1e-12</em>, <em>pad_token_id=0</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/configuration_bert.html#BertConfig"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.BertConfig" title="Permalink to this definition">¶</a></dt>
<dd><p>This is the configuration class to store the configuration of a <a class="reference internal" href="#transformers.BertModel" title="transformers.BertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertModel</span></code></a>.
It is used to instantiate an BERT model according to the specified arguments, defining the model
architecture. Instantiating a configuration with the defaults will yield a similar configuration to that of
the BERT <a class="reference external" href="https://huggingface.co/bert-base-uncased">bert-base-uncased</a> architecture.</p>
<p>Configuration objects inherit from  <a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a> and can be used
to control the model outputs. Read the documentation from  <a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>
for more information.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>vocab_size</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional, defaults to 30522) – Vocabulary size of the BERT model. Defines the different tokens that
can be represented by the <cite>inputs_ids</cite> passed to the forward method of <a class="reference internal" href="#transformers.BertModel" title="transformers.BertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertModel</span></code></a>.</li>
<li><strong>hidden_size</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional, defaults to 768) – Dimensionality of the encoder layers and the pooler layer.</li>
<li><strong>num_hidden_layers</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional, defaults to 12) – Number of hidden layers in the Transformer encoder.</li>
<li><strong>num_attention_heads</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional, defaults to 12) – Number of attention heads for each attention layer in the Transformer encoder.</li>
<li><strong>intermediate_size</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional, defaults to 3072) – Dimensionality of the “intermediate” (i.e., feed-forward) layer in the Transformer encoder.</li>
<li><strong>hidden_act</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">function</span></code>, optional, defaults to “gelu”) – The non-linear activation function (function or string) in the encoder and pooler.
If string, “gelu”, “relu”, “swish” and “gelu_new” are supported.</li>
<li><strong>hidden_dropout_prob</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, optional, defaults to 0.1) – The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.</li>
<li><strong>attention_probs_dropout_prob</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, optional, defaults to 0.1) – The dropout ratio for the attention probabilities.</li>
<li><strong>max_position_embeddings</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional, defaults to 512) – The maximum sequence length that this model might ever be used with.
Typically set this to something large just in case (e.g., 512 or 1024 or 2048).</li>
<li><strong>type_vocab_size</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional, defaults to 2) – The vocabulary size of the <cite>token_type_ids</cite> passed into <a class="reference internal" href="#transformers.BertModel" title="transformers.BertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertModel</span></code></a>.</li>
<li><strong>initializer_range</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, optional, defaults to 0.02) – The standard deviation of the truncated_normal_initializer for initializing all weight matrices.</li>
<li><strong>layer_norm_eps</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, optional, defaults to 1e-12) – The epsilon used by the layer normalization layers.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="k">import</span> <span class="n">BertModel</span><span class="p">,</span> <span class="n">BertConfig</span>

<span class="c1"># Initializing a BERT bert-base-uncased style configuration</span>
<span class="n">configuration</span> <span class="o">=</span> <span class="n">BertConfig</span><span class="p">()</span>

<span class="c1"># Initializing a model from the bert-base-uncased style configuration</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="p">(</span><span class="n">configuration</span><span class="p">)</span>

<span class="c1"># Accessing the model configuration</span>
<span class="n">configuration</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span>
</pre></div>
</div>
<dl class="attribute">
<dt id="transformers.BertConfig.pretrained_config_archive_map">
<code class="descname">pretrained_config_archive_map</code><a class="headerlink" href="#transformers.BertConfig.pretrained_config_archive_map" title="Permalink to this definition">¶</a></dt>
<dd><p>A dictionary containing all the available pre-trained checkpoints.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body">Dict[str, str]</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="berttokenizer">
<h2>BertTokenizer<a class="headerlink" href="#berttokenizer" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="transformers.BertTokenizer">
<em class="property">class </em><code class="descclassname">transformers.</code><code class="descname">BertTokenizer</code><span class="sig-paren">(</span><em>vocab_file</em>, <em>do_lower_case=True</em>, <em>do_basic_tokenize=True</em>, <em>never_split=None</em>, <em>unk_token='[UNK]'</em>, <em>sep_token='[SEP]'</em>, <em>pad_token='[PAD]'</em>, <em>cls_token='[CLS]'</em>, <em>mask_token='[MASK]'</em>, <em>tokenize_chinese_chars=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/tokenization_bert.html#BertTokenizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.BertTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Constructs a BERT tokenizer. Based on WordPiece.</p>
<p>This tokenizer inherits from <a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer" title="transformers.PreTrainedTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizer</span></code></a> which contains most of the methods. Users
should refer to the superclass for more information regarding methods.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>vocab_file</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">string</span></code>) – File containing the vocabulary.</li>
<li><strong>do_lower_case</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>) – Whether to lowercase the input when tokenizing.</li>
<li><strong>do_basic_tokenize</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>) – Whether to do basic tokenization before WordPiece.</li>
<li><strong>never_split</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>) – List of tokens which will never be split during tokenization. Only has an effect when
<code class="xref py py-obj docutils literal notranslate"><span class="pre">do_basic_tokenize=True</span></code></li>
<li><strong>unk_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">string</span></code>, <cite>optional</cite>, defaults to “[UNK]”) – The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.</li>
<li><strong>sep_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">string</span></code>, <cite>optional</cite>, defaults to “[SEP]”) – The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences
for sequence classification or for a text and a question for question answering.
It is also used as the last token of a sequence built with special tokens.</li>
<li><strong>pad_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">string</span></code>, <cite>optional</cite>, defaults to “[PAD]”) – The token used for padding, for example when batching sequences of different lengths.</li>
<li><strong>cls_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">string</span></code>, <cite>optional</cite>, defaults to “[CLS]”) – The classifier token which is used when doing sequence classification (classification of the whole
sequence instead of per-token classification). It is the first token of the sequence when built with
special tokens.</li>
<li><strong>mask_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">string</span></code>, <cite>optional</cite>, defaults to “[MASK]”) – The token used for masking values. This is the token used when training this model with masked language
modeling. This is the token which the model will try to predict.</li>
<li><strong>tokenize_chinese_chars</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>) – Whether to tokenize Chinese characters.
This should likely be deactivated for Japanese:
see: <a class="reference external" href="https://github.com/huggingface/transformers/issues/328">https://github.com/huggingface/transformers/issues/328</a></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="transformers.BertTokenizer.build_inputs_with_special_tokens">
<code class="descname">build_inputs_with_special_tokens</code><span class="sig-paren">(</span><em>token_ids_0: List[int], token_ids_1: Optional[List[int]] = None</em><span class="sig-paren">)</span> &#x2192; List[int]<a class="reference internal" href="../_modules/transformers/tokenization_bert.html#BertTokenizer.build_inputs_with_special_tokens"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.BertTokenizer.build_inputs_with_special_tokens" title="Permalink to this definition">¶</a></dt>
<dd><p>Build model inputs from a sequence or a pair of sequence for sequence classification tasks
by concatenating and adding special tokens.
A BERT sequence has the following format:</p>
<ul class="simple">
<li>single sequence: <code class="docutils literal notranslate"><span class="pre">[CLS]</span> <span class="pre">X</span> <span class="pre">[SEP]</span></code></li>
<li>pair of sequences: <code class="docutils literal notranslate"><span class="pre">[CLS]</span> <span class="pre">A</span> <span class="pre">[SEP]</span> <span class="pre">B</span> <span class="pre">[SEP]</span></code></li>
</ul>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>token_ids_0</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>) – List of IDs to which the special tokens will be added</li>
<li><strong>token_ids_1</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Optional second list of IDs for sequence pairs.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">list of <a class="reference external" href="../glossary.html#input-ids">input IDs</a> with the appropriate special tokens.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="transformers.BertTokenizer.create_token_type_ids_from_sequences">
<code class="descname">create_token_type_ids_from_sequences</code><span class="sig-paren">(</span><em>token_ids_0: List[int], token_ids_1: Optional[List[int]] = None</em><span class="sig-paren">)</span> &#x2192; List[int]<a class="reference internal" href="../_modules/transformers/tokenization_bert.html#BertTokenizer.create_token_type_ids_from_sequences"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.BertTokenizer.create_token_type_ids_from_sequences" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a mask from the two sequences passed to be used in a sequence-pair classification task.
A BERT sequence pair mask has the following format:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span>
<span class="o">|</span> <span class="n">first</span> <span class="n">sequence</span>    <span class="o">|</span> <span class="n">second</span> <span class="n">sequence</span> <span class="o">|</span>
</pre></div>
</div>
<p>if token_ids_1 is None, only returns the first portion of the mask (0’s).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>token_ids_0</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>) – List of ids.</li>
<li><strong>token_ids_1</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Optional second list of IDs for sequence pairs.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">List of <a class="reference external" href="../glossary.html#token-type-ids">token type IDs</a> according to the given
sequence(s).</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="transformers.BertTokenizer.get_special_tokens_mask">
<code class="descname">get_special_tokens_mask</code><span class="sig-paren">(</span><em>token_ids_0: List[int], token_ids_1: Optional[List[int]] = None, already_has_special_tokens: bool = False</em><span class="sig-paren">)</span> &#x2192; List[int]<a class="reference internal" href="../_modules/transformers/tokenization_bert.html#BertTokenizer.get_special_tokens_mask"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.BertTokenizer.get_special_tokens_mask" title="Permalink to this definition">¶</a></dt>
<dd><p>Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer <code class="docutils literal notranslate"><span class="pre">prepare_for_model</span></code> or <code class="docutils literal notranslate"><span class="pre">encode_plus</span></code> methods.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>token_ids_0</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>) – List of ids.</li>
<li><strong>token_ids_1</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Optional second list of IDs for sequence pairs.</li>
<li><strong>already_has_special_tokens</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – Set to True if the token list is already formatted with special tokens for the model</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">A list of integers in the range [0, 1]: 0 for a special token, 1 for a sequence token.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="transformers.BertTokenizer.save_vocabulary">
<code class="descname">save_vocabulary</code><span class="sig-paren">(</span><em>vocab_path</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/tokenization_bert.html#BertTokenizer.save_vocabulary"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.BertTokenizer.save_vocabulary" title="Permalink to this definition">¶</a></dt>
<dd><p>Save the sentencepiece vocabulary (copy original file) and special tokens file to a directory.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>vocab_path</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>) – The directory in which to save the vocabulary.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">Paths to the files saved.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple(str)</span></code></td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="bertmodel">
<h2>BertModel<a class="headerlink" href="#bertmodel" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="transformers.BertModel">
<em class="property">class </em><code class="descclassname">transformers.</code><code class="descname">BertModel</code><span class="sig-paren">(</span><em>config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_bert.html#BertModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.BertModel" title="Permalink to this definition">¶</a></dt>
<dd><p>The bare Bert Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module">torch.nn.Module</a> sub-class.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general
usage and behavior.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>config</strong> (<a class="reference internal" href="#transformers.BertConfig" title="transformers.BertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertConfig</span></code></a>) – Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the configuration.
Check out the <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> method to load the model weights.</td>
</tr>
</tbody>
</table>
<p>The model can behave as an encoder (with only self-attention) as well
as a decoder, in which case a layer of cross-attention is added between
the self-attention layers, following the architecture described in <a class="reference external" href="https://arxiv.org/abs/1706.03762">Attention is all you need</a> by Ashish Vaswani,
Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.</p>
<p>To behave as an decoder the model needs to be initialized with the
<code class="xref py py-obj docutils literal notranslate"><span class="pre">is_decoder</span></code> argument of the configuration set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>; an
<code class="xref py py-obj docutils literal notranslate"><span class="pre">encoder_hidden_states</span></code> is expected as an input to the forward pass.</p>
<dl class="method">
<dt id="transformers.BertModel.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input_ids=None</em>, <em>attention_mask=None</em>, <em>token_type_ids=None</em>, <em>position_ids=None</em>, <em>head_mask=None</em>, <em>inputs_embeds=None</em>, <em>encoder_hidden_states=None</em>, <em>encoder_attention_mask=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_bert.html#BertModel.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.BertModel.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#transformers.BertModel" title="transformers.BertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertModel</span></code></a> forward method, overrides the <code class="xref py py-func docutils literal notranslate"><span class="pre">__call__()</span></code> special method.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
pre and post processing steps while the latter silently ignores them.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>) – <p>Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a class="reference internal" href="#transformers.BertTokenizer" title="transformers.BertTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.BertTokenizer</span></code></a>.
See <a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.encode" title="transformers.PreTrainedTokenizer.encode"><code class="xref py py-func docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.encode()</span></code></a> and
<a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.encode_plus" title="transformers.PreTrainedTokenizer.encode_plus"><code class="xref py py-func docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.encode_plus()</span></code></a> for details.</p>
<p><a class="reference external" href="../glossary.html#input-ids">What are input IDs?</a></p>
</li>
<li><strong>attention_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – <p>Mask to avoid performing attention on padding token indices.
Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:
<code class="docutils literal notranslate"><span class="pre">1</span></code> for tokens that are NOT MASKED, <code class="docutils literal notranslate"><span class="pre">0</span></code> for MASKED tokens.</p>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</li>
<li><strong>token_type_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – <p>Segment token indices to indicate first and second portions of the inputs.
Indices are selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>: <code class="docutils literal notranslate"><span class="pre">0</span></code> corresponds to a <cite>sentence A</cite> token, <code class="docutils literal notranslate"><span class="pre">1</span></code>
corresponds to a <cite>sentence B</cite> token</p>
<p><a class="reference external" href="../glossary.html#token-type-ids">What are token type IDs?</a></p>
</li>
<li><strong>position_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – <p>Indices of positions of each input sequence tokens in the position embeddings.
Selected in the range <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">config.max_position_embeddings</span> <span class="pre">-</span> <span class="pre">1]</span></code>.</p>
<p><a class="reference external" href="../glossary.html#position-ids">What are position IDs?</a></p>
</li>
<li><strong>head_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_heads,)</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_layers,</span> <span class="pre">num_heads)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Mask to nullify selected heads of the self-attention modules.
Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:
<code class="xref py py-obj docutils literal notranslate"><span class="pre">1</span></code> indicates the head is <strong>not masked</strong>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">0</span></code> indicates the head is <strong>masked</strong>.</li>
<li><strong>inputs_embeds</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Optionally, instead of passing <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <cite>input_ids</cite> indices into associated vectors
than the model’s internal embedding lookup matrix.</li>
<li><strong>encoder_hidden_states</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention
if the model is configured as a decoder.</li>
<li><strong>encoder_attention_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Mask to avoid performing attention on the padding token indices of the encoder input. This mask
is used in the cross-attention if the model is configured as a decoder.
Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:
<code class="docutils literal notranslate"><span class="pre">1</span></code> for tokens that are NOT MASKED, <code class="docutils literal notranslate"><span class="pre">0</span></code> for MASKED tokens.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><dl class="docutils">
<dt>last_hidden_state (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>):</dt>
<dd><p class="first last">Sequence of hidden-states at the output of the last layer of the model.</p>
</dd>
<dt>pooler_output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>: of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">hidden_size)</span></code>):</dt>
<dd><p class="first">Last layer hidden-state of the first token of the sequence (classification token)
further processed by a Linear layer and a Tanh activation function. The Linear
layer weights are trained from the next sentence prediction (classification)
objective during pre-training.</p>
<p class="last">This output is usually <em>not</em> a good summary
of the semantic content of the input, you’re often better with averaging or pooling
the sequence of hidden-states for the whole input sequence.</p>
</dd>
<dt>hidden_states (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">config.output_hidden_states=True</span></code>):</dt>
<dd><p class="first">Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for the output of the embeddings + one for the output of each layer)
of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>.</p>
<p class="last">Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</dd>
<dt>attentions (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">config.output_attentions=True</span></code>):</dt>
<dd><p class="first">Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for each layer) of shape
<code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_heads,</span> <span class="pre">sequence_length,</span> <span class="pre">sequence_length)</span></code>.</p>
<p class="last">Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</dd>
</dl>
</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code> comprising various elements depending on the configuration (<a class="reference internal" href="#transformers.BertConfig" title="transformers.BertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertConfig</span></code></a>) and inputs</p>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="k">import</span> <span class="n">BertModel</span><span class="p">,</span> <span class="n">BertTokenizer</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>

<span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">&quot;Hello, my dog is cute&quot;</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Batch size 1</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>

<span class="n">last_hidden_states</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># The last hidden-state is the first element of the output tuple</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="transformers.BertModel.get_input_embeddings">
<code class="descname">get_input_embeddings</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_bert.html#BertModel.get_input_embeddings"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.BertModel.get_input_embeddings" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the model’s input embeddings.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">A torch module mapping vocabulary to hidden states.</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="transformers.BertModel.set_input_embeddings">
<code class="descname">set_input_embeddings</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_bert.html#BertModel.set_input_embeddings"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.BertModel.set_input_embeddings" title="Permalink to this definition">¶</a></dt>
<dd><p>Set model’s input embeddings</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>value</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code>) – A module mapping vocabulary to hidden states.</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="bertforpretraining">
<h2>BertForPreTraining<a class="headerlink" href="#bertforpretraining" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="transformers.BertForPreTraining">
<em class="property">class </em><code class="descclassname">transformers.</code><code class="descname">BertForPreTraining</code><span class="sig-paren">(</span><em>config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_bert.html#BertForPreTraining"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.BertForPreTraining" title="Permalink to this definition">¶</a></dt>
<dd><p>Bert Model with two heads on top as done during the pre-training: a <cite>masked language modeling</cite> head and
a <cite>next sentence prediction (classification)</cite> head.
This model is a PyTorch <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module">torch.nn.Module</a> sub-class.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general
usage and behavior.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>config</strong> (<a class="reference internal" href="#transformers.BertConfig" title="transformers.BertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertConfig</span></code></a>) – Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the configuration.
Check out the <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> method to load the model weights.</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="transformers.BertForPreTraining.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input_ids=None</em>, <em>attention_mask=None</em>, <em>token_type_ids=None</em>, <em>position_ids=None</em>, <em>head_mask=None</em>, <em>inputs_embeds=None</em>, <em>masked_lm_labels=None</em>, <em>next_sentence_label=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_bert.html#BertForPreTraining.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.BertForPreTraining.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#transformers.BertForPreTraining" title="transformers.BertForPreTraining"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertForPreTraining</span></code></a> forward method, overrides the <code class="xref py py-func docutils literal notranslate"><span class="pre">__call__()</span></code> special method.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
pre and post processing steps while the latter silently ignores them.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>) – <p>Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a class="reference internal" href="#transformers.BertTokenizer" title="transformers.BertTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.BertTokenizer</span></code></a>.
See <a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.encode" title="transformers.PreTrainedTokenizer.encode"><code class="xref py py-func docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.encode()</span></code></a> and
<a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.encode_plus" title="transformers.PreTrainedTokenizer.encode_plus"><code class="xref py py-func docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.encode_plus()</span></code></a> for details.</p>
<p><a class="reference external" href="../glossary.html#input-ids">What are input IDs?</a></p>
</li>
<li><strong>attention_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – <p>Mask to avoid performing attention on padding token indices.
Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:
<code class="docutils literal notranslate"><span class="pre">1</span></code> for tokens that are NOT MASKED, <code class="docutils literal notranslate"><span class="pre">0</span></code> for MASKED tokens.</p>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</li>
<li><strong>token_type_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – <p>Segment token indices to indicate first and second portions of the inputs.
Indices are selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>: <code class="docutils literal notranslate"><span class="pre">0</span></code> corresponds to a <cite>sentence A</cite> token, <code class="docutils literal notranslate"><span class="pre">1</span></code>
corresponds to a <cite>sentence B</cite> token</p>
<p><a class="reference external" href="../glossary.html#token-type-ids">What are token type IDs?</a></p>
</li>
<li><strong>position_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – <p>Indices of positions of each input sequence tokens in the position embeddings.
Selected in the range <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">config.max_position_embeddings</span> <span class="pre">-</span> <span class="pre">1]</span></code>.</p>
<p><a class="reference external" href="../glossary.html#position-ids">What are position IDs?</a></p>
</li>
<li><strong>head_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_heads,)</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_layers,</span> <span class="pre">num_heads)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Mask to nullify selected heads of the self-attention modules.
Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:
<code class="xref py py-obj docutils literal notranslate"><span class="pre">1</span></code> indicates the head is <strong>not masked</strong>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">0</span></code> indicates the head is <strong>masked</strong>.</li>
<li><strong>inputs_embeds</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Optionally, instead of passing <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <cite>input_ids</cite> indices into associated vectors
than the model’s internal embedding lookup matrix.</li>
<li><strong>encoder_hidden_states</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention
if the model is configured as a decoder.</li>
<li><strong>encoder_attention_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Mask to avoid performing attention on the padding token indices of the encoder input. This mask
is used in the cross-attention if the model is configured as a decoder.
Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:
<code class="docutils literal notranslate"><span class="pre">1</span></code> for tokens that are NOT MASKED, <code class="docutils literal notranslate"><span class="pre">0</span></code> for MASKED tokens.</li>
<li><strong>masked_lm_labels</strong> (<code class="docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Labels for computing the masked language modeling loss.
Indices should be in <code class="docutils literal notranslate"><span class="pre">[-100,</span> <span class="pre">0,</span> <span class="pre">...,</span> <span class="pre">config.vocab_size]</span></code> (see <code class="docutils literal notranslate"><span class="pre">input_ids</span></code> docstring)
Tokens with indices set to <code class="docutils literal notranslate"><span class="pre">-100</span></code> are ignored (masked), the loss is only computed for the tokens with labels
in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">...,</span> <span class="pre">config.vocab_size]</span></code></li>
<li><strong>next_sentence_label</strong> (<code class="docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="docutils literal notranslate"><span class="pre">(batch_size,)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Labels for computing the next sequence prediction (classification) loss. Input should be a sequence pair (see <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> docstring)
Indices should be in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>.
<code class="docutils literal notranslate"><span class="pre">0</span></code> indicates sequence B is a continuation of sequence A,
<code class="docutils literal notranslate"><span class="pre">1</span></code> indicates sequence B is a random sequence.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><dl class="docutils">
<dt>loss (<cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">masked_lm_labels</span></code> is provided) <code class="docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="docutils literal notranslate"><span class="pre">(1,)</span></code>:</dt>
<dd><p class="first last">Total loss as the sum of the masked language modeling loss and the next sequence prediction (classification) loss.</p>
</dd>
<dt>prediction_scores (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">config.vocab_size)</span></code>)</dt>
<dd><p class="first last">Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</dd>
<dt>seq_relationship_scores (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">2)</span></code>):</dt>
<dd><p class="first last">Prediction scores of the next sequence prediction (classification) head (scores of True/False
continuation before SoftMax).</p>
</dd>
<dt>hidden_states (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="xref py py-obj docutils literal notranslate"><span class="pre">config.output_hidden_states=True</span></code>):</dt>
<dd><p class="first">Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for the output of the embeddings + one for the output of each layer)
of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>.</p>
<p class="last">Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</dd>
<dt>attentions (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">config.output_attentions=True</span></code>):</dt>
<dd><p class="first">Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for each layer) of shape
<code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_heads,</span> <span class="pre">sequence_length,</span> <span class="pre">sequence_length)</span></code>.</p>
<p class="last">Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</dd>
</dl>
</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code> comprising various elements depending on the configuration (<a class="reference internal" href="#transformers.BertConfig" title="transformers.BertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertConfig</span></code></a>) and inputs</p>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="k">import</span> <span class="n">BertTokenizer</span><span class="p">,</span> <span class="n">BertForPreTraining</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BertForPreTraining</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>

<span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">&quot;Hello, my dog is cute&quot;</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Batch size 1</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>

<span class="n">prediction_scores</span><span class="p">,</span> <span class="n">seq_relationship_scores</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="transformers.BertForPreTraining.get_output_embeddings">
<code class="descname">get_output_embeddings</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_bert.html#BertForPreTraining.get_output_embeddings"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.BertForPreTraining.get_output_embeddings" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the model’s output embeddings.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">A torch module mapping hidden states to vocabulary.</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code></td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="bertformaskedlm">
<h2>BertForMaskedLM<a class="headerlink" href="#bertformaskedlm" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="transformers.BertForMaskedLM">
<em class="property">class </em><code class="descclassname">transformers.</code><code class="descname">BertForMaskedLM</code><span class="sig-paren">(</span><em>config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_bert.html#BertForMaskedLM"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.BertForMaskedLM" title="Permalink to this definition">¶</a></dt>
<dd><p>Bert Model with a <cite>language modeling</cite> head on top.
This model is a PyTorch <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module">torch.nn.Module</a> sub-class.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general
usage and behavior.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>config</strong> (<a class="reference internal" href="#transformers.BertConfig" title="transformers.BertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertConfig</span></code></a>) – Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the configuration.
Check out the <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> method to load the model weights.</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="transformers.BertForMaskedLM.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input_ids=None</em>, <em>attention_mask=None</em>, <em>token_type_ids=None</em>, <em>position_ids=None</em>, <em>head_mask=None</em>, <em>inputs_embeds=None</em>, <em>masked_lm_labels=None</em>, <em>encoder_hidden_states=None</em>, <em>encoder_attention_mask=None</em>, <em>lm_labels=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_bert.html#BertForMaskedLM.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.BertForMaskedLM.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#transformers.BertForMaskedLM" title="transformers.BertForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertForMaskedLM</span></code></a> forward method, overrides the <code class="xref py py-func docutils literal notranslate"><span class="pre">__call__()</span></code> special method.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
pre and post processing steps while the latter silently ignores them.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>) – <p>Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a class="reference internal" href="#transformers.BertTokenizer" title="transformers.BertTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.BertTokenizer</span></code></a>.
See <a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.encode" title="transformers.PreTrainedTokenizer.encode"><code class="xref py py-func docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.encode()</span></code></a> and
<a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.encode_plus" title="transformers.PreTrainedTokenizer.encode_plus"><code class="xref py py-func docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.encode_plus()</span></code></a> for details.</p>
<p><a class="reference external" href="../glossary.html#input-ids">What are input IDs?</a></p>
</li>
<li><strong>attention_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – <p>Mask to avoid performing attention on padding token indices.
Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:
<code class="docutils literal notranslate"><span class="pre">1</span></code> for tokens that are NOT MASKED, <code class="docutils literal notranslate"><span class="pre">0</span></code> for MASKED tokens.</p>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</li>
<li><strong>token_type_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – <p>Segment token indices to indicate first and second portions of the inputs.
Indices are selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>: <code class="docutils literal notranslate"><span class="pre">0</span></code> corresponds to a <cite>sentence A</cite> token, <code class="docutils literal notranslate"><span class="pre">1</span></code>
corresponds to a <cite>sentence B</cite> token</p>
<p><a class="reference external" href="../glossary.html#token-type-ids">What are token type IDs?</a></p>
</li>
<li><strong>position_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – <p>Indices of positions of each input sequence tokens in the position embeddings.
Selected in the range <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">config.max_position_embeddings</span> <span class="pre">-</span> <span class="pre">1]</span></code>.</p>
<p><a class="reference external" href="../glossary.html#position-ids">What are position IDs?</a></p>
</li>
<li><strong>head_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_heads,)</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_layers,</span> <span class="pre">num_heads)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Mask to nullify selected heads of the self-attention modules.
Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:
<code class="xref py py-obj docutils literal notranslate"><span class="pre">1</span></code> indicates the head is <strong>not masked</strong>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">0</span></code> indicates the head is <strong>masked</strong>.</li>
<li><strong>inputs_embeds</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Optionally, instead of passing <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <cite>input_ids</cite> indices into associated vectors
than the model’s internal embedding lookup matrix.</li>
<li><strong>encoder_hidden_states</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention
if the model is configured as a decoder.</li>
<li><strong>encoder_attention_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Mask to avoid performing attention on the padding token indices of the encoder input. This mask
is used in the cross-attention if the model is configured as a decoder.
Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:
<code class="docutils literal notranslate"><span class="pre">1</span></code> for tokens that are NOT MASKED, <code class="docutils literal notranslate"><span class="pre">0</span></code> for MASKED tokens.</li>
<li><strong>masked_lm_labels</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Labels for computing the masked language modeling loss.
Indices should be in <code class="docutils literal notranslate"><span class="pre">[-100,</span> <span class="pre">0,</span> <span class="pre">...,</span> <span class="pre">config.vocab_size]</span></code> (see <code class="docutils literal notranslate"><span class="pre">input_ids</span></code> docstring)
Tokens with indices set to <code class="docutils literal notranslate"><span class="pre">-100</span></code> are ignored (masked), the loss is only computed for the tokens with labels
in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">...,</span> <span class="pre">config.vocab_size]</span></code></li>
<li><strong>lm_labels</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Labels for computing the left-to-right language modeling loss (next word prediction).
Indices should be in <code class="docutils literal notranslate"><span class="pre">[-100,</span> <span class="pre">0,</span> <span class="pre">...,</span> <span class="pre">config.vocab_size]</span></code> (see <code class="docutils literal notranslate"><span class="pre">input_ids</span></code> docstring)
Tokens with indices set to <code class="docutils literal notranslate"><span class="pre">-100</span></code> are ignored (masked), the loss is only computed for the tokens with labels
in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">...,</span> <span class="pre">config.vocab_size]</span></code></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><dl class="docutils">
<dt>masked_lm_loss (<cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">masked_lm_labels</span></code> is provided) <code class="docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="docutils literal notranslate"><span class="pre">(1,)</span></code>:</dt>
<dd><p class="first last">Masked language modeling loss.</p>
</dd>
<dt>ltr_lm_loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(1,)</span></code>, <cite>optional</cite>, returned when <code class="xref py py-obj docutils literal notranslate"><span class="pre">lm_labels</span></code> is provided):</dt>
<dd><p class="first last">Next token prediction loss.</p>
</dd>
<dt>prediction_scores (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">config.vocab_size)</span></code>)</dt>
<dd><p class="first last">Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</dd>
<dt>hidden_states (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">config.output_hidden_states=True</span></code>):</dt>
<dd><p class="first">Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for the output of the embeddings + one for the output of each layer)
of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>.</p>
<p class="last">Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</dd>
<dt>attentions (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">config.output_attentions=True</span></code>):</dt>
<dd><p class="first">Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for each layer) of shape
<code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_heads,</span> <span class="pre">sequence_length,</span> <span class="pre">sequence_length)</span></code>.</p>
<p class="last">Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="k">import</span> <span class="n">BertTokenizer</span><span class="p">,</span> <span class="n">BertForMaskedLM</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BertForMaskedLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>

<span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">&quot;Hello, my dog is cute&quot;</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Batch size 1</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">masked_lm_labels</span><span class="o">=</span><span class="n">input_ids</span><span class="p">)</span>

<span class="n">loss</span><span class="p">,</span> <span class="n">prediction_scores</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
</pre></div>
</div>
</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code> comprising various elements depending on the configuration (<a class="reference internal" href="#transformers.BertConfig" title="transformers.BertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertConfig</span></code></a>) and inputs</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="transformers.BertForMaskedLM.get_output_embeddings">
<code class="descname">get_output_embeddings</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_bert.html#BertForMaskedLM.get_output_embeddings"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.BertForMaskedLM.get_output_embeddings" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the model’s output embeddings.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">A torch module mapping hidden states to vocabulary.</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code></td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="bertfornextsentenceprediction">
<h2>BertForNextSentencePrediction<a class="headerlink" href="#bertfornextsentenceprediction" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="transformers.BertForNextSentencePrediction">
<em class="property">class </em><code class="descclassname">transformers.</code><code class="descname">BertForNextSentencePrediction</code><span class="sig-paren">(</span><em>config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_bert.html#BertForNextSentencePrediction"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.BertForNextSentencePrediction" title="Permalink to this definition">¶</a></dt>
<dd><p>Bert Model with a <cite>next sentence prediction (classification)</cite> head on top.
This model is a PyTorch <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module">torch.nn.Module</a> sub-class.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general
usage and behavior.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>config</strong> (<a class="reference internal" href="#transformers.BertConfig" title="transformers.BertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertConfig</span></code></a>) – Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the configuration.
Check out the <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> method to load the model weights.</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="transformers.BertForNextSentencePrediction.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input_ids=None</em>, <em>attention_mask=None</em>, <em>token_type_ids=None</em>, <em>position_ids=None</em>, <em>head_mask=None</em>, <em>inputs_embeds=None</em>, <em>next_sentence_label=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_bert.html#BertForNextSentencePrediction.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.BertForNextSentencePrediction.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#transformers.BertForNextSentencePrediction" title="transformers.BertForNextSentencePrediction"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertForNextSentencePrediction</span></code></a> forward method, overrides the <code class="xref py py-func docutils literal notranslate"><span class="pre">__call__()</span></code> special method.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
pre and post processing steps while the latter silently ignores them.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>) – <p>Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a class="reference internal" href="#transformers.BertTokenizer" title="transformers.BertTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.BertTokenizer</span></code></a>.
See <a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.encode" title="transformers.PreTrainedTokenizer.encode"><code class="xref py py-func docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.encode()</span></code></a> and
<a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.encode_plus" title="transformers.PreTrainedTokenizer.encode_plus"><code class="xref py py-func docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.encode_plus()</span></code></a> for details.</p>
<p><a class="reference external" href="../glossary.html#input-ids">What are input IDs?</a></p>
</li>
<li><strong>attention_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – <p>Mask to avoid performing attention on padding token indices.
Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:
<code class="docutils literal notranslate"><span class="pre">1</span></code> for tokens that are NOT MASKED, <code class="docutils literal notranslate"><span class="pre">0</span></code> for MASKED tokens.</p>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</li>
<li><strong>token_type_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – <p>Segment token indices to indicate first and second portions of the inputs.
Indices are selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>: <code class="docutils literal notranslate"><span class="pre">0</span></code> corresponds to a <cite>sentence A</cite> token, <code class="docutils literal notranslate"><span class="pre">1</span></code>
corresponds to a <cite>sentence B</cite> token</p>
<p><a class="reference external" href="../glossary.html#token-type-ids">What are token type IDs?</a></p>
</li>
<li><strong>position_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – <p>Indices of positions of each input sequence tokens in the position embeddings.
Selected in the range <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">config.max_position_embeddings</span> <span class="pre">-</span> <span class="pre">1]</span></code>.</p>
<p><a class="reference external" href="../glossary.html#position-ids">What are position IDs?</a></p>
</li>
<li><strong>head_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_heads,)</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_layers,</span> <span class="pre">num_heads)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Mask to nullify selected heads of the self-attention modules.
Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:
<code class="xref py py-obj docutils literal notranslate"><span class="pre">1</span></code> indicates the head is <strong>not masked</strong>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">0</span></code> indicates the head is <strong>masked</strong>.</li>
<li><strong>inputs_embeds</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Optionally, instead of passing <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <cite>input_ids</cite> indices into associated vectors
than the model’s internal embedding lookup matrix.</li>
<li><strong>encoder_hidden_states</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention
if the model is configured as a decoder.</li>
<li><strong>encoder_attention_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Mask to avoid performing attention on the padding token indices of the encoder input. This mask
is used in the cross-attention if the model is configured as a decoder.
Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:
<code class="docutils literal notranslate"><span class="pre">1</span></code> for tokens that are NOT MASKED, <code class="docutils literal notranslate"><span class="pre">0</span></code> for MASKED tokens.</li>
<li><strong>next_sentence_label</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Labels for computing the next sequence prediction (classification) loss. Input should be a sequence pair (see <code class="docutils literal notranslate"><span class="pre">input_ids</span></code> docstring)
Indices should be in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>.
<code class="docutils literal notranslate"><span class="pre">0</span></code> indicates sequence B is a continuation of sequence A,
<code class="docutils literal notranslate"><span class="pre">1</span></code> indicates sequence B is a random sequence.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><dl class="docutils">
<dt>loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(1,)</span></code>, <cite>optional</cite>, returned when <code class="xref py py-obj docutils literal notranslate"><span class="pre">next_sentence_label</span></code> is provided):</dt>
<dd><p class="first last">Next sequence prediction (classification) loss.</p>
</dd>
<dt>seq_relationship_scores (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">2)</span></code>):</dt>
<dd><p class="first last">Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation before SoftMax).</p>
</dd>
<dt>hidden_states (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">config.output_hidden_states=True</span></code>):</dt>
<dd><p class="first">Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for the output of the embeddings + one for the output of each layer)
of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>.</p>
<p class="last">Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</dd>
<dt>attentions (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">config.output_attentions=True</span></code>):</dt>
<dd><p class="first">Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for each layer) of shape
<code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_heads,</span> <span class="pre">sequence_length,</span> <span class="pre">sequence_length)</span></code>.</p>
<p class="last">Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</dd>
</dl>
</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code> comprising various elements depending on the configuration (<a class="reference internal" href="#transformers.BertConfig" title="transformers.BertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertConfig</span></code></a>) and inputs</p>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="k">import</span> <span class="n">BertTokenizer</span><span class="p">,</span> <span class="n">BertForNextSentencePrediction</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BertForNextSentencePrediction</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>

<span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">&quot;Hello, my dog is cute&quot;</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Batch size 1</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>

<span class="n">seq_relationship_scores</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="bertforsequenceclassification">
<h2>BertForSequenceClassification<a class="headerlink" href="#bertforsequenceclassification" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="transformers.BertForSequenceClassification">
<em class="property">class </em><code class="descclassname">transformers.</code><code class="descname">BertForSequenceClassification</code><span class="sig-paren">(</span><em>config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_bert.html#BertForSequenceClassification"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.BertForSequenceClassification" title="Permalink to this definition">¶</a></dt>
<dd><p>Bert Model transformer with a sequence classification/regression head on top (a linear layer on top of
the pooled output) e.g. for GLUE tasks.
This model is a PyTorch <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module">torch.nn.Module</a> sub-class.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general
usage and behavior.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>config</strong> (<a class="reference internal" href="#transformers.BertConfig" title="transformers.BertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertConfig</span></code></a>) – Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the configuration.
Check out the <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> method to load the model weights.</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="transformers.BertForSequenceClassification.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input_ids=None</em>, <em>attention_mask=None</em>, <em>token_type_ids=None</em>, <em>position_ids=None</em>, <em>head_mask=None</em>, <em>inputs_embeds=None</em>, <em>labels=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_bert.html#BertForSequenceClassification.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.BertForSequenceClassification.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#transformers.BertForSequenceClassification" title="transformers.BertForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertForSequenceClassification</span></code></a> forward method, overrides the <code class="xref py py-func docutils literal notranslate"><span class="pre">__call__()</span></code> special method.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
pre and post processing steps while the latter silently ignores them.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>) – <p>Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a class="reference internal" href="#transformers.BertTokenizer" title="transformers.BertTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.BertTokenizer</span></code></a>.
See <a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.encode" title="transformers.PreTrainedTokenizer.encode"><code class="xref py py-func docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.encode()</span></code></a> and
<a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.encode_plus" title="transformers.PreTrainedTokenizer.encode_plus"><code class="xref py py-func docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.encode_plus()</span></code></a> for details.</p>
<p><a class="reference external" href="../glossary.html#input-ids">What are input IDs?</a></p>
</li>
<li><strong>attention_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – <p>Mask to avoid performing attention on padding token indices.
Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:
<code class="docutils literal notranslate"><span class="pre">1</span></code> for tokens that are NOT MASKED, <code class="docutils literal notranslate"><span class="pre">0</span></code> for MASKED tokens.</p>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</li>
<li><strong>token_type_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – <p>Segment token indices to indicate first and second portions of the inputs.
Indices are selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>: <code class="docutils literal notranslate"><span class="pre">0</span></code> corresponds to a <cite>sentence A</cite> token, <code class="docutils literal notranslate"><span class="pre">1</span></code>
corresponds to a <cite>sentence B</cite> token</p>
<p><a class="reference external" href="../glossary.html#token-type-ids">What are token type IDs?</a></p>
</li>
<li><strong>position_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – <p>Indices of positions of each input sequence tokens in the position embeddings.
Selected in the range <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">config.max_position_embeddings</span> <span class="pre">-</span> <span class="pre">1]</span></code>.</p>
<p><a class="reference external" href="../glossary.html#position-ids">What are position IDs?</a></p>
</li>
<li><strong>head_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_heads,)</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_layers,</span> <span class="pre">num_heads)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Mask to nullify selected heads of the self-attention modules.
Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:
<code class="xref py py-obj docutils literal notranslate"><span class="pre">1</span></code> indicates the head is <strong>not masked</strong>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">0</span></code> indicates the head is <strong>masked</strong>.</li>
<li><strong>inputs_embeds</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Optionally, instead of passing <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <cite>input_ids</cite> indices into associated vectors
than the model’s internal embedding lookup matrix.</li>
<li><strong>encoder_hidden_states</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention
if the model is configured as a decoder.</li>
<li><strong>encoder_attention_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Mask to avoid performing attention on the padding token indices of the encoder input. This mask
is used in the cross-attention if the model is configured as a decoder.
Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:
<code class="docutils literal notranslate"><span class="pre">1</span></code> for tokens that are NOT MASKED, <code class="docutils literal notranslate"><span class="pre">0</span></code> for MASKED tokens.</li>
<li><strong>labels</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Labels for computing the sequence classification/regression loss.
Indices should be in <code class="xref py py-obj docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">...,</span> <span class="pre">config.num_labels</span> <span class="pre">-</span> <span class="pre">1]</span></code>.
If <code class="xref py py-obj docutils literal notranslate"><span class="pre">config.num_labels</span> <span class="pre">==</span> <span class="pre">1</span></code> a regression loss is computed (Mean-Square loss),
If <code class="xref py py-obj docutils literal notranslate"><span class="pre">config.num_labels</span> <span class="pre">&gt;</span> <span class="pre">1</span></code> a classification loss is computed (Cross-Entropy).</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><dl class="docutils">
<dt>loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(1,)</span></code>, <cite>optional</cite>, returned when <code class="xref py py-obj docutils literal notranslate"><span class="pre">label</span></code> is provided):</dt>
<dd><p class="first last">Classification (or regression if config.num_labels==1) loss.</p>
</dd>
<dt>logits (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">config.num_labels)</span></code>):</dt>
<dd><p class="first last">Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</dd>
<dt>hidden_states (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">config.output_hidden_states=True</span></code>):</dt>
<dd><p class="first">Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for the output of the embeddings + one for the output of each layer)
of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>.</p>
<p class="last">Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</dd>
<dt>attentions (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">config.output_attentions=True</span></code>):</dt>
<dd><p class="first">Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for each layer) of shape
<code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_heads,</span> <span class="pre">sequence_length,</span> <span class="pre">sequence_length)</span></code>.</p>
<p class="last">Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</dd>
</dl>
</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code> comprising various elements depending on the configuration (<a class="reference internal" href="#transformers.BertConfig" title="transformers.BertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertConfig</span></code></a>) and inputs</p>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="k">import</span> <span class="n">BertTokenizer</span><span class="p">,</span> <span class="n">BertForSequenceClassification</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BertForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>

<span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">&quot;Hello, my dog is cute&quot;</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Batch size 1</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Batch size 1</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>

<span class="n">loss</span><span class="p">,</span> <span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="bertformultiplechoice">
<h2>BertForMultipleChoice<a class="headerlink" href="#bertformultiplechoice" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="transformers.BertForMultipleChoice">
<em class="property">class </em><code class="descclassname">transformers.</code><code class="descname">BertForMultipleChoice</code><span class="sig-paren">(</span><em>config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_bert.html#BertForMultipleChoice"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.BertForMultipleChoice" title="Permalink to this definition">¶</a></dt>
<dd><p>Bert Model with a multiple choice classification head on top (a linear layer on top of
the pooled output and a softmax) e.g. for RocStories/SWAG tasks.
This model is a PyTorch <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module">torch.nn.Module</a> sub-class.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general
usage and behavior.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>config</strong> (<a class="reference internal" href="#transformers.BertConfig" title="transformers.BertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertConfig</span></code></a>) – Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the configuration.
Check out the <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> method to load the model weights.</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="transformers.BertForMultipleChoice.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input_ids=None</em>, <em>attention_mask=None</em>, <em>token_type_ids=None</em>, <em>position_ids=None</em>, <em>head_mask=None</em>, <em>inputs_embeds=None</em>, <em>labels=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_bert.html#BertForMultipleChoice.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.BertForMultipleChoice.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#transformers.BertForMultipleChoice" title="transformers.BertForMultipleChoice"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertForMultipleChoice</span></code></a> forward method, overrides the <code class="xref py py-func docutils literal notranslate"><span class="pre">__call__()</span></code> special method.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
pre and post processing steps while the latter silently ignores them.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>) – <p>Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a class="reference internal" href="#transformers.BertTokenizer" title="transformers.BertTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.BertTokenizer</span></code></a>.
See <a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.encode" title="transformers.PreTrainedTokenizer.encode"><code class="xref py py-func docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.encode()</span></code></a> and
<a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.encode_plus" title="transformers.PreTrainedTokenizer.encode_plus"><code class="xref py py-func docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.encode_plus()</span></code></a> for details.</p>
<p><a class="reference external" href="../glossary.html#input-ids">What are input IDs?</a></p>
</li>
<li><strong>attention_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – <p>Mask to avoid performing attention on padding token indices.
Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:
<code class="docutils literal notranslate"><span class="pre">1</span></code> for tokens that are NOT MASKED, <code class="docutils literal notranslate"><span class="pre">0</span></code> for MASKED tokens.</p>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</li>
<li><strong>token_type_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – <p>Segment token indices to indicate first and second portions of the inputs.
Indices are selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>: <code class="docutils literal notranslate"><span class="pre">0</span></code> corresponds to a <cite>sentence A</cite> token, <code class="docutils literal notranslate"><span class="pre">1</span></code>
corresponds to a <cite>sentence B</cite> token</p>
<p><a class="reference external" href="../glossary.html#token-type-ids">What are token type IDs?</a></p>
</li>
<li><strong>position_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – <p>Indices of positions of each input sequence tokens in the position embeddings.
Selected in the range <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">config.max_position_embeddings</span> <span class="pre">-</span> <span class="pre">1]</span></code>.</p>
<p><a class="reference external" href="../glossary.html#position-ids">What are position IDs?</a></p>
</li>
<li><strong>head_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_heads,)</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_layers,</span> <span class="pre">num_heads)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Mask to nullify selected heads of the self-attention modules.
Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:
<code class="xref py py-obj docutils literal notranslate"><span class="pre">1</span></code> indicates the head is <strong>not masked</strong>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">0</span></code> indicates the head is <strong>masked</strong>.</li>
<li><strong>inputs_embeds</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Optionally, instead of passing <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <cite>input_ids</cite> indices into associated vectors
than the model’s internal embedding lookup matrix.</li>
<li><strong>encoder_hidden_states</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention
if the model is configured as a decoder.</li>
<li><strong>encoder_attention_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Mask to avoid performing attention on the padding token indices of the encoder input. This mask
is used in the cross-attention if the model is configured as a decoder.
Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:
<code class="docutils literal notranslate"><span class="pre">1</span></code> for tokens that are NOT MASKED, <code class="docutils literal notranslate"><span class="pre">0</span></code> for MASKED tokens.</li>
<li><strong>labels</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Labels for computing the multiple choice classification loss.
Indices should be in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">...,</span> <span class="pre">num_choices]</span></code> where <cite>num_choices</cite> is the size of the second dimension
of the input tensors. (see <cite>input_ids</cite> above)</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><dl class="docutils">
<dt>loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <cite>(1,)</cite>, <cite>optional</cite>, returned when <code class="xref py py-obj docutils literal notranslate"><span class="pre">labels</span></code> is provided):</dt>
<dd><p class="first last">Classification loss.</p>
</dd>
<dt>classification_scores (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_choices)</span></code>):</dt>
<dd><p class="first"><cite>num_choices</cite> is the second dimension of the input tensors. (see <cite>input_ids</cite> above).</p>
<p class="last">Classification scores (before SoftMax).</p>
</dd>
<dt>hidden_states (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">config.output_hidden_states=True</span></code>):</dt>
<dd><p class="first">Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for the output of the embeddings + one for the output of each layer)
of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>.</p>
<p class="last">Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</dd>
<dt>attentions (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">config.output_attentions=True</span></code>):</dt>
<dd><p class="first">Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for each layer) of shape
<code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_heads,</span> <span class="pre">sequence_length,</span> <span class="pre">sequence_length)</span></code>.</p>
<p class="last">Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</dd>
</dl>
</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code> comprising various elements depending on the configuration (<a class="reference internal" href="#transformers.BertConfig" title="transformers.BertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertConfig</span></code></a>) and inputs</p>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="k">import</span> <span class="n">BertTokenizer</span><span class="p">,</span> <span class="n">BertForMultipleChoice</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BertForMultipleChoice</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>
<span class="n">choices</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Hello, my dog is cute&quot;</span><span class="p">,</span> <span class="s2">&quot;Hello, my cat is amazing&quot;</span><span class="p">]</span>

<span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">choices</span><span class="p">])</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Batch size 1, 2 choices</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Batch size 1</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>

<span class="n">loss</span><span class="p">,</span> <span class="n">classification_scores</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="bertfortokenclassification">
<h2>BertForTokenClassification<a class="headerlink" href="#bertfortokenclassification" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="transformers.BertForTokenClassification">
<em class="property">class </em><code class="descclassname">transformers.</code><code class="descname">BertForTokenClassification</code><span class="sig-paren">(</span><em>config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_bert.html#BertForTokenClassification"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.BertForTokenClassification" title="Permalink to this definition">¶</a></dt>
<dd><p>Bert Model with a token classification head on top (a linear layer on top of
the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks.
This model is a PyTorch <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module">torch.nn.Module</a> sub-class.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general
usage and behavior.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>config</strong> (<a class="reference internal" href="#transformers.BertConfig" title="transformers.BertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertConfig</span></code></a>) – Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the configuration.
Check out the <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> method to load the model weights.</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="transformers.BertForTokenClassification.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input_ids=None</em>, <em>attention_mask=None</em>, <em>token_type_ids=None</em>, <em>position_ids=None</em>, <em>head_mask=None</em>, <em>inputs_embeds=None</em>, <em>labels=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_bert.html#BertForTokenClassification.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.BertForTokenClassification.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#transformers.BertForTokenClassification" title="transformers.BertForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertForTokenClassification</span></code></a> forward method, overrides the <code class="xref py py-func docutils literal notranslate"><span class="pre">__call__()</span></code> special method.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
pre and post processing steps while the latter silently ignores them.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>) – <p>Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a class="reference internal" href="#transformers.BertTokenizer" title="transformers.BertTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.BertTokenizer</span></code></a>.
See <a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.encode" title="transformers.PreTrainedTokenizer.encode"><code class="xref py py-func docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.encode()</span></code></a> and
<a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.encode_plus" title="transformers.PreTrainedTokenizer.encode_plus"><code class="xref py py-func docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.encode_plus()</span></code></a> for details.</p>
<p><a class="reference external" href="../glossary.html#input-ids">What are input IDs?</a></p>
</li>
<li><strong>attention_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – <p>Mask to avoid performing attention on padding token indices.
Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:
<code class="docutils literal notranslate"><span class="pre">1</span></code> for tokens that are NOT MASKED, <code class="docutils literal notranslate"><span class="pre">0</span></code> for MASKED tokens.</p>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</li>
<li><strong>token_type_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – <p>Segment token indices to indicate first and second portions of the inputs.
Indices are selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>: <code class="docutils literal notranslate"><span class="pre">0</span></code> corresponds to a <cite>sentence A</cite> token, <code class="docutils literal notranslate"><span class="pre">1</span></code>
corresponds to a <cite>sentence B</cite> token</p>
<p><a class="reference external" href="../glossary.html#token-type-ids">What are token type IDs?</a></p>
</li>
<li><strong>position_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – <p>Indices of positions of each input sequence tokens in the position embeddings.
Selected in the range <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">config.max_position_embeddings</span> <span class="pre">-</span> <span class="pre">1]</span></code>.</p>
<p><a class="reference external" href="../glossary.html#position-ids">What are position IDs?</a></p>
</li>
<li><strong>head_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_heads,)</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_layers,</span> <span class="pre">num_heads)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Mask to nullify selected heads of the self-attention modules.
Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:
<code class="xref py py-obj docutils literal notranslate"><span class="pre">1</span></code> indicates the head is <strong>not masked</strong>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">0</span></code> indicates the head is <strong>masked</strong>.</li>
<li><strong>inputs_embeds</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Optionally, instead of passing <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <cite>input_ids</cite> indices into associated vectors
than the model’s internal embedding lookup matrix.</li>
<li><strong>encoder_hidden_states</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention
if the model is configured as a decoder.</li>
<li><strong>encoder_attention_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Mask to avoid performing attention on the padding token indices of the encoder input. This mask
is used in the cross-attention if the model is configured as a decoder.
Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:
<code class="docutils literal notranslate"><span class="pre">1</span></code> for tokens that are NOT MASKED, <code class="docutils literal notranslate"><span class="pre">0</span></code> for MASKED tokens.</li>
<li><strong>labels</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Labels for computing the token classification loss.
Indices should be in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">...,</span> <span class="pre">config.num_labels</span> <span class="pre">-</span> <span class="pre">1]</span></code>.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><dl class="docutils">
<dt>loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(1,)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">labels</span></code> is provided) :</dt>
<dd><p class="first last">Classification loss.</p>
</dd>
<dt>scores (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">config.num_labels)</span></code>)</dt>
<dd><p class="first last">Classification scores (before SoftMax).</p>
</dd>
<dt>hidden_states (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">config.output_hidden_states=True</span></code>):</dt>
<dd><p class="first">Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for the output of the embeddings + one for the output of each layer)
of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>.</p>
<p class="last">Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</dd>
<dt>attentions (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">config.output_attentions=True</span></code>):</dt>
<dd><p class="first">Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for each layer) of shape
<code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_heads,</span> <span class="pre">sequence_length,</span> <span class="pre">sequence_length)</span></code>.</p>
<p class="last">Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</dd>
</dl>
</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code> comprising various elements depending on the configuration (<a class="reference internal" href="#transformers.BertConfig" title="transformers.BertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertConfig</span></code></a>) and inputs</p>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="k">import</span> <span class="n">BertTokenizer</span><span class="p">,</span> <span class="n">BertForTokenClassification</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BertForTokenClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>

<span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">&quot;Hello, my dog is cute&quot;</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Batch size 1</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Batch size 1</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>

<span class="n">loss</span><span class="p">,</span> <span class="n">scores</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="bertforquestionanswering">
<h2>BertForQuestionAnswering<a class="headerlink" href="#bertforquestionanswering" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="transformers.BertForQuestionAnswering">
<em class="property">class </em><code class="descclassname">transformers.</code><code class="descname">BertForQuestionAnswering</code><span class="sig-paren">(</span><em>config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_bert.html#BertForQuestionAnswering"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.BertForQuestionAnswering" title="Permalink to this definition">¶</a></dt>
<dd><p>Bert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear
layers on top of the hidden-states output to compute <cite>span start logits</cite> and <cite>span end logits</cite>).
This model is a PyTorch <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module">torch.nn.Module</a> sub-class.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general
usage and behavior.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>config</strong> (<a class="reference internal" href="#transformers.BertConfig" title="transformers.BertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertConfig</span></code></a>) – Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the configuration.
Check out the <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> method to load the model weights.</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="transformers.BertForQuestionAnswering.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input_ids=None</em>, <em>attention_mask=None</em>, <em>token_type_ids=None</em>, <em>position_ids=None</em>, <em>head_mask=None</em>, <em>inputs_embeds=None</em>, <em>start_positions=None</em>, <em>end_positions=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_bert.html#BertForQuestionAnswering.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.BertForQuestionAnswering.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#transformers.BertForQuestionAnswering" title="transformers.BertForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertForQuestionAnswering</span></code></a> forward method, overrides the <code class="xref py py-func docutils literal notranslate"><span class="pre">__call__()</span></code> special method.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
pre and post processing steps while the latter silently ignores them.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>) – <p>Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a class="reference internal" href="#transformers.BertTokenizer" title="transformers.BertTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.BertTokenizer</span></code></a>.
See <a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.encode" title="transformers.PreTrainedTokenizer.encode"><code class="xref py py-func docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.encode()</span></code></a> and
<a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.encode_plus" title="transformers.PreTrainedTokenizer.encode_plus"><code class="xref py py-func docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.encode_plus()</span></code></a> for details.</p>
<p><a class="reference external" href="../glossary.html#input-ids">What are input IDs?</a></p>
</li>
<li><strong>attention_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – <p>Mask to avoid performing attention on padding token indices.
Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:
<code class="docutils literal notranslate"><span class="pre">1</span></code> for tokens that are NOT MASKED, <code class="docutils literal notranslate"><span class="pre">0</span></code> for MASKED tokens.</p>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</li>
<li><strong>token_type_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – <p>Segment token indices to indicate first and second portions of the inputs.
Indices are selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>: <code class="docutils literal notranslate"><span class="pre">0</span></code> corresponds to a <cite>sentence A</cite> token, <code class="docutils literal notranslate"><span class="pre">1</span></code>
corresponds to a <cite>sentence B</cite> token</p>
<p><a class="reference external" href="../glossary.html#token-type-ids">What are token type IDs?</a></p>
</li>
<li><strong>position_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – <p>Indices of positions of each input sequence tokens in the position embeddings.
Selected in the range <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">config.max_position_embeddings</span> <span class="pre">-</span> <span class="pre">1]</span></code>.</p>
<p><a class="reference external" href="../glossary.html#position-ids">What are position IDs?</a></p>
</li>
<li><strong>head_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_heads,)</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_layers,</span> <span class="pre">num_heads)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Mask to nullify selected heads of the self-attention modules.
Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:
<code class="xref py py-obj docutils literal notranslate"><span class="pre">1</span></code> indicates the head is <strong>not masked</strong>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">0</span></code> indicates the head is <strong>masked</strong>.</li>
<li><strong>inputs_embeds</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Optionally, instead of passing <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <cite>input_ids</cite> indices into associated vectors
than the model’s internal embedding lookup matrix.</li>
<li><strong>encoder_hidden_states</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention
if the model is configured as a decoder.</li>
<li><strong>encoder_attention_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Mask to avoid performing attention on the padding token indices of the encoder input. This mask
is used in the cross-attention if the model is configured as a decoder.
Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:
<code class="docutils literal notranslate"><span class="pre">1</span></code> for tokens that are NOT MASKED, <code class="docutils literal notranslate"><span class="pre">0</span></code> for MASKED tokens.</li>
<li><strong>start_positions</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Labels for position (index) of the start of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<cite>sequence_length</cite>).
Position outside of the sequence are not taken into account for computing the loss.</li>
<li><strong>end_positions</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Labels for position (index) of the end of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<cite>sequence_length</cite>).
Position outside of the sequence are not taken into account for computing the loss.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><dl class="docutils">
<dt>loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(1,)</span></code>, <cite>optional</cite>, returned when <code class="xref py py-obj docutils literal notranslate"><span class="pre">labels</span></code> is provided):</dt>
<dd><p class="first last">Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.</p>
</dd>
<dt>start_scores (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,)</span></code>):</dt>
<dd><p class="first last">Span-start scores (before SoftMax).</p>
</dd>
<dt>end_scores (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,)</span></code>):</dt>
<dd><p class="first last">Span-end scores (before SoftMax).</p>
</dd>
<dt>hidden_states (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">config.output_hidden_states=True</span></code>):</dt>
<dd><p class="first">Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for the output of the embeddings + one for the output of each layer)
of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>.</p>
<p class="last">Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</dd>
<dt>attentions (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">config.output_attentions=True</span></code>):</dt>
<dd><p class="first">Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for each layer) of shape
<code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_heads,</span> <span class="pre">sequence_length,</span> <span class="pre">sequence_length)</span></code>.</p>
<p class="last">Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</dd>
</dl>
</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code> comprising various elements depending on the configuration (<a class="reference internal" href="#transformers.BertConfig" title="transformers.BertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertConfig</span></code></a>) and inputs</p>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="k">import</span> <span class="n">BertTokenizer</span><span class="p">,</span> <span class="n">BertForQuestionAnswering</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BertForQuestionAnswering</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-large-uncased-whole-word-masking-finetuned-squad&#39;</span><span class="p">)</span>

<span class="n">question</span><span class="p">,</span> <span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;Who was Jim Henson?&quot;</span><span class="p">,</span> <span class="s2">&quot;Jim Henson was a nice puppet&quot;</span>
<span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">question</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
<span class="n">token_type_ids</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="mi">102</span><span class="p">)</span> <span class="k">else</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_ids</span><span class="p">))]</span>
<span class="n">start_scores</span><span class="p">,</span> <span class="n">end_scores</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">input_ids</span><span class="p">]),</span> <span class="n">token_type_ids</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">token_type_ids</span><span class="p">]))</span>

<span class="n">all_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_ids_to_tokens</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>
<span class="n">answer</span> <span class="o">=</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">all_tokens</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">start_scores</span><span class="p">)</span> <span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">end_scores</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span>

<span class="k">assert</span> <span class="n">answer</span> <span class="o">==</span> <span class="s2">&quot;a nice puppet&quot;</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="tfbertmodel">
<h2>TFBertModel<a class="headerlink" href="#tfbertmodel" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="tfbertforpretraining">
<h2>TFBertForPreTraining<a class="headerlink" href="#tfbertforpretraining" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="tfbertformaskedlm">
<h2>TFBertForMaskedLM<a class="headerlink" href="#tfbertformaskedlm" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="tfbertfornextsentenceprediction">
<h2>TFBertForNextSentencePrediction<a class="headerlink" href="#tfbertfornextsentenceprediction" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="tfbertforsequenceclassification">
<h2>TFBertForSequenceClassification<a class="headerlink" href="#tfbertforsequenceclassification" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="tfbertformultiplechoice">
<h2>TFBertForMultipleChoice<a class="headerlink" href="#tfbertformultiplechoice" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="tfbertfortokenclassification">
<h2>TFBertForTokenClassification<a class="headerlink" href="#tfbertfortokenclassification" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="tfbertforquestionanswering">
<h2>TFBertForQuestionAnswering<a class="headerlink" href="#tfbertforquestionanswering" title="Permalink to this headline">¶</a></h2>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="gpt.html" class="btn btn-neutral float-right" title="OpenAI GPT" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="auto.html" class="btn btn-neutral" title="AutoModels" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, huggingface

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
        <script type="text/javascript" src="../_static/js/custom.js"></script>
    

  

  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>