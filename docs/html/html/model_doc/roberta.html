

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>RoBERTa &mdash; transformers 2.6.0 documentation</title>
  

  
  
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/huggingface.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/code-snippets.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="DistilBERT" href="distilbert.html" />
    <link rel="prev" title="XLNet" href="xlnet.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> transformers
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pretrained_models.html">Pretrained models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../usage.html">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_sharing.html">Model Sharing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks.html">Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../serialization.html">Loading Google AI or OpenAI pre-trained weights or PyTorch dump</a></li>
<li class="toctree-l1"><a class="reference internal" href="../serialization.html#serialization-best-practices">Serialization best-practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../converting_tensorflow_models.html">Converting Tensorflow Checkpoints</a></li>
<li class="toctree-l1"><a class="reference internal" href="../torchscript.html">Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../multilingual.html">Multi-lingual models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../benchmarks.html">Benchmarks</a></li>
</ul>
<p class="caption"><span class="caption-text">Core Classes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/configuration.html">Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/model.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/tokenizer.html">Tokenizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/pipelines.html">Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/optimizer_schedules.html">Optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/optimizer_schedules.html#schedules">Schedules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/optimizer_schedules.html#gradient-strategies">Gradient Strategies</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/processors.html">Processors</a></li>
</ul>
<p class="caption"><span class="caption-text">Features</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../migration.html">Migrating from pytorch-pretrained-bert</a></li>
<li class="toctree-l1"><a class="reference internal" href="../glossary.html">Glossary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../bertology.html">BERTology</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
</ul>
<p class="caption"><span class="caption-text">Models</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="auto.html">AutoModels</a></li>
<li class="toctree-l1"><a class="reference internal" href="bert.html">BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpt.html">OpenAI GPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="transformerxl.html">Transformer XL</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpt2.html">OpenAI GPT2</a></li>
<li class="toctree-l1"><a class="reference internal" href="xlm.html">XLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="xlnet.html">XLNet</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">RoBERTa</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#robertaconfig">RobertaConfig</a></li>
<li class="toctree-l2"><a class="reference internal" href="#robertatokenizer">RobertaTokenizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="#robertamodel">RobertaModel</a></li>
<li class="toctree-l2"><a class="reference internal" href="#robertaformaskedlm">RobertaForMaskedLM</a></li>
<li class="toctree-l2"><a class="reference internal" href="#robertaforsequenceclassification">RobertaForSequenceClassification</a></li>
<li class="toctree-l2"><a class="reference internal" href="#robertafortokenclassification">RobertaForTokenClassification</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tfrobertamodel">TFRobertaModel</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tfrobertaformaskedlm">TFRobertaForMaskedLM</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tfrobertaforsequenceclassification">TFRobertaForSequenceClassification</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tfrobertafortokenclassification">TFRobertaForTokenClassification</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="distilbert.html">DistilBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="ctrl.html">CTRL</a></li>
<li class="toctree-l1"><a class="reference internal" href="camembert.html">CamemBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="albert.html">ALBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="xlmroberta.html">XLM-RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="flaubert.html">FlauBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="bart.html">Bart</a></li>
<li class="toctree-l1"><a class="reference internal" href="t5.html">T5</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">transformers</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>RoBERTa</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/model_doc/roberta.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="roberta">
<h1>RoBERTa<a class="headerlink" href="#roberta" title="Permalink to this headline">¶</a></h1>
<p>The RoBERTa model was proposed in <a class="reference external" href="https://arxiv.org/abs/1907.11692">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a>
by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer,
Veselin Stoyanov. It is based on Google’s BERT model released in 2018.</p>
<p>It builds on BERT and modifies key hyperparameters, removing the next-sentence pretraining
objective and training with much larger mini-batches and learning rates.</p>
<p>The abstract from the paper is the following:</p>
<p><em>Language model pretraining has led to significant performance gains but careful comparison between different
approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes,
and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication
study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and
training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of
every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These
results highlight the importance of previously overlooked design choices, and raise questions about the source
of recently reported improvements. We release our models and code.</em></p>
<p>Tips:</p>
<ul class="simple">
<li>This implementation is the same as <a class="reference internal" href="bert.html#transformers.BertModel" title="transformers.BertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertModel</span></code></a> with a tiny embeddings tweak as well as a
setup for Roberta pretrained models.</li>
<li>RoBERTa has the same architecture as BERT, but uses a byte-level BPE as a tokenizer (same as GPT-2) and uses a
different pre-training scheme.</li>
<li>RoBERTa doesn’t have <cite>token_type_ids</cite>, you don’t need to indicate which token belongs to which segment. Just separate your segments with the separation token <cite>tokenizer.sep_token</cite> (or <cite>&lt;/s&gt;</cite>)</li>
<li><a class="reference external" href="./camembert.html">Camembert</a> is a wrapper around RoBERTa. Refer to this page for usage examples.</li>
</ul>
<div class="section" id="robertaconfig">
<h2>RobertaConfig<a class="headerlink" href="#robertaconfig" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="transformers.RobertaConfig">
<em class="property">class </em><code class="descclassname">transformers.</code><code class="descname">RobertaConfig</code><span class="sig-paren">(</span><em>pad_token_id=1</em>, <em>bos_token_id=0</em>, <em>eos_token_id=2</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/configuration_roberta.html#RobertaConfig"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.RobertaConfig" title="Permalink to this definition">¶</a></dt>
<dd><p>This is the configuration class to store the configuration of an <a class="reference internal" href="#transformers.RobertaModel" title="transformers.RobertaModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaModel</span></code></a>.
It is used to instantiate an RoBERTa model according to the specified arguments, defining the model
architecture. Instantiating a configuration with the defaults will yield a similar configuration to that of
the BERT <a class="reference external" href="https://huggingface.co/bert-base-uncased">bert-base-uncased</a> architecture.</p>
<p>Configuration objects inherit from  <a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a> and can be used
to control the model outputs. Read the documentation from  <a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>
for more information.</p>
<p>The <a class="reference internal" href="#transformers.RobertaConfig" title="transformers.RobertaConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaConfig</span></code></a> class directly inherits <a class="reference internal" href="bert.html#transformers.BertConfig" title="transformers.BertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertConfig</span></code></a>.
It reuses the same defaults. Please check the parent class for more information.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="k">import</span> <span class="n">RobertaConfig</span><span class="p">,</span> <span class="n">RobertaModel</span>

<span class="c1"># Initializing a RoBERTa configuration</span>
<span class="n">configuration</span> <span class="o">=</span> <span class="n">RobertaConfig</span><span class="p">()</span>

<span class="c1"># Initializing a model from the configuration</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">RobertaModel</span><span class="p">(</span><span class="n">configuration</span><span class="p">)</span>

<span class="c1"># Accessing the model configuration</span>
<span class="n">configuration</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span>
</pre></div>
</div>
<dl class="attribute">
<dt id="transformers.RobertaConfig.pretrained_config_archive_map">
<code class="descname">pretrained_config_archive_map</code><a class="headerlink" href="#transformers.RobertaConfig.pretrained_config_archive_map" title="Permalink to this definition">¶</a></dt>
<dd><p>A dictionary containing all the available pre-trained checkpoints.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body">Dict[str, str]</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="robertatokenizer">
<h2>RobertaTokenizer<a class="headerlink" href="#robertatokenizer" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="transformers.RobertaTokenizer">
<em class="property">class </em><code class="descclassname">transformers.</code><code class="descname">RobertaTokenizer</code><span class="sig-paren">(</span><em>vocab_file</em>, <em>merges_file</em>, <em>errors='replace'</em>, <em>bos_token='&lt;s&gt;'</em>, <em>eos_token='&lt;/s&gt;'</em>, <em>sep_token='&lt;/s&gt;'</em>, <em>cls_token='&lt;s&gt;'</em>, <em>unk_token='&lt;unk&gt;'</em>, <em>pad_token='&lt;pad&gt;'</em>, <em>mask_token='&lt;mask&gt;'</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/tokenization_roberta.html#RobertaTokenizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.RobertaTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Constructs a RoBERTa BPE tokenizer, derived from the GPT-2 tokenizer. Peculiarities:</p>
<ul class="simple">
<li>Byte-level Byte-Pair-Encoding</li>
<li>Requires a space to start the input string =&gt; the encoding methods should be called with the
<code class="docutils literal notranslate"><span class="pre">add_prefix_space</span></code> flag set to <code class="docutils literal notranslate"><span class="pre">True</span></code>.
Otherwise, this tokenizer <code class="docutils literal notranslate"><span class="pre">encode</span></code> and <code class="docutils literal notranslate"><span class="pre">decode</span></code> method will not conserve
the absence of a space at the beginning of a string:</li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">&quot;Hello&quot;</span><span class="p">))</span> <span class="o">=</span> <span class="s2">&quot; Hello&quot;</span>
</pre></div>
</div>
<p>This tokenizer inherits from <a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer" title="transformers.PreTrainedTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizer</span></code></a> which contains most of the methods. Users
should refer to the superclass for more information regarding methods.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>vocab_file</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>) – Path to the vocabulary file.</li>
<li><strong>merges_file</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>) – Path to the merges file.</li>
<li><strong>errors</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to “replace”) – Paradigm to follow when decoding bytes to UTF-8. See <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#bytes.decode">bytes.decode</a> for more information.</li>
<li><strong>bos_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">string</span></code>, <cite>optional</cite>, defaults to “&lt;s&gt;”) – <p>The beginning of sequence token that was used during pre-training. Can be used a sequence classifier token.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">When building a sequence using special tokens, this is not the token that is used for the beginning
of sequence. The token used is the <code class="xref py py-obj docutils literal notranslate"><span class="pre">cls_token</span></code>.</p>
</div>
</li>
<li><strong>eos_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">string</span></code>, <cite>optional</cite>, defaults to “&lt;/s&gt;”) – <p>The end of sequence token.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">When building a sequence using special tokens, this is not the token that is used for the end
of sequence. The token used is the <code class="xref py py-obj docutils literal notranslate"><span class="pre">sep_token</span></code>.</p>
</div>
</li>
<li><strong>sep_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">string</span></code>, <cite>optional</cite>, defaults to “&lt;/s&gt;”) – The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences
for sequence classification or for a text and a question for question answering.
It is also used as the last token of a sequence built with special tokens.</li>
<li><strong>cls_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">string</span></code>, <cite>optional</cite>, defaults to “&lt;s&gt;”) – The classifier token which is used when doing sequence classification (classification of the whole
sequence instead of per-token classification). It is the first token of the sequence when built with
special tokens.</li>
<li><strong>unk_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">string</span></code>, <cite>optional</cite>, defaults to “&lt;unk&gt;”) – The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.</li>
<li><strong>pad_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">string</span></code>, <cite>optional</cite>, defaults to “&lt;pad&gt;”) – The token used for padding, for example when batching sequences of different lengths.</li>
<li><strong>mask_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">string</span></code>, <cite>optional</cite>, defaults to “&lt;mask&gt;”) – The token used for masking values. This is the token used when training this model with masked language
modeling. This is the token which the model will try to predict.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="transformers.RobertaTokenizer.build_inputs_with_special_tokens">
<code class="descname">build_inputs_with_special_tokens</code><span class="sig-paren">(</span><em>token_ids_0: List[int], token_ids_1: Optional[List[int]] = None</em><span class="sig-paren">)</span> &#x2192; List[int]<a class="reference internal" href="../_modules/transformers/tokenization_roberta.html#RobertaTokenizer.build_inputs_with_special_tokens"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.RobertaTokenizer.build_inputs_with_special_tokens" title="Permalink to this definition">¶</a></dt>
<dd><p>Build model inputs from a sequence or a pair of sequence for sequence classification tasks
by concatenating and adding special tokens.
A RoBERTa sequence has the following format:</p>
<ul class="simple">
<li>single sequence: <code class="docutils literal notranslate"><span class="pre">&lt;s&gt;</span> <span class="pre">X</span> <span class="pre">&lt;/s&gt;</span></code></li>
<li>pair of sequences: <code class="docutils literal notranslate"><span class="pre">&lt;s&gt;</span> <span class="pre">A</span> <span class="pre">&lt;/s&gt;&lt;/s&gt;</span> <span class="pre">B</span> <span class="pre">&lt;/s&gt;</span></code></li>
</ul>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>token_ids_0</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>) – List of IDs to which the special tokens will be added</li>
<li><strong>token_ids_1</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Optional second list of IDs for sequence pairs.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">list of <a class="reference external" href="../glossary.html#input-ids">input IDs</a> with the appropriate special tokens.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="transformers.RobertaTokenizer.create_token_type_ids_from_sequences">
<code class="descname">create_token_type_ids_from_sequences</code><span class="sig-paren">(</span><em>token_ids_0: List[int], token_ids_1: Optional[List[int]] = None</em><span class="sig-paren">)</span> &#x2192; List[int]<a class="reference internal" href="../_modules/transformers/tokenization_roberta.html#RobertaTokenizer.create_token_type_ids_from_sequences"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.RobertaTokenizer.create_token_type_ids_from_sequences" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a mask from the two sequences passed to be used in a sequence-pair classification task.
RoBERTa does not make use of token type ids, therefore a list of zeros is returned.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>token_ids_0</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>) – List of ids.</li>
<li><strong>token_ids_1</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Optional second list of IDs for sequence pairs.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">List of zeros.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="transformers.RobertaTokenizer.get_special_tokens_mask">
<code class="descname">get_special_tokens_mask</code><span class="sig-paren">(</span><em>token_ids_0: List[int], token_ids_1: Optional[List[int]] = None, already_has_special_tokens: bool = False</em><span class="sig-paren">)</span> &#x2192; List[int]<a class="reference internal" href="../_modules/transformers/tokenization_roberta.html#RobertaTokenizer.get_special_tokens_mask"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.RobertaTokenizer.get_special_tokens_mask" title="Permalink to this definition">¶</a></dt>
<dd><p>Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer <code class="docutils literal notranslate"><span class="pre">prepare_for_model</span></code> or <code class="docutils literal notranslate"><span class="pre">encode_plus</span></code> methods.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>token_ids_0</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>) – List of ids.</li>
<li><strong>token_ids_1</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Optional second list of IDs for sequence pairs.</li>
<li><strong>already_has_special_tokens</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – Set to True if the token list is already formatted with special tokens for the model</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">A list of integers in the range [0, 1]: 0 for a special token, 1 for a sequence token.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="transformers.RobertaTokenizer.save_vocabulary">
<code class="descname">save_vocabulary</code><span class="sig-paren">(</span><em>save_directory</em><span class="sig-paren">)</span><a class="headerlink" href="#transformers.RobertaTokenizer.save_vocabulary" title="Permalink to this definition">¶</a></dt>
<dd><p>Save the vocabulary and special tokens file to a directory.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>save_directory</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>) – The directory in which to save the vocabulary.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">Paths to the files saved.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple(str)</span></code></td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="robertamodel">
<h2>RobertaModel<a class="headerlink" href="#robertamodel" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="transformers.RobertaModel">
<em class="property">class </em><code class="descclassname">transformers.</code><code class="descname">RobertaModel</code><span class="sig-paren">(</span><em>config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_roberta.html#RobertaModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.RobertaModel" title="Permalink to this definition">¶</a></dt>
<dd><p>The bare RoBERTa Model transformer outputting raw hidden-states without any specific head on top.</p>
<p>This model is a PyTorch <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module">torch.nn.Module</a> sub-class.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general
usage and behavior.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>config</strong> (<a class="reference internal" href="#transformers.RobertaConfig" title="transformers.RobertaConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaConfig</span></code></a>) – Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the configuration.
Check out the <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> method to load the model weights.</td>
</tr>
</tbody>
</table>
<p>This class overrides <a class="reference internal" href="bert.html#transformers.BertModel" title="transformers.BertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertModel</span></code></a>. Please check the
superclass for the appropriate documentation alongside usage examples.</p>
<dl class="attribute">
<dt id="transformers.RobertaModel.config_class">
<code class="descname">config_class</code><a class="headerlink" href="#transformers.RobertaModel.config_class" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.configuration_roberta.RobertaConfig</span></code></p>
</dd></dl>

<dl class="method">
<dt id="transformers.RobertaModel.get_input_embeddings">
<code class="descname">get_input_embeddings</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_roberta.html#RobertaModel.get_input_embeddings"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.RobertaModel.get_input_embeddings" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the model’s input embeddings.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">A torch module mapping vocabulary to hidden states.</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="transformers.RobertaModel.set_input_embeddings">
<code class="descname">set_input_embeddings</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_roberta.html#RobertaModel.set_input_embeddings"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.RobertaModel.set_input_embeddings" title="Permalink to this definition">¶</a></dt>
<dd><p>Set model’s input embeddings</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>value</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code>) – A module mapping vocabulary to hidden states.</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="robertaformaskedlm">
<h2>RobertaForMaskedLM<a class="headerlink" href="#robertaformaskedlm" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="transformers.RobertaForMaskedLM">
<em class="property">class </em><code class="descclassname">transformers.</code><code class="descname">RobertaForMaskedLM</code><span class="sig-paren">(</span><em>config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_roberta.html#RobertaForMaskedLM"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.RobertaForMaskedLM" title="Permalink to this definition">¶</a></dt>
<dd><p>RoBERTa Model with a <cite>language modeling</cite> head on top.</p>
<p>This model is a PyTorch <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module">torch.nn.Module</a> sub-class.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general
usage and behavior.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>config</strong> (<a class="reference internal" href="#transformers.RobertaConfig" title="transformers.RobertaConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaConfig</span></code></a>) – Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the configuration.
Check out the <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> method to load the model weights.</td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="transformers.RobertaForMaskedLM.config_class">
<code class="descname">config_class</code><a class="headerlink" href="#transformers.RobertaForMaskedLM.config_class" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.configuration_roberta.RobertaConfig</span></code></p>
</dd></dl>

<dl class="method">
<dt id="transformers.RobertaForMaskedLM.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input_ids=None</em>, <em>attention_mask=None</em>, <em>token_type_ids=None</em>, <em>position_ids=None</em>, <em>head_mask=None</em>, <em>inputs_embeds=None</em>, <em>masked_lm_labels=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_roberta.html#RobertaForMaskedLM.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.RobertaForMaskedLM.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#transformers.RobertaForMaskedLM" title="transformers.RobertaForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaForMaskedLM</span></code></a> forward method, overrides the <code class="xref py py-func docutils literal notranslate"><span class="pre">__call__()</span></code> special method.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
pre and post processing steps while the latter silently ignores them.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>) – <p>Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a class="reference internal" href="#transformers.RobertaTokenizer" title="transformers.RobertaTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.RobertaTokenizer</span></code></a>.
See <a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.encode" title="transformers.PreTrainedTokenizer.encode"><code class="xref py py-func docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.encode()</span></code></a> and
<a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.encode_plus" title="transformers.PreTrainedTokenizer.encode_plus"><code class="xref py py-func docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.encode_plus()</span></code></a> for details.</p>
<p><a class="reference external" href="../glossary.html#input-ids">What are input IDs?</a></p>
</li>
<li><strong>attention_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – <p>Mask to avoid performing attention on padding token indices.
Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:
<code class="docutils literal notranslate"><span class="pre">1</span></code> for tokens that are NOT MASKED, <code class="docutils literal notranslate"><span class="pre">0</span></code> for MASKED tokens.</p>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</li>
<li><strong>token_type_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – <p>Segment token indices to indicate first and second portions of the inputs.
Indices are selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>: <code class="docutils literal notranslate"><span class="pre">0</span></code> corresponds to a <cite>sentence A</cite> token, <code class="docutils literal notranslate"><span class="pre">1</span></code>
corresponds to a <cite>sentence B</cite> token</p>
<p><a class="reference external" href="../glossary.html#token-type-ids">What are token type IDs?</a></p>
</li>
<li><strong>position_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – <p>Indices of positions of each input sequence tokens in the position embeddings.
Selected in the range <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">config.max_position_embeddings</span> <span class="pre">-</span> <span class="pre">1]</span></code>.</p>
<p><a class="reference external" href="../glossary.html#position-ids">What are position IDs?</a></p>
</li>
<li><strong>head_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_heads,)</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_layers,</span> <span class="pre">num_heads)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Mask to nullify selected heads of the self-attention modules.
Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:
<code class="xref py py-obj docutils literal notranslate"><span class="pre">1</span></code> indicates the head is <strong>not masked</strong>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">0</span></code> indicates the head is <strong>masked</strong>.</li>
<li><strong>inputs_embeds</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Optionally, instead of passing <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <cite>input_ids</cite> indices into associated vectors
than the model’s internal embedding lookup matrix.</li>
<li><strong>masked_lm_labels</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Labels for computing the masked language modeling loss.
Indices should be in <code class="docutils literal notranslate"><span class="pre">[-100,</span> <span class="pre">0,</span> <span class="pre">...,</span> <span class="pre">config.vocab_size]</span></code> (see <code class="docutils literal notranslate"><span class="pre">input_ids</span></code> docstring)
Tokens with indices set to <code class="docutils literal notranslate"><span class="pre">-100</span></code> are ignored (masked), the loss is only computed for the tokens with labels
in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">...,</span> <span class="pre">config.vocab_size]</span></code></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><dl class="docutils">
<dt>masked_lm_loss (<cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">masked_lm_labels</span></code> is provided) <code class="docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="docutils literal notranslate"><span class="pre">(1,)</span></code>:</dt>
<dd><p class="first last">Masked language modeling loss.</p>
</dd>
<dt>prediction_scores (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">config.vocab_size)</span></code>)</dt>
<dd><p class="first last">Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</dd>
<dt>hidden_states (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">config.output_hidden_states=True</span></code>):</dt>
<dd><p class="first">Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for the output of the embeddings + one for the output of each layer)
of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>.</p>
<p class="last">Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</dd>
<dt>attentions (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">config.output_attentions=True</span></code>):</dt>
<dd><p class="first">Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for each layer) of shape
<code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_heads,</span> <span class="pre">sequence_length,</span> <span class="pre">sequence_length)</span></code>.</p>
<p class="last">Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</dd>
</dl>
</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code> comprising various elements depending on the configuration (<a class="reference internal" href="#transformers.RobertaConfig" title="transformers.RobertaConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaConfig</span></code></a>) and inputs</p>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="k">import</span> <span class="n">RobertaTokenizer</span><span class="p">,</span> <span class="n">RobertaForMaskedLM</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">RobertaTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;roberta-base&#39;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">RobertaForMaskedLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;roberta-base&#39;</span><span class="p">)</span>
<span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">&quot;Hello, my dog is cute&quot;</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Batch size 1</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">masked_lm_labels</span><span class="o">=</span><span class="n">input_ids</span><span class="p">)</span>
<span class="n">loss</span><span class="p">,</span> <span class="n">prediction_scores</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="transformers.RobertaForMaskedLM.get_output_embeddings">
<code class="descname">get_output_embeddings</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_roberta.html#RobertaForMaskedLM.get_output_embeddings"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.RobertaForMaskedLM.get_output_embeddings" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the model’s output embeddings.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">A torch module mapping hidden states to vocabulary.</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code></td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="robertaforsequenceclassification">
<h2>RobertaForSequenceClassification<a class="headerlink" href="#robertaforsequenceclassification" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="transformers.RobertaForSequenceClassification">
<em class="property">class </em><code class="descclassname">transformers.</code><code class="descname">RobertaForSequenceClassification</code><span class="sig-paren">(</span><em>config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_roberta.html#RobertaForSequenceClassification"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.RobertaForSequenceClassification" title="Permalink to this definition">¶</a></dt>
<dd><p>RoBERTa Model transformer with a sequence classification/regression head on top (a linear layer
on top of the pooled output) e.g. for GLUE tasks.</p>
<p>This model is a PyTorch <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module">torch.nn.Module</a> sub-class.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general
usage and behavior.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>config</strong> (<a class="reference internal" href="#transformers.RobertaConfig" title="transformers.RobertaConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaConfig</span></code></a>) – Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the configuration.
Check out the <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> method to load the model weights.</td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="transformers.RobertaForSequenceClassification.config_class">
<code class="descname">config_class</code><a class="headerlink" href="#transformers.RobertaForSequenceClassification.config_class" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.configuration_roberta.RobertaConfig</span></code></p>
</dd></dl>

<dl class="method">
<dt id="transformers.RobertaForSequenceClassification.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input_ids=None</em>, <em>attention_mask=None</em>, <em>token_type_ids=None</em>, <em>position_ids=None</em>, <em>head_mask=None</em>, <em>inputs_embeds=None</em>, <em>labels=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_roberta.html#RobertaForSequenceClassification.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.RobertaForSequenceClassification.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#transformers.RobertaForSequenceClassification" title="transformers.RobertaForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaForSequenceClassification</span></code></a> forward method, overrides the <code class="xref py py-func docutils literal notranslate"><span class="pre">__call__()</span></code> special method.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
pre and post processing steps while the latter silently ignores them.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>) – <p>Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a class="reference internal" href="#transformers.RobertaTokenizer" title="transformers.RobertaTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.RobertaTokenizer</span></code></a>.
See <a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.encode" title="transformers.PreTrainedTokenizer.encode"><code class="xref py py-func docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.encode()</span></code></a> and
<a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.encode_plus" title="transformers.PreTrainedTokenizer.encode_plus"><code class="xref py py-func docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.encode_plus()</span></code></a> for details.</p>
<p><a class="reference external" href="../glossary.html#input-ids">What are input IDs?</a></p>
</li>
<li><strong>attention_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – <p>Mask to avoid performing attention on padding token indices.
Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:
<code class="docutils literal notranslate"><span class="pre">1</span></code> for tokens that are NOT MASKED, <code class="docutils literal notranslate"><span class="pre">0</span></code> for MASKED tokens.</p>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</li>
<li><strong>token_type_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – <p>Segment token indices to indicate first and second portions of the inputs.
Indices are selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>: <code class="docutils literal notranslate"><span class="pre">0</span></code> corresponds to a <cite>sentence A</cite> token, <code class="docutils literal notranslate"><span class="pre">1</span></code>
corresponds to a <cite>sentence B</cite> token</p>
<p><a class="reference external" href="../glossary.html#token-type-ids">What are token type IDs?</a></p>
</li>
<li><strong>position_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – <p>Indices of positions of each input sequence tokens in the position embeddings.
Selected in the range <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">config.max_position_embeddings</span> <span class="pre">-</span> <span class="pre">1]</span></code>.</p>
<p><a class="reference external" href="../glossary.html#position-ids">What are position IDs?</a></p>
</li>
<li><strong>head_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_heads,)</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_layers,</span> <span class="pre">num_heads)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Mask to nullify selected heads of the self-attention modules.
Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:
<code class="xref py py-obj docutils literal notranslate"><span class="pre">1</span></code> indicates the head is <strong>not masked</strong>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">0</span></code> indicates the head is <strong>masked</strong>.</li>
<li><strong>inputs_embeds</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Optionally, instead of passing <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <cite>input_ids</cite> indices into associated vectors
than the model’s internal embedding lookup matrix.</li>
<li><strong>labels</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Labels for computing the sequence classification/regression loss.
Indices should be in <code class="xref py py-obj docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">...,</span> <span class="pre">config.num_labels</span> <span class="pre">-</span> <span class="pre">1]</span></code>.
If <code class="xref py py-obj docutils literal notranslate"><span class="pre">config.num_labels</span> <span class="pre">==</span> <span class="pre">1</span></code> a regression loss is computed (Mean-Square loss),
If <code class="xref py py-obj docutils literal notranslate"><span class="pre">config.num_labels</span> <span class="pre">&gt;</span> <span class="pre">1</span></code> a classification loss is computed (Cross-Entropy).</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><dl class="docutils">
<dt>loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(1,)</span></code>, <cite>optional</cite>, returned when <code class="xref py py-obj docutils literal notranslate"><span class="pre">label</span></code> is provided):</dt>
<dd><p class="first last">Classification (or regression if config.num_labels==1) loss.</p>
</dd>
<dt>logits (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">config.num_labels)</span></code>):</dt>
<dd><p class="first last">Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</dd>
<dt>hidden_states (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">config.output_hidden_states=True</span></code>):</dt>
<dd><p class="first">Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for the output of the embeddings + one for the output of each layer)
of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>.</p>
<p class="last">Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</dd>
<dt>attentions (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">config.output_attentions=True</span></code>):</dt>
<dd><p class="first">Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for each layer) of shape
<code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_heads,</span> <span class="pre">sequence_length,</span> <span class="pre">sequence_length)</span></code>.</p>
<p class="last">Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</dd>
</dl>
</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code> comprising various elements depending on the configuration (<a class="reference internal" href="#transformers.RobertaConfig" title="transformers.RobertaConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaConfig</span></code></a>) and inputs</p>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="k">import</span> <span class="n">RobertaTokenizer</span><span class="p">,</span> <span class="n">RobertaForSequenceClassification</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">RobertaTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;roberta-base&#39;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">RobertaForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;roberta-base&#39;</span><span class="p">)</span>
<span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">&quot;Hello, my dog is cute&quot;</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Batch size 1</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Batch size 1</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
<span class="n">loss</span><span class="p">,</span> <span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="robertafortokenclassification">
<h2>RobertaForTokenClassification<a class="headerlink" href="#robertafortokenclassification" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="transformers.RobertaForTokenClassification">
<em class="property">class </em><code class="descclassname">transformers.</code><code class="descname">RobertaForTokenClassification</code><span class="sig-paren">(</span><em>config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_roberta.html#RobertaForTokenClassification"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.RobertaForTokenClassification" title="Permalink to this definition">¶</a></dt>
<dd><p>Roberta Model with a token classification head on top (a linear layer on top of
the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks.</p>
<p>This model is a PyTorch <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module">torch.nn.Module</a> sub-class.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general
usage and behavior.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>config</strong> (<a class="reference internal" href="#transformers.RobertaConfig" title="transformers.RobertaConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaConfig</span></code></a>) – Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the configuration.
Check out the <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> method to load the model weights.</td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="transformers.RobertaForTokenClassification.config_class">
<code class="descname">config_class</code><a class="headerlink" href="#transformers.RobertaForTokenClassification.config_class" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.configuration_roberta.RobertaConfig</span></code></p>
</dd></dl>

<dl class="method">
<dt id="transformers.RobertaForTokenClassification.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input_ids=None</em>, <em>attention_mask=None</em>, <em>token_type_ids=None</em>, <em>position_ids=None</em>, <em>head_mask=None</em>, <em>inputs_embeds=None</em>, <em>labels=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_roberta.html#RobertaForTokenClassification.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.RobertaForTokenClassification.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#transformers.RobertaForTokenClassification" title="transformers.RobertaForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaForTokenClassification</span></code></a> forward method, overrides the <code class="xref py py-func docutils literal notranslate"><span class="pre">__call__()</span></code> special method.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
pre and post processing steps while the latter silently ignores them.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>) – <p>Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a class="reference internal" href="#transformers.RobertaTokenizer" title="transformers.RobertaTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.RobertaTokenizer</span></code></a>.
See <a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.encode" title="transformers.PreTrainedTokenizer.encode"><code class="xref py py-func docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.encode()</span></code></a> and
<a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.encode_plus" title="transformers.PreTrainedTokenizer.encode_plus"><code class="xref py py-func docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.encode_plus()</span></code></a> for details.</p>
<p><a class="reference external" href="../glossary.html#input-ids">What are input IDs?</a></p>
</li>
<li><strong>attention_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – <p>Mask to avoid performing attention on padding token indices.
Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:
<code class="docutils literal notranslate"><span class="pre">1</span></code> for tokens that are NOT MASKED, <code class="docutils literal notranslate"><span class="pre">0</span></code> for MASKED tokens.</p>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</li>
<li><strong>token_type_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – <p>Segment token indices to indicate first and second portions of the inputs.
Indices are selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>: <code class="docutils literal notranslate"><span class="pre">0</span></code> corresponds to a <cite>sentence A</cite> token, <code class="docutils literal notranslate"><span class="pre">1</span></code>
corresponds to a <cite>sentence B</cite> token</p>
<p><a class="reference external" href="../glossary.html#token-type-ids">What are token type IDs?</a></p>
</li>
<li><strong>position_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – <p>Indices of positions of each input sequence tokens in the position embeddings.
Selected in the range <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">config.max_position_embeddings</span> <span class="pre">-</span> <span class="pre">1]</span></code>.</p>
<p><a class="reference external" href="../glossary.html#position-ids">What are position IDs?</a></p>
</li>
<li><strong>head_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_heads,)</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_layers,</span> <span class="pre">num_heads)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Mask to nullify selected heads of the self-attention modules.
Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:
<code class="xref py py-obj docutils literal notranslate"><span class="pre">1</span></code> indicates the head is <strong>not masked</strong>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">0</span></code> indicates the head is <strong>masked</strong>.</li>
<li><strong>inputs_embeds</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Optionally, instead of passing <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <cite>input_ids</cite> indices into associated vectors
than the model’s internal embedding lookup matrix.</li>
<li><strong>labels</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Labels for computing the token classification loss.
Indices should be in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">...,</span> <span class="pre">config.num_labels</span> <span class="pre">-</span> <span class="pre">1]</span></code>.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><dl class="docutils">
<dt>loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(1,)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">labels</span></code> is provided) :</dt>
<dd><p class="first last">Classification loss.</p>
</dd>
<dt>scores (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">config.num_labels)</span></code>)</dt>
<dd><p class="first last">Classification scores (before SoftMax).</p>
</dd>
<dt>hidden_states (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">config.output_hidden_states=True</span></code>):</dt>
<dd><p class="first">Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for the output of the embeddings + one for the output of each layer)
of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>.</p>
<p class="last">Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</dd>
<dt>attentions (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">config.output_attentions=True</span></code>):</dt>
<dd><p class="first">Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for each layer) of shape
<code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_heads,</span> <span class="pre">sequence_length,</span> <span class="pre">sequence_length)</span></code>.</p>
<p class="last">Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</dd>
</dl>
</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code> comprising various elements depending on the configuration (<a class="reference internal" href="#transformers.RobertaConfig" title="transformers.RobertaConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaConfig</span></code></a>) and inputs</p>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="k">import</span> <span class="n">RobertaTokenizer</span><span class="p">,</span> <span class="n">RobertaForTokenClassification</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">RobertaTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;roberta-base&#39;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">RobertaForTokenClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;roberta-base&#39;</span><span class="p">)</span>
<span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">&quot;Hello, my dog is cute&quot;</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Batch size 1</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Batch size 1</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
<span class="n">loss</span><span class="p">,</span> <span class="n">scores</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="tfrobertamodel">
<h2>TFRobertaModel<a class="headerlink" href="#tfrobertamodel" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="tfrobertaformaskedlm">
<h2>TFRobertaForMaskedLM<a class="headerlink" href="#tfrobertaformaskedlm" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="tfrobertaforsequenceclassification">
<h2>TFRobertaForSequenceClassification<a class="headerlink" href="#tfrobertaforsequenceclassification" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="tfrobertafortokenclassification">
<h2>TFRobertaForTokenClassification<a class="headerlink" href="#tfrobertafortokenclassification" title="Permalink to this headline">¶</a></h2>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="distilbert.html" class="btn btn-neutral float-right" title="DistilBERT" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="xlnet.html" class="btn btn-neutral" title="XLNet" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, huggingface

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
        <script type="text/javascript" src="../_static/js/custom.js"></script>
    

  

  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>