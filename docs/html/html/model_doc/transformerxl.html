

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Transformer XL &mdash; transformers 2.6.0 documentation</title>
  

  
  
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/huggingface.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/code-snippets.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="OpenAI GPT2" href="gpt2.html" />
    <link rel="prev" title="OpenAI GPT" href="gpt.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> transformers
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pretrained_models.html">Pretrained models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../usage.html">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_sharing.html">Model Sharing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks.html">Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../serialization.html">Loading Google AI or OpenAI pre-trained weights or PyTorch dump</a></li>
<li class="toctree-l1"><a class="reference internal" href="../serialization.html#serialization-best-practices">Serialization best-practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../converting_tensorflow_models.html">Converting Tensorflow Checkpoints</a></li>
<li class="toctree-l1"><a class="reference internal" href="../torchscript.html">Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../multilingual.html">Multi-lingual models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../benchmarks.html">Benchmarks</a></li>
</ul>
<p class="caption"><span class="caption-text">Core Classes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/configuration.html">Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/model.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/tokenizer.html">Tokenizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/pipelines.html">Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/optimizer_schedules.html">Optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/optimizer_schedules.html#schedules">Schedules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/optimizer_schedules.html#gradient-strategies">Gradient Strategies</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/processors.html">Processors</a></li>
</ul>
<p class="caption"><span class="caption-text">Features</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../migration.html">Migrating from pytorch-pretrained-bert</a></li>
<li class="toctree-l1"><a class="reference internal" href="../glossary.html">Glossary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../bertology.html">BERTology</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
</ul>
<p class="caption"><span class="caption-text">Models</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="auto.html">AutoModels</a></li>
<li class="toctree-l1"><a class="reference internal" href="bert.html">BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpt.html">OpenAI GPT</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Transformer XL</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#transfoxlconfig">TransfoXLConfig</a></li>
<li class="toctree-l2"><a class="reference internal" href="#transfoxltokenizer">TransfoXLTokenizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="#transfoxlmodel">TransfoXLModel</a></li>
<li class="toctree-l2"><a class="reference internal" href="#transfoxllmheadmodel">TransfoXLLMHeadModel</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tftransfoxlmodel">TFTransfoXLModel</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tftransfoxllmheadmodel">TFTransfoXLLMHeadModel</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="gpt2.html">OpenAI GPT2</a></li>
<li class="toctree-l1"><a class="reference internal" href="xlm.html">XLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="xlnet.html">XLNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="roberta.html">RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="distilbert.html">DistilBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="ctrl.html">CTRL</a></li>
<li class="toctree-l1"><a class="reference internal" href="camembert.html">CamemBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="albert.html">ALBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="xlmroberta.html">XLM-RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="flaubert.html">FlauBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="bart.html">Bart</a></li>
<li class="toctree-l1"><a class="reference internal" href="t5.html">T5</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">transformers</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>Transformer XL</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/model_doc/transformerxl.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="transformer-xl">
<h1>Transformer XL<a class="headerlink" href="#transformer-xl" title="Permalink to this headline">¶</a></h1>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>The Transformer-XL model was proposed in
<a class="reference external" href="https://arxiv.org/abs/1901.02860">Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context</a>
by Zihang Dai*, Zhilin Yang*, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov.
It’s a causal (uni-directional) transformer with relative positioning (sinusoïdal) embeddings which can reuse
previously computed hidden-states to attend to longer context (memory).
This model also uses adaptive softmax inputs and outputs (tied).</p>
<p>The abstract from the paper is the following:</p>
<p><em>Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the
setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency
beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and
a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves
the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and
450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up
to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results
of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on
Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably
coherent, novel text articles with thousands of tokens.</em></p>
<p>Tips:</p>
<ul class="simple">
<li>Transformer-XL uses relative sinusoidal positional embeddings. Padding can be done on the left or on the right.
The original implementation trains on SQuAD with padding on the left, therefore the padding defaults are set to left.</li>
<li>Transformer-XL is one of the few models that has no sequence length limit.</li>
</ul>
</div>
<div class="section" id="transfoxlconfig">
<h2>TransfoXLConfig<a class="headerlink" href="#transfoxlconfig" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="transformers.TransfoXLConfig">
<em class="property">class </em><code class="descclassname">transformers.</code><code class="descname">TransfoXLConfig</code><span class="sig-paren">(</span><em>vocab_size=267735, cutoffs=[20000, 40000, 200000], d_model=1024, d_embed=1024, n_head=16, d_head=64, d_inner=4096, div_val=4, pre_lnorm=False, n_layer=18, tgt_len=128, ext_len=0, mem_len=1600, clamp_len=1000, same_length=True, proj_share_all_but_first=True, attn_type=0, sample_softmax=-1, adaptive=True, tie_weight=True, dropout=0.1, dropatt=0.0, untie_r=True, init='normal', init_range=0.01, proj_init_std=0.01, init_std=0.02, layer_norm_epsilon=1e-05, eos_token_id=0, **kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/configuration_transfo_xl.html#TransfoXLConfig"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TransfoXLConfig" title="Permalink to this definition">¶</a></dt>
<dd><p>This is the configuration class to store the configuration of an <a class="reference internal" href="#transformers.TransfoXLModel" title="transformers.TransfoXLModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TransfoXLModel</span></code></a>.
It is used to instantiate a Transformer XL model according to the specified arguments, defining the model
architecture. Instantiating a configuration with the defaults will yield a similar configuration to that of
the <a class="reference external" href="https://huggingface.co/transfo-xl-wt103">Transformer XL</a> architecture.</p>
<p>Configuration objects inherit from  <a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a> and can be used
to control the model outputs. Read the documentation from  <a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>
for more information.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>vocab_size</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional, defaults to 267735) – Vocabulary size of the Transformer XL model. Defines the different tokens that
can be represented by the <cite>inputs_ids</cite> passed to the forward method of <a class="reference internal" href="#transformers.TransfoXLModel" title="transformers.TransfoXLModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TransfoXLModel</span></code></a>.</li>
<li><strong>cutoffs</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>, optional, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">[20000,</span> <span class="pre">40000,</span> <span class="pre">200000]</span></code>) – Cutoffs for the adaptive softmax</li>
<li><strong>d_model</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional, defaults to 1024) – Dimensionality of the model’s hidden states.</li>
<li><strong>d_embed</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional, defaults to 1024) – Dimensionality of the embeddings</li>
<li><strong>n_head</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional, defaults to 16) – Number of attention heads for each attention layer in the Transformer encoder.</li>
<li><strong>d_head</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional, defaults to 64) – Dimensionality of the model’s heads.</li>
<li><strong>d_inner</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional, defaults to 4096) – Inner dimension in FF</li>
<li><strong>div_val</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional, defaults to 4) – Divident value for adapative input and softmax</li>
<li><strong>pre_lnorm</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">boolean</span></code>, optional, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – Apply LayerNorm to the input instead of the output</li>
<li><strong>n_layer</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional, defaults to 18) – Number of hidden layers in the Transformer encoder.</li>
<li><strong>tgt_len</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional, defaults to 128) – Number of tokens to predict</li>
<li><strong>ext_len</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional, defaults to 0) – Length of the extended context</li>
<li><strong>mem_len</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional, defaults to 1600) – Length of the retained previous heads</li>
<li><strong>clamp_len</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional, defaults to 1000) – use the same pos embeddings after clamp_len</li>
<li><strong>same_length</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">boolean</span></code>, optional, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>) – Use the same attn length for all tokens</li>
<li><strong>proj_share_all_but_first</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">boolean</span></code>, optional, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>) – True to share all but first projs, False not to share.</li>
<li><strong>attn_type</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional, defaults to 0) – Attention type. 0 for Transformer-XL, 1 for Shaw et al, 2 for Vaswani et al, 3 for Al Rfou et al.</li>
<li><strong>sample_softmax</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional, defaults to -1) – number of samples in sampled softmax</li>
<li><strong>adaptive</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">boolean</span></code>, optional, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>) – use adaptive softmax</li>
<li><strong>tie_weight</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">boolean</span></code>, optional, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>) – tie the word embedding and softmax weights</li>
<li><strong>dropout</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, optional, defaults to 0.1) – The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.</li>
<li><strong>dropatt</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, optional, defaults to 0) – The dropout ratio for the attention probabilities.</li>
<li><strong>untie_r</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">boolean</span></code>, optional, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>) – Untie relative position biases</li>
<li><strong>init</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">string</span></code>, optional, defaults to <cite>normal</cite>) – Parameter initializer to use</li>
<li><strong>init_range</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, optional, defaults to 0.01) – Parameters initialized by U(-init_range, init_range).</li>
<li><strong>proj_init_std</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, optional, defaults to 0.01) – Parameters initialized by N(0, init_std)</li>
<li><strong>init_std</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, optional, defaults to 0.02) – Parameters initialized by N(0, init_std)</li>
<li><strong>layer_norm_epsilon</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, optional, defaults to 1e-5) – The epsilon to use in the layer normalization layers</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="k">import</span> <span class="n">TransfoXLConfig</span><span class="p">,</span> <span class="n">TransfoXLModel</span>

<span class="c1"># Initializing a Transformer XL configuration</span>
<span class="n">configuration</span> <span class="o">=</span> <span class="n">TransfoXLConfig</span><span class="p">()</span>

<span class="c1"># Initializing a model from the configuration</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">TransfoXLModel</span><span class="p">(</span><span class="n">configuration</span><span class="p">)</span>

<span class="c1"># Accessing the model configuration</span>
<span class="n">configuration</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span>
</pre></div>
</div>
<dl class="attribute">
<dt id="transformers.TransfoXLConfig.pretrained_config_archive_map">
<code class="descname">pretrained_config_archive_map</code><a class="headerlink" href="#transformers.TransfoXLConfig.pretrained_config_archive_map" title="Permalink to this definition">¶</a></dt>
<dd><p>A dictionary containing all the available pre-trained checkpoints.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Type:</th><td class="field-body">Dict[str, str]</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="transfoxltokenizer">
<h2>TransfoXLTokenizer<a class="headerlink" href="#transfoxltokenizer" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="transformers.TransfoXLTokenizer">
<em class="property">class </em><code class="descclassname">transformers.</code><code class="descname">TransfoXLTokenizer</code><span class="sig-paren">(</span><em>special=None, min_freq=0, max_size=None, lower_case=False, delimiter=None, vocab_file=None, pretrained_vocab_file=None, never_split=None, unk_token='&lt;unk&gt;', eos_token='&lt;eos&gt;', additional_special_tokens=['&lt;formula&gt;'], **kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/tokenization_transfo_xl.html#TransfoXLTokenizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TransfoXLTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Transformer-XL tokenizer adapted from Vocab class in <a class="reference external" href="https://github.com/kimiyoung/transformer-xl">https://github.com/kimiyoung/transformer-xl</a></p>
<p>This tokenizer inherits from <a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer" title="transformers.PreTrainedTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizer</span></code></a> which contains most of the methods. Users
should refer to the superclass for more information regarding methods.</p>
<dl class="method">
<dt id="transformers.TransfoXLTokenizer.save_vocabulary">
<code class="descname">save_vocabulary</code><span class="sig-paren">(</span><em>vocab_path</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/tokenization_transfo_xl.html#TransfoXLTokenizer.save_vocabulary"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TransfoXLTokenizer.save_vocabulary" title="Permalink to this definition">¶</a></dt>
<dd><p>Save the vocabulary and special tokens file to a directory.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>vocab_path</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>) – The directory in which to save the vocabulary.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">Paths to the files saved.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple(str)</span></code></td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="transfoxlmodel">
<h2>TransfoXLModel<a class="headerlink" href="#transfoxlmodel" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="transformers.TransfoXLModel">
<em class="property">class </em><code class="descclassname">transformers.</code><code class="descname">TransfoXLModel</code><span class="sig-paren">(</span><em>config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_transfo_xl.html#TransfoXLModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TransfoXLModel" title="Permalink to this definition">¶</a></dt>
<dd><p>The bare Bert Model transformer outputting raw hidden-states without any specific head on top.</p>
<p>This model is a PyTorch <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module">torch.nn.Module</a> sub-class.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general
usage and behavior.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>config</strong> (<a class="reference internal" href="#transformers.TransfoXLConfig" title="transformers.TransfoXLConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">TransfoXLConfig</span></code></a>) – Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the configuration.
Check out the <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> method to load the model weights.</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="transformers.TransfoXLModel.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input_ids=None</em>, <em>mems=None</em>, <em>head_mask=None</em>, <em>inputs_embeds=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_transfo_xl.html#TransfoXLModel.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TransfoXLModel.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#transformers.TransfoXLModel" title="transformers.TransfoXLModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TransfoXLModel</span></code></a> forward method, overrides the <code class="xref py py-func docutils literal notranslate"><span class="pre">__call__()</span></code> special method.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
pre and post processing steps while the latter silently ignores them.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>) – <p>Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a class="reference internal" href="#transformers.TransfoXLTokenizer" title="transformers.TransfoXLTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.TransfoXLTokenizer</span></code></a>.
See <a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.encode" title="transformers.PreTrainedTokenizer.encode"><code class="xref py py-func docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.encode()</span></code></a> and
<a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.encode_plus" title="transformers.PreTrainedTokenizer.encode_plus"><code class="xref py py-func docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.encode_plus()</span></code></a> for details.</p>
<p><a class="reference external" href="../glossary.html#input-ids">What are input IDs?</a></p>
</li>
<li><strong>mems</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[torch.FloatTensor]</span></code> of length <code class="xref py py-obj docutils literal notranslate"><span class="pre">config.n_layers</span></code>) – Contains pre-computed hidden-states (key and values in the attention blocks) as computed by the model
(see <cite>mems</cite> output below). Can be used to speed up sequential decoding. The token ids which have their mems
given to this model should not be passed as input ids as they have already been computed.</li>
<li><strong>head_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_heads,)</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_layers,</span> <span class="pre">num_heads)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Mask to nullify selected heads of the self-attention modules.
Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:
<code class="xref py py-obj docutils literal notranslate"><span class="pre">1</span></code> indicates the head is <strong>not masked</strong>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">0</span></code> indicates the head is <strong>masked</strong>.</li>
<li><strong>input_embeds</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Optionally, instead of passing <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <cite>input_ids</cite> indices into associated vectors
than the model’s internal embedding lookup matrix.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><dl class="docutils">
<dt>last_hidden_state (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>):</dt>
<dd><p class="first last">Sequence of hidden-states at the last layer of the model.</p>
</dd>
<dt>mems (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[torch.FloatTensor]</span></code> of length <code class="xref py py-obj docutils literal notranslate"><span class="pre">config.n_layers</span></code>):</dt>
<dd><p class="first last">Contains pre-computed hidden-states (key and values in the attention blocks).
Can be used (see <cite>mems</cite> input) to speed up sequential decoding. The token ids which have their past given to this model
should not be passed as input ids as they have already been computed.</p>
</dd>
<dt>hidden_states (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">config.output_hidden_states=True</span></code>):</dt>
<dd><p class="first">Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for the output of the embeddings + one for the output of each layer)
of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>.</p>
<p class="last">Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</dd>
<dt>attentions (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">config.output_attentions=True</span></code>):</dt>
<dd><p class="first">Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for each layer) of shape
<code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_heads,</span> <span class="pre">sequence_length,</span> <span class="pre">sequence_length)</span></code>.</p>
<p class="last">Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</dd>
</dl>
</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code> comprising various elements depending on the configuration (<a class="reference internal" href="#transformers.TransfoXLConfig" title="transformers.TransfoXLConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">TransfoXLConfig</span></code></a>) and inputs</p>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="k">import</span> <span class="n">TransfoXLTokenizer</span><span class="p">,</span> <span class="n">TransfoXLModel</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">TransfoXLTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;transfo-xl-wt103&#39;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">TransfoXLModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;transfo-xl-wt103&#39;</span><span class="p">)</span>
<span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">&quot;Hello, my dog is cute&quot;</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Batch size 1</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>
<span class="n">last_hidden_states</span><span class="p">,</span> <span class="n">mems</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="transformers.TransfoXLModel.get_input_embeddings">
<code class="descname">get_input_embeddings</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_transfo_xl.html#TransfoXLModel.get_input_embeddings"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TransfoXLModel.get_input_embeddings" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the model’s input embeddings.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">A torch module mapping vocabulary to hidden states.</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="transformers.TransfoXLModel.set_input_embeddings">
<code class="descname">set_input_embeddings</code><span class="sig-paren">(</span><em>new_embeddings</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_transfo_xl.html#TransfoXLModel.set_input_embeddings"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TransfoXLModel.set_input_embeddings" title="Permalink to this definition">¶</a></dt>
<dd><p>Set model’s input embeddings</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>value</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code>) – A module mapping vocabulary to hidden states.</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="transfoxllmheadmodel">
<h2>TransfoXLLMHeadModel<a class="headerlink" href="#transfoxllmheadmodel" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="transformers.TransfoXLLMHeadModel">
<em class="property">class </em><code class="descclassname">transformers.</code><code class="descname">TransfoXLLMHeadModel</code><span class="sig-paren">(</span><em>config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_transfo_xl.html#TransfoXLLMHeadModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TransfoXLLMHeadModel" title="Permalink to this definition">¶</a></dt>
<dd><p>The Transformer-XL Model with a language modeling head on top
(adaptive softmax with weights tied to the adaptive input embeddings)</p>
<p>This model is a PyTorch <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module">torch.nn.Module</a> sub-class.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general
usage and behavior.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>config</strong> (<a class="reference internal" href="#transformers.TransfoXLConfig" title="transformers.TransfoXLConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">TransfoXLConfig</span></code></a>) – Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the configuration.
Check out the <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> method to load the model weights.</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="transformers.TransfoXLLMHeadModel.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input_ids=None</em>, <em>mems=None</em>, <em>head_mask=None</em>, <em>inputs_embeds=None</em>, <em>labels=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_transfo_xl.html#TransfoXLLMHeadModel.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TransfoXLLMHeadModel.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#transformers.TransfoXLLMHeadModel" title="transformers.TransfoXLLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TransfoXLLMHeadModel</span></code></a> forward method, overrides the <code class="xref py py-func docutils literal notranslate"><span class="pre">__call__()</span></code> special method.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
pre and post processing steps while the latter silently ignores them.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>) – <p>Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a class="reference internal" href="#transformers.TransfoXLTokenizer" title="transformers.TransfoXLTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.TransfoXLTokenizer</span></code></a>.
See <a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.encode" title="transformers.PreTrainedTokenizer.encode"><code class="xref py py-func docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.encode()</span></code></a> and
<a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.encode_plus" title="transformers.PreTrainedTokenizer.encode_plus"><code class="xref py py-func docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.encode_plus()</span></code></a> for details.</p>
<p><a class="reference external" href="../glossary.html#input-ids">What are input IDs?</a></p>
</li>
<li><strong>mems</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[torch.FloatTensor]</span></code> of length <code class="xref py py-obj docutils literal notranslate"><span class="pre">config.n_layers</span></code>) – Contains pre-computed hidden-states (key and values in the attention blocks) as computed by the model
(see <cite>mems</cite> output below). Can be used to speed up sequential decoding. The token ids which have their mems
given to this model should not be passed as input ids as they have already been computed.</li>
<li><strong>head_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_heads,)</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_layers,</span> <span class="pre">num_heads)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Mask to nullify selected heads of the self-attention modules.
Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:
<code class="xref py py-obj docutils literal notranslate"><span class="pre">1</span></code> indicates the head is <strong>not masked</strong>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">0</span></code> indicates the head is <strong>masked</strong>.</li>
<li><strong>input_embeds</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Optionally, instead of passing <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <cite>input_ids</cite> indices into associated vectors
than the model’s internal embedding lookup matrix.</li>
<li><strong>labels</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Labels for language modeling.
Note that the labels <strong>are shifted</strong> inside the model, i.e. you can set <code class="docutils literal notranslate"><span class="pre">lm_labels</span> <span class="pre">=</span> <span class="pre">input_ids</span></code>
Indices are selected in <code class="docutils literal notranslate"><span class="pre">[-100,</span> <span class="pre">0,</span> <span class="pre">...,</span> <span class="pre">config.vocab_size]</span></code>
All labels set to <code class="docutils literal notranslate"><span class="pre">-100</span></code> are ignored (masked), the loss is only
computed for labels in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">...,</span> <span class="pre">config.vocab_size]</span></code></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><dl class="docutils">
<dt>loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <cite>(1,)</cite>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">labels</span></code> is provided)</dt>
<dd><p class="first last">Language modeling loss.</p>
</dd>
<dt>prediction_scores (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">config.vocab_size)</span></code>):</dt>
<dd><p class="first last">Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</dd>
<dt>mems (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[torch.FloatTensor]</span></code> of length <code class="xref py py-obj docutils literal notranslate"><span class="pre">config.n_layers</span></code>):</dt>
<dd><p class="first last">Contains pre-computed hidden-states (key and values in the attention blocks).
Can be used (see <cite>past</cite> input) to speed up sequential decoding. The token ids which have their past given to this model
should not be passed as input ids as they have already been computed.</p>
</dd>
<dt>hidden_states (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">config.output_hidden_states=True</span></code>):</dt>
<dd><p class="first">Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for the output of the embeddings + one for the output of each layer)
of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>.</p>
<p class="last">Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</dd>
<dt>attentions (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">config.output_attentions=True</span></code>):</dt>
<dd><p class="first">Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for each layer) of shape
<code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_heads,</span> <span class="pre">sequence_length,</span> <span class="pre">sequence_length)</span></code>.</p>
<p class="last">Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</dd>
</dl>
</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code> comprising various elements depending on the configuration (<a class="reference internal" href="#transformers.TransfoXLConfig" title="transformers.TransfoXLConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">TransfoXLConfig</span></code></a>) and inputs</p>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="k">import</span> <span class="n">TransfoXLTokenizer</span><span class="p">,</span> <span class="n">TransfoXLLMHeadModel</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">TransfoXLTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;transfo-xl-wt103&#39;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">TransfoXLLMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;transfo-xl-wt103&#39;</span><span class="p">)</span>
<span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">&quot;Hello, my dog is cute&quot;</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Batch size 1</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>
<span class="n">prediction_scores</span><span class="p">,</span> <span class="n">mems</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="transformers.TransfoXLLMHeadModel.get_output_embeddings">
<code class="descname">get_output_embeddings</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_transfo_xl.html#TransfoXLLMHeadModel.get_output_embeddings"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TransfoXLLMHeadModel.get_output_embeddings" title="Permalink to this definition">¶</a></dt>
<dd><p>Double-check if you are using adaptive softmax.</p>
</dd></dl>

<dl class="method">
<dt id="transformers.TransfoXLLMHeadModel.tie_weights">
<code class="descname">tie_weights</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_transfo_xl.html#TransfoXLLMHeadModel.tie_weights"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TransfoXLLMHeadModel.tie_weights" title="Permalink to this definition">¶</a></dt>
<dd><p>Run this to be sure output and input (adaptive) softmax weights are tied</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="tftransfoxlmodel">
<h2>TFTransfoXLModel<a class="headerlink" href="#tftransfoxlmodel" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="tftransfoxllmheadmodel">
<h2>TFTransfoXLLMHeadModel<a class="headerlink" href="#tftransfoxllmheadmodel" title="Permalink to this headline">¶</a></h2>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="gpt2.html" class="btn btn-neutral float-right" title="OpenAI GPT2" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="gpt.html" class="btn btn-neutral" title="OpenAI GPT" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, huggingface

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
        <script type="text/javascript" src="../_static/js/custom.js"></script>
    

  

  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>