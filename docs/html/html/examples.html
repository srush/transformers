

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Examples &mdash; transformers 2.6.0 documentation</title>
  

  
  
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/huggingface.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/code-snippets.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Notebooks" href="notebooks.html" />
    <link rel="prev" title="Model Sharing" href="model_sharing.html" /> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html" class="icon icon-home"> transformers
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="pretrained_models.html">Pretrained models</a></li>
<li class="toctree-l1"><a class="reference internal" href="usage.html">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_sharing.html">Model Sharing</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Examples</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#tensorflow-2-0-bert-models-on-glue">TensorFlow 2.0 Bert models on GLUE</a></li>
<li class="toctree-l2"><a class="reference internal" href="#language-model-training">Language model training</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#gpt-2-gpt-and-causal-language-modeling">GPT-2/GPT and causal language modeling</a></li>
<li class="toctree-l3"><a class="reference internal" href="#roberta-bert-and-masked-language-modeling">RoBERTa/BERT and masked language modeling</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#language-generation">Language generation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#glue">GLUE</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#mrpc">MRPC</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#fine-tuning-example">Fine-tuning example</a></li>
<li class="toctree-l4"><a class="reference internal" href="#using-apex-and-mixed-precision">Using Apex and mixed-precision</a></li>
<li class="toctree-l4"><a class="reference internal" href="#distributed-training">Distributed training</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#mnli">MNLI</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#multiple-choice">Multiple Choice</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#fine-tuning-on-swag">Fine-tuning on SWAG</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#squad">SQuAD</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#fine-tuning-bert-on-squad1-0">Fine-tuning BERT on SQuAD1.0</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id1">Distributed training</a></li>
<li class="toctree-l3"><a class="reference internal" href="#fine-tuning-xlnet-on-squad">Fine-tuning XLNet on SQuAD</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#command-for-squad1-0">Command for SQuAD1.0:</a></li>
<li class="toctree-l4"><a class="reference internal" href="#command-for-squad2-0">Command for SQuAD2.0:</a></li>
<li class="toctree-l4"><a class="reference internal" href="#results-for-squad1-0-with-the-previously-defined-hyper-parameters">Results for SQuAD1.0 with the previously defined hyper-parameters:</a></li>
<li class="toctree-l4"><a class="reference internal" href="#results-for-squad2-0-with-the-previously-defined-hyper-parameters">Results for SQuAD2.0 with the previously defined hyper-parameters:</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#xnli">XNLI</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#fine-tuning-on-xnli">Fine-tuning on XNLI</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#mm-imdb">MM-IMDb</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#training-on-mm-imdb">Training on MM-IMDb</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#adversarial-evaluation-of-model-performances">Adversarial evaluation of model performances</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="notebooks.html">Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="serialization.html">Loading Google AI or OpenAI pre-trained weights or PyTorch dump</a></li>
<li class="toctree-l1"><a class="reference internal" href="serialization.html#serialization-best-practices">Serialization best-practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="converting_tensorflow_models.html">Converting Tensorflow Checkpoints</a></li>
<li class="toctree-l1"><a class="reference internal" href="torchscript.html">Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="multilingual.html">Multi-lingual models</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmarks.html">Benchmarks</a></li>
</ul>
<p class="caption"><span class="caption-text">Core Classes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="main_classes/configuration.html">Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/model.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/tokenizer.html">Tokenizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/pipelines.html">Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/optimizer_schedules.html">Optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/optimizer_schedules.html#schedules">Schedules</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/optimizer_schedules.html#gradient-strategies">Gradient Strategies</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/processors.html">Processors</a></li>
</ul>
<p class="caption"><span class="caption-text">Features</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="migration.html">Migrating from pytorch-pretrained-bert</a></li>
<li class="toctree-l1"><a class="reference internal" href="glossary.html">Glossary</a></li>
<li class="toctree-l1"><a class="reference internal" href="bertology.html">BERTology</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
</ul>
<p class="caption"><span class="caption-text">Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="model_doc/auto.html">AutoModels</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/bert.html">BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/gpt.html">OpenAI GPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/transformerxl.html">Transformer XL</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/gpt2.html">OpenAI GPT2</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/xlm.html">XLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/xlnet.html">XLNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/roberta.html">RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/distilbert.html">DistilBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/ctrl.html">CTRL</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/camembert.html">CamemBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/albert.html">ALBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/xlmroberta.html">XLM-RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/flaubert.html">FlauBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/bart.html">Bart</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/t5.html">T5</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">transformers</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Examples</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/examples.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="examples">
<h1>Examples<a class="headerlink" href="#examples" title="Permalink to this headline">¶</a></h1>
<p>In this section a few examples are put together. All of these examples work for several models, making use of the very
similar API between the different models.</p>
<p><strong>Important</strong>
To run the latest versions of the examples, you have to install from source and install some specific requirements for the examples.
Execute the following steps in a new virtual environment:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git clone https://github.com/huggingface/transformers
<span class="nb">cd</span> transformers
pip install .
pip install -r ./examples/requirements.txt
</pre></div>
</div>
<table border="1" class="docutils">
<thead>
<tr>
<th>Section</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="#TensorFlow-2.0-Bert-models-on-GLUE">TensorFlow 2.0 models on GLUE</a></td>
<td>Examples running BERT TensorFlow 2.0 model on the GLUE tasks.</td>
</tr>
<tr>
<td><a href="#language-model-training">Language Model training</a></td>
<td>Fine-tuning (or training from scratch) the library models for language modeling on a text dataset. Causal language modeling for GPT/GPT-2, masked language modeling for BERT/RoBERTa.</td>
</tr>
<tr>
<td><a href="#language-generation">Language Generation</a></td>
<td>Conditional text generation using the auto-regressive models of the library: GPT, GPT-2, Transformer-XL and XLNet.</td>
</tr>
<tr>
<td><a href="#glue">GLUE</a></td>
<td>Examples running BERT/XLM/XLNet/RoBERTa on the 9 GLUE tasks. Examples feature distributed training as well as half-precision.</td>
</tr>
<tr>
<td><a href="#squad">SQuAD</a></td>
<td>Using BERT/RoBERTa/XLNet/XLM for question answering, examples with distributed training.</td>
</tr>
<tr>
<td><a href="#multiple-choice">Multiple Choice</a></td>
<td>Examples running BERT/XLNet/RoBERTa on the SWAG/RACE/ARC tasks.</td>
</tr>
<tr>
<td><a href="https://github.com/huggingface/transformers/tree/master/examples/ner">Named Entity Recognition</a></td>
<td>Using BERT for Named Entity Recognition (NER) on the CoNLL 2003 dataset, examples with distributed training.</td>
</tr>
<tr>
<td><a href="#xnli">XNLI</a></td>
<td>Examples running BERT/XLM on the XNLI benchmark.</td>
</tr>
<tr>
<td><a href="#adversarial-evaluation-of-model-performances">Adversarial evaluation of model performances</a></td>
<td>Testing a model with adversarial evaluation of natural language inference on the Heuristic Analysis for NLI Systems (HANS) dataset (McCoy et al., 2019.)</td>
</tr>
</tbody>
</table><div class="section" id="tensorflow-2-0-bert-models-on-glue">
<h2>TensorFlow 2.0 Bert models on GLUE<a class="headerlink" href="#tensorflow-2-0-bert-models-on-glue" title="Permalink to this headline">¶</a></h2>
<p>Based on the script <a class="reference external" href="https://github.com/huggingface/transformers/blob/master/examples/run_tf_glue.py"><code class="docutils literal notranslate"><span class="pre">run_tf_glue.py</span></code></a>.</p>
<p>Fine-tuning the library TensorFlow 2.0 Bert model for sequence classification on the  MRPC task of the GLUE benchmark: <a class="reference external" href="https://gluebenchmark.com/">General Language Understanding Evaluation</a>.</p>
<p>This script has an option for mixed precision (Automatic Mixed Precision / AMP) to run models on Tensor Cores (NVIDIA Volta/Turing GPUs) and future hardware and an option for XLA, which uses the XLA compiler to reduce model runtime.
Options are toggled using <code class="docutils literal notranslate"><span class="pre">USE_XLA</span></code> or <code class="docutils literal notranslate"><span class="pre">USE_AMP</span></code> variables in the script.
These options and the below benchmark are provided by &#64;tlkh.</p>
<p>Quick benchmarks from the script (no other modifications):</p>
<table border="1" class="docutils">
<thead>
<tr>
<th>GPU</th>
<th>Mode</th>
<th>Time (2nd epoch)</th>
<th>Val Acc (3 runs)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Titan V</td>
<td>FP32</td>
<td>41s</td>
<td>0.8438/0.8281/0.8333</td>
</tr>
<tr>
<td>Titan V</td>
<td>AMP</td>
<td>26s</td>
<td>0.8281/0.8568/0.8411</td>
</tr>
<tr>
<td>V100</td>
<td>FP32</td>
<td>35s</td>
<td>0.8646/0.8359/0.8464</td>
</tr>
<tr>
<td>V100</td>
<td>AMP</td>
<td>22s</td>
<td>0.8646/0.8385/0.8411</td>
</tr>
<tr>
<td>1080 Ti</td>
<td>FP32</td>
<td>55s</td>
<td>-</td>
</tr>
</tbody>
</table><p>Mixed precision (AMP) reduces the training time considerably for the same hardware and hyper-parameters (same batch size was used).</p>
</div>
<div class="section" id="language-model-training">
<h2>Language model training<a class="headerlink" href="#language-model-training" title="Permalink to this headline">¶</a></h2>
<p>Based on the script <a class="reference external" href="https://github.com/huggingface/transformers/blob/master/examples/run_language_modeling.py"><code class="docutils literal notranslate"><span class="pre">run_language_modeling.py</span></code></a>.</p>
<p>Fine-tuning (or training from scratch) the library models for language modeling on a text dataset for GPT, GPT-2, BERT and RoBERTa (DistilBERT
to be added soon). GPT and GPT-2 are fine-tuned using a causal language modeling (CLM) loss while BERT and RoBERTa
are fine-tuned using a masked language modeling (MLM) loss.</p>
<p>Before running the following example, you should get a file that contains text on which the language model will be
trained or fine-tuned. A good example of such text is the <a class="reference external" href="https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/">WikiText-2 dataset</a>.</p>
<p>We will refer to two different files: <code class="docutils literal notranslate"><span class="pre">$TRAIN_FILE</span></code>, which contains text for training, and <code class="docutils literal notranslate"><span class="pre">$TEST_FILE</span></code>, which contains
text that will be used for evaluation.</p>
<div class="section" id="gpt-2-gpt-and-causal-language-modeling">
<h3>GPT-2/GPT and causal language modeling<a class="headerlink" href="#gpt-2-gpt-and-causal-language-modeling" title="Permalink to this headline">¶</a></h3>
<p>The following example fine-tunes GPT-2 on WikiText-2. We’re using the raw WikiText-2 (no tokens were replaced before
the tokenization). The loss here is that of causal language modeling.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span> <span class="nv">TRAIN_FILE</span><span class="o">=</span>/path/to/dataset/wiki.train.raw
<span class="nb">export</span> <span class="nv">TEST_FILE</span><span class="o">=</span>/path/to/dataset/wiki.test.raw

python run_language_modeling.py <span class="se">\</span>
    --output_dir<span class="o">=</span>output <span class="se">\</span>
    --model_type<span class="o">=</span>gpt2 <span class="se">\</span>
    --model_name_or_path<span class="o">=</span>gpt2 <span class="se">\</span>
    --do_train <span class="se">\</span>
    --train_data_file<span class="o">=</span><span class="nv">$TRAIN_FILE</span> <span class="se">\</span>
    --do_eval <span class="se">\</span>
    --eval_data_file<span class="o">=</span><span class="nv">$TEST_FILE</span>
</pre></div>
</div>
<p>This takes about half an hour to train on a single K80 GPU and about one minute for the evaluation to run. It reaches
a score of ~20 perplexity once fine-tuned on the dataset.</p>
</div>
<div class="section" id="roberta-bert-and-masked-language-modeling">
<h3>RoBERTa/BERT and masked language modeling<a class="headerlink" href="#roberta-bert-and-masked-language-modeling" title="Permalink to this headline">¶</a></h3>
<p>The following example fine-tunes RoBERTa on WikiText-2. Here too, we’re using the raw WikiText-2. The loss is different
as BERT/RoBERTa have a bidirectional mechanism; we’re therefore using the same loss that was used during their
pre-training: masked language modeling.</p>
<p>In accordance to the RoBERTa paper, we use dynamic masking rather than static masking. The model may, therefore, converge
slightly slower (over-fitting takes more epochs).</p>
<p>We use the <code class="docutils literal notranslate"><span class="pre">--mlm</span></code> flag so that the script may change its loss function.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span> <span class="nv">TRAIN_FILE</span><span class="o">=</span>/path/to/dataset/wiki.train.raw
<span class="nb">export</span> <span class="nv">TEST_FILE</span><span class="o">=</span>/path/to/dataset/wiki.test.raw

python run_language_modeling.py <span class="se">\</span>
    --output_dir<span class="o">=</span>output <span class="se">\</span>
    --model_type<span class="o">=</span>roberta <span class="se">\</span>
    --model_name_or_path<span class="o">=</span>roberta-base <span class="se">\</span>
    --do_train <span class="se">\</span>
    --train_data_file<span class="o">=</span><span class="nv">$TRAIN_FILE</span> <span class="se">\</span>
    --do_eval <span class="se">\</span>
    --eval_data_file<span class="o">=</span><span class="nv">$TEST_FILE</span> <span class="se">\</span>
    --mlm
</pre></div>
</div>
</div>
</div>
<div class="section" id="language-generation">
<h2>Language generation<a class="headerlink" href="#language-generation" title="Permalink to this headline">¶</a></h2>
<p>Based on the script <a class="reference external" href="https://github.com/huggingface/transformers/blob/master/examples/run_generation.py"><code class="docutils literal notranslate"><span class="pre">run_generation.py</span></code></a>.</p>
<p>Conditional text generation using the auto-regressive models of the library: GPT, GPT-2, Transformer-XL, XLNet, CTRL.
A similar script is used for our official demo <a class="reference external" href="https://transformer.huggingface.co">Write With Transfomer</a>, where you
can try out the different models available in the library.</p>
<p>Example usage:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python run_generation.py <span class="se">\</span>
    --model_type<span class="o">=</span>gpt2 <span class="se">\</span>
    --model_name_or_path<span class="o">=</span>gpt2
</pre></div>
</div>
</div>
<div class="section" id="glue">
<h2>GLUE<a class="headerlink" href="#glue" title="Permalink to this headline">¶</a></h2>
<p>Based on the script <a class="reference external" href="https://github.com/huggingface/transformers/blob/master/examples/run_glue.py"><code class="docutils literal notranslate"><span class="pre">run_glue.py</span></code></a>.</p>
<p>Fine-tuning the library models for sequence classification on the GLUE benchmark: <a class="reference external" href="https://gluebenchmark.com/">General Language Understanding
Evaluation</a>. This script can fine-tune the following models: BERT, XLM, XLNet and RoBERTa.</p>
<p>GLUE is made up of a total of 9 different tasks. We get the following results on the dev set of the benchmark with an
uncased  BERT base model (the checkpoint <code class="docutils literal notranslate"><span class="pre">bert-base-uncased</span></code>). All experiments ran single V100 GPUs with a total train
batch sizes between 16 and 64. Some of these tasks have a small dataset and training can lead to high variance in the results
between different runs. We report the median on 5 runs (with different seeds) for each of the metrics.</p>
<table border="1" class="docutils">
<thead>
<tr>
<th>Task</th>
<th>Metric</th>
<th>Result</th>
</tr>
</thead>
<tbody>
<tr>
<td>CoLA</td>
<td>Matthew's corr</td>
<td>49.23</td>
</tr>
<tr>
<td>SST-2</td>
<td>Accuracy</td>
<td>91.97</td>
</tr>
<tr>
<td>MRPC</td>
<td>F1/Accuracy</td>
<td>89.47/85.29</td>
</tr>
<tr>
<td>STS-B</td>
<td>Person/Spearman corr.</td>
<td>83.95/83.70</td>
</tr>
<tr>
<td>QQP</td>
<td>Accuracy/F1</td>
<td>88.40/84.31</td>
</tr>
<tr>
<td>MNLI</td>
<td>Matched acc./Mismatched acc.</td>
<td>80.61/81.08</td>
</tr>
<tr>
<td>QNLI</td>
<td>Accuracy</td>
<td>87.46</td>
</tr>
<tr>
<td>RTE</td>
<td>Accuracy</td>
<td>61.73</td>
</tr>
<tr>
<td>WNLI</td>
<td>Accuracy</td>
<td>45.07</td>
</tr>
</tbody>
</table><p>Some of these results are significantly different from the ones reported on the test set
of GLUE benchmark on the website. For QQP and WNLI, please refer to <a class="reference external" href="https://gluebenchmark.com/faq">FAQ #12</a> on the webite.</p>
<p>Before running any one of these GLUE tasks you should download the
<a class="reference external" href="https://gluebenchmark.com/tasks">GLUE data</a> by running
<a class="reference external" href="https://gist.github.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e">this script</a>
and unpack it to some directory <code class="docutils literal notranslate"><span class="pre">$GLUE_DIR</span></code>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span> <span class="nv">GLUE_DIR</span><span class="o">=</span>/path/to/glue
<span class="nb">export</span> <span class="nv">TASK_NAME</span><span class="o">=</span>MRPC

python run_glue.py <span class="se">\</span>
  --model_type bert <span class="se">\</span>
  --model_name_or_path bert-base-cased <span class="se">\</span>
  --task_name <span class="nv">$TASK_NAME</span> <span class="se">\</span>
  --do_train <span class="se">\</span>
  --do_eval <span class="se">\</span>
  --do_lower_case <span class="se">\</span>
  --data_dir <span class="nv">$GLUE_DIR</span>/<span class="nv">$TASK_NAME</span> <span class="se">\</span>
  --max_seq_length <span class="m">128</span> <span class="se">\</span>
  --per_gpu_train_batch_size <span class="m">32</span> <span class="se">\</span>
  --learning_rate 2e-5 <span class="se">\</span>
  --num_train_epochs <span class="m">3</span>.0 <span class="se">\</span>
  --output_dir /tmp/<span class="nv">$TASK_NAME</span>/
</pre></div>
</div>
<p>where task name can be one of CoLA, SST-2, MRPC, STS-B, QQP, MNLI, QNLI, RTE, WNLI.</p>
<p>The dev set results will be present within the text file <code class="docutils literal notranslate"><span class="pre">eval_results.txt</span></code> in the specified output_dir.
In case of MNLI, since there are two separate dev sets (matched and mismatched), there will be a separate
output folder called <code class="docutils literal notranslate"><span class="pre">/tmp/MNLI-MM/</span></code> in addition to <code class="docutils literal notranslate"><span class="pre">/tmp/MNLI/</span></code>.</p>
<p>The code has not been tested with half-precision training with apex on any GLUE task apart from MRPC, MNLI,
CoLA, SST-2. The following section provides details on how to run half-precision training with MRPC. With that being
said, there shouldn’t be any issues in running half-precision training with the remaining GLUE tasks as well,
since the data processor for each task inherits from the base class DataProcessor.</p>
<div class="section" id="mrpc">
<h3>MRPC<a class="headerlink" href="#mrpc" title="Permalink to this headline">¶</a></h3>
<div class="section" id="fine-tuning-example">
<h4>Fine-tuning example<a class="headerlink" href="#fine-tuning-example" title="Permalink to this headline">¶</a></h4>
<p>The following examples fine-tune BERT on the Microsoft Research Paraphrase Corpus (MRPC) corpus and runs in less
than 10 minutes on a single K-80 and in 27 seconds (!) on single tesla V100 16GB with apex installed.</p>
<p>Before running any one of these GLUE tasks you should download the
<a class="reference external" href="https://gluebenchmark.com/tasks">GLUE data</a> by running
<a class="reference external" href="https://gist.github.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e">this script</a>
and unpack it to some directory <code class="docutils literal notranslate"><span class="pre">$GLUE_DIR</span></code>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span> <span class="nv">GLUE_DIR</span><span class="o">=</span>/path/to/glue

python run_glue.py <span class="se">\</span>
  --model_type bert <span class="se">\</span>
  --model_name_or_path bert-base-cased <span class="se">\</span>
  --task_name MRPC <span class="se">\</span>
  --do_train <span class="se">\</span>
  --do_eval <span class="se">\</span>
  --do_lower_case <span class="se">\</span>
  --data_dir <span class="nv">$GLUE_DIR</span>/MRPC/ <span class="se">\</span>
  --max_seq_length <span class="m">128</span> <span class="se">\</span>
  --per_gpu_train_batch_size <span class="m">32</span> <span class="se">\</span>
  --learning_rate 2e-5 <span class="se">\</span>
  --num_train_epochs <span class="m">3</span>.0 <span class="se">\</span>
  --output_dir /tmp/mrpc_output/
</pre></div>
</div>
<p>Our test ran on a few seeds with <a class="reference external" href="https://github.com/google-research/bert#sentence-and-sentence-pair-classification-tasks">the original implementation hyper-
parameters</a> gave evaluation
results between 84% and 88%.</p>
</div>
<div class="section" id="using-apex-and-mixed-precision">
<h4>Using Apex and mixed-precision<a class="headerlink" href="#using-apex-and-mixed-precision" title="Permalink to this headline">¶</a></h4>
<p>Using Apex and 16 bit precision, the fine-tuning on MRPC only takes 27 seconds. First install
<a class="reference external" href="https://github.com/NVIDIA/apex">apex</a>, then run the following example:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span> <span class="nv">GLUE_DIR</span><span class="o">=</span>/path/to/glue

python run_glue.py <span class="se">\</span>
  --model_type bert <span class="se">\</span>
  --model_name_or_path bert-base-cased <span class="se">\</span>
  --task_name MRPC <span class="se">\</span>
  --do_train <span class="se">\</span>
  --do_eval <span class="se">\</span>
  --do_lower_case <span class="se">\</span>
  --data_dir <span class="nv">$GLUE_DIR</span>/MRPC/ <span class="se">\</span>
  --max_seq_length <span class="m">128</span> <span class="se">\</span>
  --per_gpu_train_batch_size <span class="m">32</span> <span class="se">\</span>
  --learning_rate 2e-5 <span class="se">\</span>
  --num_train_epochs <span class="m">3</span>.0 <span class="se">\</span>
  --output_dir /tmp/mrpc_output/ <span class="se">\</span>
  --fp16
</pre></div>
</div>
</div>
<div class="section" id="distributed-training">
<h4>Distributed training<a class="headerlink" href="#distributed-training" title="Permalink to this headline">¶</a></h4>
<p>Here is an example using distributed training on 8 V100 GPUs. The model used is the BERT whole-word-masking and it
reaches F1 &gt; 92 on MRPC.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span> <span class="nv">GLUE_DIR</span><span class="o">=</span>/path/to/glue

python -m torch.distributed.launch <span class="se">\</span>
    --nproc_per_node <span class="m">8</span> run_glue.py <span class="se">\</span>
    --model_type bert <span class="se">\</span>
    --model_name_or_path bert-base-cased <span class="se">\</span>
    --task_name MRPC <span class="se">\</span>
    --do_train <span class="se">\</span>
    --do_eval <span class="se">\</span>
    --do_lower_case <span class="se">\</span>
    --data_dir <span class="nv">$GLUE_DIR</span>/MRPC/ <span class="se">\</span>
    --max_seq_length <span class="m">128</span> <span class="se">\</span>
    --per_gpu_train_batch_size <span class="m">8</span> <span class="se">\</span>
    --learning_rate 2e-5 <span class="se">\</span>
    --num_train_epochs <span class="m">3</span>.0 <span class="se">\</span>
    --output_dir /tmp/mrpc_output/
</pre></div>
</div>
<p>Training with these hyper-parameters gave us the following results:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">acc</span> <span class="o">=</span> <span class="m">0</span>.8823529411764706
<span class="nv">acc_and_f1</span> <span class="o">=</span> <span class="m">0</span>.901702786377709
<span class="nv">eval_loss</span> <span class="o">=</span> <span class="m">0</span>.3418912578906332
<span class="nv">f1</span> <span class="o">=</span> <span class="m">0</span>.9210526315789473
<span class="nv">global_step</span> <span class="o">=</span> <span class="m">174</span>
<span class="nv">loss</span> <span class="o">=</span> <span class="m">0</span>.07231863956341798
</pre></div>
</div>
</div>
</div>
<div class="section" id="mnli">
<h3>MNLI<a class="headerlink" href="#mnli" title="Permalink to this headline">¶</a></h3>
<p>The following example uses the BERT-large, uncased, whole-word-masking model and fine-tunes it on the MNLI task.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span> <span class="nv">GLUE_DIR</span><span class="o">=</span>/path/to/glue

python -m torch.distributed.launch <span class="se">\</span>
    --nproc_per_node <span class="m">8</span> run_glue.py <span class="se">\</span>
    --model_type bert <span class="se">\</span>
    --model_name_or_path bert-base-cased <span class="se">\</span>
    --task_name mnli <span class="se">\</span>
    --do_train <span class="se">\</span>
    --do_eval <span class="se">\</span>
    --do_lower_case <span class="se">\</span>
    --data_dir <span class="nv">$GLUE_DIR</span>/MNLI/ <span class="se">\</span>
    --max_seq_length <span class="m">128</span> <span class="se">\</span>
    --per_gpu_train_batch_size <span class="m">8</span> <span class="se">\</span>
    --learning_rate 2e-5 <span class="se">\</span>
    --num_train_epochs <span class="m">3</span>.0 <span class="se">\</span>
    --output_dir output_dir <span class="se">\</span>
</pre></div>
</div>
<p>The results  are the following:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>***** Eval results *****
  <span class="nv">acc</span> <span class="o">=</span> <span class="m">0</span>.8679706601466992
  <span class="nv">eval_loss</span> <span class="o">=</span> <span class="m">0</span>.4911287787382479
  <span class="nv">global_step</span> <span class="o">=</span> <span class="m">18408</span>
  <span class="nv">loss</span> <span class="o">=</span> <span class="m">0</span>.04755385363816904

***** Eval results *****
  <span class="nv">acc</span> <span class="o">=</span> <span class="m">0</span>.8747965825874695
  <span class="nv">eval_loss</span> <span class="o">=</span> <span class="m">0</span>.45516540421714036
  <span class="nv">global_step</span> <span class="o">=</span> <span class="m">18408</span>
  <span class="nv">loss</span> <span class="o">=</span> <span class="m">0</span>.04755385363816904
</pre></div>
</div>
</div>
</div>
<div class="section" id="multiple-choice">
<h2>Multiple Choice<a class="headerlink" href="#multiple-choice" title="Permalink to this headline">¶</a></h2>
<p>Based on the script <a class="reference external" href="#"><code class="docutils literal notranslate"><span class="pre">run_multiple_choice.py</span></code></a>.</p>
<div class="section" id="fine-tuning-on-swag">
<h3>Fine-tuning on SWAG<a class="headerlink" href="#fine-tuning-on-swag" title="Permalink to this headline">¶</a></h3>
<p>Download <a class="reference external" href="https://github.com/rowanz/swagaf/tree/master/data">swag</a> data</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1">#training on 4 tesla V100(16GB) GPUS</span>
<span class="nb">export</span> <span class="nv">SWAG_DIR</span><span class="o">=</span>/path/to/swag_data_dir
python ./examples/run_multiple_choice.py <span class="se">\</span>
--model_type roberta <span class="se">\</span>
--task_name swag <span class="se">\</span>
--model_name_or_path roberta-base <span class="se">\</span>
--do_train <span class="se">\</span>
--do_eval <span class="se">\</span>
--do_lower_case <span class="se">\</span>
--data_dir <span class="nv">$SWAG_DIR</span> <span class="se">\</span>
--learning_rate 5e-5 <span class="se">\</span>
--num_train_epochs <span class="m">3</span> <span class="se">\</span>
--max_seq_length <span class="m">80</span> <span class="se">\</span>
--output_dir models_bert/swag_base <span class="se">\</span>
--per_gpu_eval_batch_size<span class="o">=</span><span class="m">16</span> <span class="se">\</span>
--per_gpu_train_batch_size<span class="o">=</span><span class="m">16</span> <span class="se">\</span>
--gradient_accumulation_steps <span class="m">2</span> <span class="se">\</span>
--overwrite_output
</pre></div>
</div>
<p>Training with the defined hyper-parameters yields the following results:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">*****</span> <span class="n">Eval</span> <span class="n">results</span> <span class="o">*****</span>
<span class="n">eval_acc</span> <span class="o">=</span> <span class="mf">0.8338998300509847</span>
<span class="n">eval_loss</span> <span class="o">=</span> <span class="mf">0.44457291918821606</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="squad">
<h2>SQuAD<a class="headerlink" href="#squad" title="Permalink to this headline">¶</a></h2>
<p>Based on the script <a class="reference external" href="https://github.com/huggingface/transformers/blob/master/examples/run_squad.py"><code class="docutils literal notranslate"><span class="pre">run_squad.py</span></code></a>.</p>
<div class="section" id="fine-tuning-bert-on-squad1-0">
<h3>Fine-tuning BERT on SQuAD1.0<a class="headerlink" href="#fine-tuning-bert-on-squad1-0" title="Permalink to this headline">¶</a></h3>
<p>This example code fine-tunes BERT on the SQuAD1.0 dataset. It runs in 24 min (with BERT-base) or 68 min (with BERT-large)
on a single tesla V100 16GB. The data for SQuAD can be downloaded with the following links and should be saved in a
$SQUAD_DIR directory.</p>
<ul class="simple">
<li><a class="reference external" href="https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json">train-v1.1.json</a></li>
<li><a class="reference external" href="https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json">dev-v1.1.json</a></li>
<li><a class="reference external" href="https://github.com/allenai/bi-att-flow/blob/master/squad/evaluate-v1.1.py">evaluate-v1.1.py</a></li>
</ul>
<p>And for SQuAD2.0, you need to download:</p>
<ul class="simple">
<li><a class="reference external" href="https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json">train-v2.0.json</a></li>
<li><a class="reference external" href="https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json">dev-v2.0.json</a></li>
<li><a class="reference external" href="https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/">evaluate-v2.0.py</a></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span> <span class="nv">SQUAD_DIR</span><span class="o">=</span>/path/to/SQUAD

python run_squad.py <span class="se">\</span>
  --model_type bert <span class="se">\</span>
  --model_name_or_path bert-base-uncased <span class="se">\</span>
  --do_train <span class="se">\</span>
  --do_eval <span class="se">\</span>
  --do_lower_case <span class="se">\</span>
  --train_file <span class="nv">$SQUAD_DIR</span>/train-v1.1.json <span class="se">\</span>
  --predict_file <span class="nv">$SQUAD_DIR</span>/dev-v1.1.json <span class="se">\</span>
  --per_gpu_train_batch_size <span class="m">12</span> <span class="se">\</span>
  --learning_rate 3e-5 <span class="se">\</span>
  --num_train_epochs <span class="m">2</span>.0 <span class="se">\</span>
  --max_seq_length <span class="m">384</span> <span class="se">\</span>
  --doc_stride <span class="m">128</span> <span class="se">\</span>
  --output_dir /tmp/debug_squad/
</pre></div>
</div>
<p>Training with the previously defined hyper-parameters yields the following results:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">f1</span> <span class="o">=</span> <span class="m">88</span>.52
<span class="nv">exact_match</span> <span class="o">=</span> <span class="m">81</span>.22
</pre></div>
</div>
</div>
<div class="section" id="id1">
<h3>Distributed training<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<p>Here is an example using distributed training on 8 V100 GPUs and Bert Whole Word Masking uncased model to reach a F1 &gt; 93 on SQuAD1.1:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python -m torch.distributed.launch --nproc_per_node<span class="o">=</span><span class="m">8</span> ./examples/run_squad.py <span class="se">\</span>
    --model_type bert <span class="se">\</span>
    --model_name_or_path bert-large-uncased-whole-word-masking <span class="se">\</span>
    --do_train <span class="se">\</span>
    --do_eval <span class="se">\</span>
    --do_lower_case <span class="se">\</span>
    --train_file <span class="nv">$SQUAD_DIR</span>/train-v1.1.json <span class="se">\</span>
    --predict_file <span class="nv">$SQUAD_DIR</span>/dev-v1.1.json <span class="se">\</span>
    --learning_rate 3e-5 <span class="se">\</span>
    --num_train_epochs <span class="m">2</span> <span class="se">\</span>
    --max_seq_length <span class="m">384</span> <span class="se">\</span>
    --doc_stride <span class="m">128</span> <span class="se">\</span>
    --output_dir ./examples/models/wwm_uncased_finetuned_squad/ <span class="se">\</span>
    --per_gpu_eval_batch_size<span class="o">=</span><span class="m">3</span>   <span class="se">\</span>
    --per_gpu_train_batch_size<span class="o">=</span><span class="m">3</span>   <span class="se">\</span>
</pre></div>
</div>
<p>Training with the previously defined hyper-parameters yields the following results:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">f1</span> <span class="o">=</span> <span class="m">93</span>.15
<span class="nv">exact_match</span> <span class="o">=</span> <span class="m">86</span>.91
</pre></div>
</div>
<p>This fine-tuned model is available as a checkpoint under the reference
<code class="docutils literal notranslate"><span class="pre">bert-large-uncased-whole-word-masking-finetuned-squad</span></code>.</p>
</div>
<div class="section" id="fine-tuning-xlnet-on-squad">
<h3>Fine-tuning XLNet on SQuAD<a class="headerlink" href="#fine-tuning-xlnet-on-squad" title="Permalink to this headline">¶</a></h3>
<p>This example code fine-tunes XLNet on both SQuAD1.0 and SQuAD2.0 dataset. See above to download the data for SQuAD .</p>
<div class="section" id="command-for-squad1-0">
<h4>Command for SQuAD1.0:<a class="headerlink" href="#command-for-squad1-0" title="Permalink to this headline">¶</a></h4>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span> <span class="nv">SQUAD_DIR</span><span class="o">=</span>/path/to/SQUAD

python run_squad.py <span class="se">\</span>
    --model_type xlnet <span class="se">\</span>
    --model_name_or_path xlnet-large-cased <span class="se">\</span>
    --do_train <span class="se">\</span>
    --do_eval <span class="se">\</span>
    --do_lower_case <span class="se">\</span>
    --train_file <span class="nv">$SQUAD_DIR</span>/train-v1.1.json <span class="se">\</span>
    --predict_file <span class="nv">$SQUAD_DIR</span>/dev-v1.1.json <span class="se">\</span>
    --learning_rate 3e-5 <span class="se">\</span>
    --num_train_epochs <span class="m">2</span> <span class="se">\</span>
    --max_seq_length <span class="m">384</span> <span class="se">\</span>
    --doc_stride <span class="m">128</span> <span class="se">\</span>
    --output_dir ./wwm_cased_finetuned_squad/ <span class="se">\</span>
    --per_gpu_eval_batch_size<span class="o">=</span><span class="m">4</span>  <span class="se">\</span>
    --per_gpu_train_batch_size<span class="o">=</span><span class="m">4</span>   <span class="se">\</span>
    --save_steps <span class="m">5000</span>
</pre></div>
</div>
</div>
<div class="section" id="command-for-squad2-0">
<h4>Command for SQuAD2.0:<a class="headerlink" href="#command-for-squad2-0" title="Permalink to this headline">¶</a></h4>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span> <span class="nv">SQUAD_DIR</span><span class="o">=</span>/path/to/SQUAD

python run_squad.py <span class="se">\</span>
    --model_type xlnet <span class="se">\</span>
    --model_name_or_path xlnet-large-cased <span class="se">\</span>
    --do_train <span class="se">\</span>
    --do_eval <span class="se">\</span>
    --version_2_with_negative <span class="se">\</span>
    --train_file <span class="nv">$SQUAD_DIR</span>/train-v2.0.json <span class="se">\</span>
    --predict_file <span class="nv">$SQUAD_DIR</span>/dev-v2.0.json <span class="se">\</span>
    --learning_rate 3e-5 <span class="se">\</span>
    --num_train_epochs <span class="m">4</span> <span class="se">\</span>
    --max_seq_length <span class="m">384</span> <span class="se">\</span>
    --doc_stride <span class="m">128</span> <span class="se">\</span>
    --output_dir ./wwm_cased_finetuned_squad/ <span class="se">\</span>
    --per_gpu_eval_batch_size<span class="o">=</span><span class="m">2</span>  <span class="se">\</span>
    --per_gpu_train_batch_size<span class="o">=</span><span class="m">2</span>   <span class="se">\</span>
    --save_steps <span class="m">5000</span>
</pre></div>
</div>
<p>Larger batch size may improve the performance while costing more memory.</p>
</div>
<div class="section" id="results-for-squad1-0-with-the-previously-defined-hyper-parameters">
<h4>Results for SQuAD1.0 with the previously defined hyper-parameters:<a class="headerlink" href="#results-for-squad1-0-with-the-previously-defined-hyper-parameters" title="Permalink to this headline">¶</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="s2">&quot;exact&quot;</span><span class="p">:</span> <span class="mf">85.45884578997162</span><span class="p">,</span>
<span class="s2">&quot;f1&quot;</span><span class="p">:</span> <span class="mf">92.5974600601065</span><span class="p">,</span>
<span class="s2">&quot;total&quot;</span><span class="p">:</span> <span class="mi">10570</span><span class="p">,</span>
<span class="s2">&quot;HasAns_exact&quot;</span><span class="p">:</span> <span class="mf">85.45884578997162</span><span class="p">,</span>
<span class="s2">&quot;HasAns_f1&quot;</span><span class="p">:</span> <span class="mf">92.59746006010651</span><span class="p">,</span>
<span class="s2">&quot;HasAns_total&quot;</span><span class="p">:</span> <span class="mi">10570</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="section" id="results-for-squad2-0-with-the-previously-defined-hyper-parameters">
<h4>Results for SQuAD2.0 with the previously defined hyper-parameters:<a class="headerlink" href="#results-for-squad2-0-with-the-previously-defined-hyper-parameters" title="Permalink to this headline">¶</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="s2">&quot;exact&quot;</span><span class="p">:</span> <span class="mf">80.4177545691906</span><span class="p">,</span>
<span class="s2">&quot;f1&quot;</span><span class="p">:</span> <span class="mf">84.07154997729623</span><span class="p">,</span>
<span class="s2">&quot;total&quot;</span><span class="p">:</span> <span class="mi">11873</span><span class="p">,</span>
<span class="s2">&quot;HasAns_exact&quot;</span><span class="p">:</span> <span class="mf">76.73751686909581</span><span class="p">,</span>
<span class="s2">&quot;HasAns_f1&quot;</span><span class="p">:</span> <span class="mf">84.05558584352873</span><span class="p">,</span>
<span class="s2">&quot;HasAns_total&quot;</span><span class="p">:</span> <span class="mi">5928</span><span class="p">,</span>
<span class="s2">&quot;NoAns_exact&quot;</span><span class="p">:</span> <span class="mf">84.0874684608915</span><span class="p">,</span>
<span class="s2">&quot;NoAns_f1&quot;</span><span class="p">:</span> <span class="mf">84.0874684608915</span><span class="p">,</span>
<span class="s2">&quot;NoAns_total&quot;</span><span class="p">:</span> <span class="mi">5945</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="xnli">
<h2>XNLI<a class="headerlink" href="#xnli" title="Permalink to this headline">¶</a></h2>
<p>Based on the script <a class="reference external" href="https://github.com/huggingface/transformers/blob/master/examples/run_xnli.py"><code class="docutils literal notranslate"><span class="pre">run_xnli.py</span></code></a>.</p>
<p><a class="reference external" href="https://www.nyu.edu/projects/bowman/xnli/">XNLI</a> is crowd-sourced dataset based on <a class="reference external" href="http://www.nyu.edu/projects/bowman/multinli/">MultiNLI</a>. It is an evaluation benchmark for cross-lingual text representations. Pairs of text are labeled with textual entailment annotations for 15 different languages (including both high-resource language such as English and low-resource languages such as Swahili).</p>
<div class="section" id="fine-tuning-on-xnli">
<h3>Fine-tuning on XNLI<a class="headerlink" href="#fine-tuning-on-xnli" title="Permalink to this headline">¶</a></h3>
<p>This example code fine-tunes mBERT (multi-lingual BERT) on the XNLI dataset. It runs in 106 mins
on a single tesla V100 16GB. The data for XNLI can be downloaded with the following links and should be both saved (and un-zipped) in a
<code class="docutils literal notranslate"><span class="pre">$XNLI_DIR</span></code> directory.</p>
<ul class="simple">
<li><a class="reference external" href="https://www.nyu.edu/projects/bowman/xnli/XNLI-1.0.zip">XNLI 1.0</a></li>
<li><a class="reference external" href="https://www.nyu.edu/projects/bowman/xnli/XNLI-MT-1.0.zip">XNLI-MT 1.0</a></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span> <span class="nv">XNLI_DIR</span><span class="o">=</span>/path/to/XNLI

python run_xnli.py <span class="se">\</span>
  --model_type bert <span class="se">\</span>
  --model_name_or_path bert-base-multilingual-cased <span class="se">\</span>
  --language de <span class="se">\</span>
  --train_language en <span class="se">\</span>
  --do_train <span class="se">\</span>
  --do_eval <span class="se">\</span>
  --data_dir <span class="nv">$XNLI_DIR</span> <span class="se">\</span>
  --per_gpu_train_batch_size <span class="m">32</span> <span class="se">\</span>
  --learning_rate 5e-5 <span class="se">\</span>
  --num_train_epochs <span class="m">2</span>.0 <span class="se">\</span>
  --max_seq_length <span class="m">128</span> <span class="se">\</span>
  --output_dir /tmp/debug_xnli/ <span class="se">\</span>
  --save_steps -1
</pre></div>
</div>
<p>Training with the previously defined hyper-parameters yields the following results on the <strong>test</strong> set:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">acc</span> <span class="o">=</span> <span class="m">0</span>.7093812375249501
</pre></div>
</div>
</div>
</div>
<div class="section" id="mm-imdb">
<h2>MM-IMDb<a class="headerlink" href="#mm-imdb" title="Permalink to this headline">¶</a></h2>
<p>Based on the script <a class="reference external" href="https://github.com/huggingface/transformers/blob/master/examples/mm-imdb/run_mmimdb.py"><code class="docutils literal notranslate"><span class="pre">run_mmimdb.py</span></code></a>.</p>
<p><a class="reference external" href="http://lisi1.unal.edu.co/mmimdb/">MM-IMDb</a> is a Multimodal dataset with around 26,000 movies including images, plots and other metadata.</p>
<div class="section" id="training-on-mm-imdb">
<h3>Training on MM-IMDb<a class="headerlink" href="#training-on-mm-imdb" title="Permalink to this headline">¶</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">run_mmimdb</span><span class="o">.</span><span class="n">py</span> \
    <span class="o">--</span><span class="n">data_dir</span> <span class="o">/</span><span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">mmimdb</span><span class="o">/</span><span class="n">dataset</span><span class="o">/</span> \
    <span class="o">--</span><span class="n">model_type</span> <span class="n">bert</span> \
    <span class="o">--</span><span class="n">model_name_or_path</span> <span class="n">bert</span><span class="o">-</span><span class="n">base</span><span class="o">-</span><span class="n">uncased</span> \
    <span class="o">--</span><span class="n">output_dir</span> <span class="o">/</span><span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">save</span><span class="o">/</span><span class="nb">dir</span><span class="o">/</span> \
    <span class="o">--</span><span class="n">do_train</span> \
    <span class="o">--</span><span class="n">do_eval</span> \
    <span class="o">--</span><span class="n">max_seq_len</span> <span class="mi">512</span> \
    <span class="o">--</span><span class="n">gradient_accumulation_steps</span> <span class="mi">20</span> \
    <span class="o">--</span><span class="n">num_image_embeds</span> <span class="mi">3</span> \
    <span class="o">--</span><span class="n">num_train_epochs</span> <span class="mi">100</span> \
    <span class="o">--</span><span class="n">patience</span> <span class="mi">5</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="adversarial-evaluation-of-model-performances">
<h2>Adversarial evaluation of model performances<a class="headerlink" href="#adversarial-evaluation-of-model-performances" title="Permalink to this headline">¶</a></h2>
<p>Here is an example on evaluating a model using adversarial evaluation of natural language inference with the Heuristic Analysis for NLI Systems (HANS) dataset <a class="reference external" href="https://arxiv.org/abs/1902.01007">McCoy et al., 2019</a>. The example was gracefully provided by <a class="reference external" href="https://github.com/ns-moosavi">Nafise Sadat Moosavi</a>.</p>
<p>The HANS dataset can be downloaded from <a class="reference external" href="https://github.com/tommccoy1/hans">this location</a>.</p>
<p>This is an example of using test_hans.py:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span> <span class="nv">HANS_DIR</span><span class="o">=</span>path-to-hans
<span class="nb">export</span> <span class="nv">MODEL_TYPE</span><span class="o">=</span>type-of-the-model-e.g.-bert-roberta-xlnet-etc
<span class="nb">export</span> <span class="nv">MODEL_PATH</span><span class="o">=</span>path-to-the-model-directory-that-is-trained-on-NLI-e.g.-by-using-run_glue.py

python examples/hans/test_hans.py <span class="se">\</span>
        --task_name hans <span class="se">\</span>
        --model_type <span class="nv">$MODEL_TYPE</span> <span class="se">\</span>
        --do_eval <span class="se">\</span>
        --do_lower_case <span class="se">\</span>
        --data_dir <span class="nv">$HANS_DIR</span> <span class="se">\</span>
        --model_name_or_path <span class="nv">$MODEL_PATH</span> <span class="se">\</span>
        --max_seq_length <span class="m">128</span> <span class="se">\</span>
        --output_dir <span class="nv">$MODEL_PATH</span> <span class="se">\</span>
</pre></div>
</div>
<p>This will create the hans_predictions.txt file in MODEL_PATH, which can then be evaluated using hans/evaluate_heur_output.py from the HANS dataset.</p>
<p>The results of the BERT-base model that is trained on MNLI using batch size 8 and the random seed 42 on the HANS dataset is as follows:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Heuristic entailed results:
lexical_overlap: <span class="m">0</span>.9702
subsequence: <span class="m">0</span>.9942
constituent: <span class="m">0</span>.9962

Heuristic non-entailed results:
lexical_overlap: <span class="m">0</span>.199
subsequence: <span class="m">0</span>.0396
constituent: <span class="m">0</span>.118
</pre></div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="notebooks.html" class="btn btn-neutral float-right" title="Notebooks" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="model_sharing.html" class="btn btn-neutral" title="Model Sharing" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, huggingface

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
        <script type="text/javascript" src="_static/language_data.js"></script>
        <script type="text/javascript" src="_static/js/custom.js"></script>
    

  

  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>