

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Loading Google AI or OpenAI pre-trained weights or PyTorch dump &mdash; transformers 2.6.0 documentation</title>
  

  
  
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/huggingface.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/code-snippets.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Converting Tensorflow Checkpoints" href="converting_tensorflow_models.html" />
    <link rel="prev" title="Notebooks" href="notebooks.html" /> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html" class="icon icon-home"> transformers
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="pretrained_models.html">Pretrained models</a></li>
<li class="toctree-l1"><a class="reference internal" href="usage.html">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_sharing.html">Model Sharing</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks.html">Notebooks</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Loading Google AI or OpenAI pre-trained weights or PyTorch dump</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#from-pretrained-method"><code class="docutils literal notranslate"><span class="pre">from_pretrained()</span></code> method</a></li>
<li class="toctree-l2"><a class="reference internal" href="#cache-directory">Cache directory</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#serialization-best-practices">Serialization best-practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="converting_tensorflow_models.html">Converting Tensorflow Checkpoints</a></li>
<li class="toctree-l1"><a class="reference internal" href="torchscript.html">Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="multilingual.html">Multi-lingual models</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmarks.html">Benchmarks</a></li>
</ul>
<p class="caption"><span class="caption-text">Core Classes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="main_classes/configuration.html">Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/model.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/tokenizer.html">Tokenizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/pipelines.html">Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/optimizer_schedules.html">Optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/optimizer_schedules.html#schedules">Schedules</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/optimizer_schedules.html#gradient-strategies">Gradient Strategies</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/processors.html">Processors</a></li>
</ul>
<p class="caption"><span class="caption-text">Features</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="migration.html">Migrating from pytorch-pretrained-bert</a></li>
<li class="toctree-l1"><a class="reference internal" href="glossary.html">Glossary</a></li>
<li class="toctree-l1"><a class="reference internal" href="bertology.html">BERTology</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
</ul>
<p class="caption"><span class="caption-text">Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="model_doc/auto.html">AutoModels</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/bert.html">BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/gpt.html">OpenAI GPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/transformerxl.html">Transformer XL</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/gpt2.html">OpenAI GPT2</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/xlm.html">XLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/xlnet.html">XLNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/roberta.html">RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/distilbert.html">DistilBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/ctrl.html">CTRL</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/camembert.html">CamemBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/albert.html">ALBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/xlmroberta.html">XLM-RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/flaubert.html">FlauBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/bart.html">Bart</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/t5.html">T5</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">transformers</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Loading Google AI or OpenAI pre-trained weights or PyTorch dump</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/serialization.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="loading-google-ai-or-openai-pre-trained-weights-or-pytorch-dump">
<h1>Loading Google AI or OpenAI pre-trained weights or PyTorch dump<a class="headerlink" href="#loading-google-ai-or-openai-pre-trained-weights-or-pytorch-dump" title="Permalink to this headline">¶</a></h1>
<div class="section" id="from-pretrained-method">
<h2><code class="docutils literal notranslate"><span class="pre">from_pretrained()</span></code> method<a class="headerlink" href="#from-pretrained-method" title="Permalink to this headline">¶</a></h2>
<p>To load one of Google AI’s, OpenAI’s pre-trained models or a PyTorch saved model (an instance of <code class="docutils literal notranslate"><span class="pre">BertForPreTraining</span></code> saved with <code class="docutils literal notranslate"><span class="pre">torch.save()</span></code>), the PyTorch model classes and the tokenizer can be instantiated using the <code class="docutils literal notranslate"><span class="pre">from_pretrained()</span></code> method:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">BERT_CLASS</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">PRE_TRAINED_MODEL_NAME_OR_PATH</span><span class="p">,</span> <span class="n">cache_dir</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">from_tf</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">state_dict</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="o">*</span><span class="nb">input</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>
</div>
<p>where</p>
<ul>
<li><p class="first"><code class="docutils literal notranslate"><span class="pre">BERT_CLASS</span></code> is either a tokenizer to load the vocabulary (<code class="docutils literal notranslate"><span class="pre">BertTokenizer</span></code> or <code class="docutils literal notranslate"><span class="pre">OpenAIGPTTokenizer</span></code> classes) or one of the eight BERT or three OpenAI GPT PyTorch model classes (to load the pre-trained weights): <code class="docutils literal notranslate"><span class="pre">BertModel</span></code>, <code class="docutils literal notranslate"><span class="pre">BertForMaskedLM</span></code>, <code class="docutils literal notranslate"><span class="pre">BertForNextSentencePrediction</span></code>, <code class="docutils literal notranslate"><span class="pre">BertForPreTraining</span></code>, <code class="docutils literal notranslate"><span class="pre">BertForSequenceClassification</span></code>, <code class="docutils literal notranslate"><span class="pre">BertForTokenClassification</span></code>, <code class="docutils literal notranslate"><span class="pre">BertForMultipleChoice</span></code>, <code class="docutils literal notranslate"><span class="pre">BertForQuestionAnswering</span></code>, <code class="docutils literal notranslate"><span class="pre">OpenAIGPTModel</span></code>, <code class="docutils literal notranslate"><span class="pre">OpenAIGPTLMHeadModel</span></code> or <code class="docutils literal notranslate"><span class="pre">OpenAIGPTDoubleHeadsModel</span></code>, and</p>
</li>
<li><p class="first"><code class="docutils literal notranslate"><span class="pre">PRE_TRAINED_MODEL_NAME_OR_PATH</span></code> is either:</p>
<ul class="simple">
<li>the shortcut name of a Google AI’s or OpenAI’s pre-trained model selected in the list:<ul>
<li><code class="docutils literal notranslate"><span class="pre">bert-base-uncased</span></code>: 12-layer, 768-hidden, 12-heads, 110M parameters</li>
<li><code class="docutils literal notranslate"><span class="pre">bert-large-uncased</span></code>: 24-layer, 1024-hidden, 16-heads, 340M parameters</li>
<li><code class="docutils literal notranslate"><span class="pre">bert-base-cased</span></code>: 12-layer, 768-hidden, 12-heads , 110M parameters</li>
<li><code class="docutils literal notranslate"><span class="pre">bert-large-cased</span></code>: 24-layer, 1024-hidden, 16-heads, 340M parameters</li>
<li><code class="docutils literal notranslate"><span class="pre">bert-base-multilingual-uncased</span></code>: (Orig, not recommended) 102 languages, 12-layer, 768-hidden, 12-heads, 110M parameters</li>
<li><code class="docutils literal notranslate"><span class="pre">bert-base-multilingual-cased</span></code>: <strong>(New, recommended)</strong> 104 languages, 12-layer, 768-hidden, 12-heads, 110M parameters</li>
<li><code class="docutils literal notranslate"><span class="pre">bert-base-chinese</span></code>: Chinese Simplified and Traditional, 12-layer, 768-hidden, 12-heads, 110M parameters</li>
<li><code class="docutils literal notranslate"><span class="pre">bert-base-german-cased</span></code>: Trained on German data only, 12-layer, 768-hidden, 12-heads, 110M parameters <a class="reference external" href="https://deepset.ai/german-bert">Performance Evaluation</a></li>
<li><code class="docutils literal notranslate"><span class="pre">bert-large-uncased-whole-word-masking</span></code>: 24-layer, 1024-hidden, 16-heads, 340M parameters - Trained with Whole Word Masking (mask all of the the tokens corresponding to a word at once)</li>
<li><code class="docutils literal notranslate"><span class="pre">bert-large-cased-whole-word-masking</span></code>: 24-layer, 1024-hidden, 16-heads, 340M parameters - Trained with Whole Word Masking (mask all of the the tokens corresponding to a word at once)</li>
<li><code class="docutils literal notranslate"><span class="pre">bert-large-uncased-whole-word-masking-finetuned-squad</span></code>: The <code class="docutils literal notranslate"><span class="pre">bert-large-uncased-whole-word-masking</span></code> model finetuned on SQuAD (using the <code class="docutils literal notranslate"><span class="pre">run_bert_squad.py</span></code> examples). Results: <em>exact_match: 86.91579943235573, f1: 93.1532499015869</em></li>
<li><code class="docutils literal notranslate"><span class="pre">bert-base-german-dbmdz-cased</span></code>: Trained on German data only, 12-layer, 768-hidden, 12-heads, 110M parameters <a class="reference external" href="https://github.com/dbmdz/german-bert">Performance Evaluation</a></li>
<li><code class="docutils literal notranslate"><span class="pre">bert-base-german-dbmdz-uncased</span></code>: Trained on (uncased) German data only, 12-layer, 768-hidden, 12-heads, 110M parameters <a class="reference external" href="https://github.com/dbmdz/german-bert">Performance Evaluation</a></li>
<li><code class="docutils literal notranslate"><span class="pre">openai-gpt</span></code>: OpenAI GPT English model, 12-layer, 768-hidden, 12-heads, 110M parameters</li>
<li><code class="docutils literal notranslate"><span class="pre">gpt2</span></code>: OpenAI GPT-2 English model, 12-layer, 768-hidden, 12-heads, 117M parameters</li>
<li><code class="docutils literal notranslate"><span class="pre">gpt2-medium</span></code>: OpenAI GPT-2 English model, 24-layer, 1024-hidden, 16-heads, 345M parameters</li>
<li><code class="docutils literal notranslate"><span class="pre">transfo-xl-wt103</span></code>: Transformer-XL English model trained on wikitext-103, 18-layer, 1024-hidden, 16-heads, 257M parameters</li>
</ul>
</li>
<li>a path or url to a pretrained model archive containing:<ul>
<li><code class="docutils literal notranslate"><span class="pre">bert_config.json</span></code> or <code class="docutils literal notranslate"><span class="pre">openai_gpt_config.json</span></code> a configuration file for the model, and</li>
<li><code class="docutils literal notranslate"><span class="pre">pytorch_model.bin</span></code> a PyTorch dump of a pre-trained instance of <code class="docutils literal notranslate"><span class="pre">BertForPreTraining</span></code>, <code class="docutils literal notranslate"><span class="pre">OpenAIGPTModel</span></code>, <code class="docutils literal notranslate"><span class="pre">TransfoXLModel</span></code>, <code class="docutils literal notranslate"><span class="pre">GPT2LMHeadModel</span></code> (saved with the usual <code class="docutils literal notranslate"><span class="pre">torch.save()</span></code>)</li>
</ul>
</li>
</ul>
<p>If <code class="docutils literal notranslate"><span class="pre">PRE_TRAINED_MODEL_NAME_OR_PATH</span></code> is a shortcut name, the pre-trained weights will be downloaded from AWS S3 (see the links <a class="reference external" href="https://github.com/huggingface/transformers/blob/master/transformers/modeling_bert.py">here</a>) and stored in a cache folder to avoid future download (the cache folder can be found at <code class="docutils literal notranslate"><span class="pre">~/.pytorch_pretrained_bert/</span></code>).</p>
</li>
<li><p class="first"><code class="docutils literal notranslate"><span class="pre">cache_dir</span></code> can be an optional path to a specific directory to download and cache the pre-trained model weights. This option is useful in particular when you are using distributed training: to avoid concurrent access to the same weights you can set for example <code class="docutils literal notranslate"><span class="pre">cache_dir='./pretrained_model_{}'.format(args.local_rank)</span></code> (see the section on distributed training for more information).</p>
</li>
<li><p class="first"><code class="docutils literal notranslate"><span class="pre">from_tf</span></code>: should we load the weights from a locally saved TensorFlow checkpoint</p>
</li>
<li><p class="first"><code class="docutils literal notranslate"><span class="pre">state_dict</span></code>: an optional state dictionary (collections.OrderedDict object) to use instead of Google pre-trained models</p>
</li>
<li><p class="first"><code class="docutils literal notranslate"><span class="pre">*inputs</span></code>, <cite>**kwargs</cite>: additional input for the specific Bert class (ex: num_labels for BertForSequenceClassification)</p>
</li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">Uncased</span></code> means that the text has been lowercased before WordPiece tokenization, e.g., <code class="docutils literal notranslate"><span class="pre">John</span> <span class="pre">Smith</span></code> becomes <code class="docutils literal notranslate"><span class="pre">john</span> <span class="pre">smith</span></code>. The Uncased model also strips out any accent markers. <code class="docutils literal notranslate"><span class="pre">Cased</span></code> means that the true case and accent markers are preserved. Typically, the Uncased model is better unless you know that case information is important for your task (e.g., Named Entity Recognition or Part-of-Speech tagging). For information about the Multilingual and Chinese model, see the <a class="reference external" href="https://github.com/google-research/bert/blob/master/multilingual.md">Multilingual README</a> or the original TensorFlow repository.</p>
<p>When using an <code class="docutils literal notranslate"><span class="pre">uncased</span> <span class="pre">model</span></code>, make sure to pass <code class="docutils literal notranslate"><span class="pre">--do_lower_case</span></code> to the example training scripts (or pass <code class="docutils literal notranslate"><span class="pre">do_lower_case=True</span></code> to FullTokenizer if you’re using your own script and loading the tokenizer your-self.).</p>
<p>Examples:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># BERT</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">,</span> <span class="n">do_lower_case</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">do_basic_tokenize</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BertForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>

<span class="c1"># OpenAI GPT</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">OpenAIGPTTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;openai-gpt&#39;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">OpenAIGPTModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;openai-gpt&#39;</span><span class="p">)</span>

<span class="c1"># Transformer-XL</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">TransfoXLTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;transfo-xl-wt103&#39;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">TransfoXLModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;transfo-xl-wt103&#39;</span><span class="p">)</span>

<span class="c1"># OpenAI GPT-2</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">GPT2Tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;gpt2&#39;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GPT2Model</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;gpt2&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="cache-directory">
<h2>Cache directory<a class="headerlink" href="#cache-directory" title="Permalink to this headline">¶</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">pytorch_pretrained_bert</span></code> save the pretrained weights in a cache directory which is located at (in this order of priority):</p>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">cache_dir</span></code> optional arguments to the <code class="docutils literal notranslate"><span class="pre">from_pretrained()</span></code> method (see above),</li>
<li>shell environment variable <code class="docutils literal notranslate"><span class="pre">PYTORCH_PRETRAINED_BERT_CACHE</span></code>,</li>
<li>PyTorch cache home + <code class="docutils literal notranslate"><span class="pre">/pytorch_pretrained_bert/</span></code>
where PyTorch cache home is defined by (in this order):<ul>
<li>shell environment variable <code class="docutils literal notranslate"><span class="pre">ENV_TORCH_HOME</span></code></li>
<li>shell environment variable <code class="docutils literal notranslate"><span class="pre">ENV_XDG_CACHE_HOME</span></code> + <code class="docutils literal notranslate"><span class="pre">/torch/</span></code>)</li>
<li>default: <code class="docutils literal notranslate"><span class="pre">~/.cache/torch/</span></code></li>
</ul>
</li>
</ul>
<p>Usually, if you don’t set any specific environment variable, <code class="docutils literal notranslate"><span class="pre">pytorch_pretrained_bert</span></code> cache will be at <code class="docutils literal notranslate"><span class="pre">~/.cache/torch/pytorch_pretrained_bert/</span></code>.</p>
<p>You can alsways safely delete <code class="docutils literal notranslate"><span class="pre">pytorch_pretrained_bert</span></code> cache but the pretrained model weights and vocabulary files wil have to be re-downloaded from our S3.</p>
</div>
</div>
<div class="section" id="serialization-best-practices">
<h1>Serialization best-practices<a class="headerlink" href="#serialization-best-practices" title="Permalink to this headline">¶</a></h1>
<p>This section explain how you can save and re-load a fine-tuned model (BERT, GPT, GPT-2 and Transformer-XL).
There are three types of files you need to save to be able to reload a fine-tuned model:</p>
<ul class="simple">
<li>the model itself which should be saved following PyTorch serialization <a class="reference external" href="https://pytorch.org/docs/stable/notes/serialization.html#best-practices">best practices</a>,</li>
<li>the configuration file of the model which is saved as a JSON file, and</li>
<li>the vocabulary (and the merges for the BPE-based models GPT and GPT-2).</li>
</ul>
<p>The <em>default filenames</em> of these files are as follow:</p>
<ul class="simple">
<li>the model weights file: <code class="docutils literal notranslate"><span class="pre">pytorch_model.bin</span></code>,</li>
<li>the configuration file: <code class="docutils literal notranslate"><span class="pre">config.json</span></code>,</li>
<li>the vocabulary file: <code class="docutils literal notranslate"><span class="pre">vocab.txt</span></code> for BERT and Transformer-XL, <code class="docutils literal notranslate"><span class="pre">vocab.json</span></code> for GPT/GPT-2 (BPE vocabulary),</li>
<li>for GPT/GPT-2 (BPE vocabulary) the additional merges file: <code class="docutils literal notranslate"><span class="pre">merges.txt</span></code>.</li>
</ul>
<p><strong>If you save a model using these *default filenames*, you can then re-load the model and tokenizer using the ``from_pretrained()`` method.</strong></p>
<p>Here is the recommended way of saving the model, configuration and vocabulary to an <code class="docutils literal notranslate"><span class="pre">output_dir</span></code> directory and reloading the model and tokenizer afterwards:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">WEIGHTS_NAME</span><span class="p">,</span> <span class="n">CONFIG_NAME</span>

<span class="n">output_dir</span> <span class="o">=</span> <span class="s2">&quot;./models/&quot;</span>

<span class="c1"># Step 1: Save a model, configuration and vocabulary that you have fine-tuned</span>

<span class="c1"># If we have a distributed model, save only the encapsulated model</span>
<span class="c1"># (it was wrapped in PyTorch DistributedDataParallel or DataParallel)</span>
<span class="n">model_to_save</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">module</span> <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s1">&#39;module&#39;</span><span class="p">)</span> <span class="k">else</span> <span class="n">model</span>

<span class="c1"># If we save using the predefined names, we can load using `from_pretrained`</span>
<span class="n">output_model_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_dir</span><span class="p">,</span> <span class="n">WEIGHTS_NAME</span><span class="p">)</span>
<span class="n">output_config_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_dir</span><span class="p">,</span> <span class="n">CONFIG_NAME</span><span class="p">)</span>

<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model_to_save</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">output_model_file</span><span class="p">)</span>
<span class="n">model_to_save</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">to_json_file</span><span class="p">(</span><span class="n">output_config_file</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">save_vocabulary</span><span class="p">(</span><span class="n">output_dir</span><span class="p">)</span>

<span class="c1"># Step 2: Re-load the saved model and vocabulary</span>

<span class="c1"># Example for a Bert model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BertForQuestionAnswering</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">output_dir</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">output_dir</span><span class="p">,</span> <span class="n">do_lower_case</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">do_lower_case</span><span class="p">)</span>  <span class="c1"># Add specific options if needed</span>
<span class="c1"># Example for a GPT model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">OpenAIGPTDoubleHeadsModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">output_dir</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">OpenAIGPTTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">output_dir</span><span class="p">)</span>
</pre></div>
</div>
<p>Here is another way you can save and reload the model if you want to use specific paths for each type of files:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">output_model_file</span> <span class="o">=</span> <span class="s2">&quot;./models/my_own_model_file.bin&quot;</span>
<span class="n">output_config_file</span> <span class="o">=</span> <span class="s2">&quot;./models/my_own_config_file.bin&quot;</span>
<span class="n">output_vocab_file</span> <span class="o">=</span> <span class="s2">&quot;./models/my_own_vocab_file.bin&quot;</span>

<span class="c1"># Step 1: Save a model, configuration and vocabulary that you have fine-tuned</span>

<span class="c1"># If we have a distributed model, save only the encapsulated model</span>
<span class="c1"># (it was wrapped in PyTorch DistributedDataParallel or DataParallel)</span>
<span class="n">model_to_save</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">module</span> <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s1">&#39;module&#39;</span><span class="p">)</span> <span class="k">else</span> <span class="n">model</span>

<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model_to_save</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">output_model_file</span><span class="p">)</span>
<span class="n">model_to_save</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">to_json_file</span><span class="p">(</span><span class="n">output_config_file</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">save_vocabulary</span><span class="p">(</span><span class="n">output_vocab_file</span><span class="p">)</span>

<span class="c1"># Step 2: Re-load the saved model and vocabulary</span>

<span class="c1"># We didn&#39;t save using the predefined WEIGHTS_NAME, CONFIG_NAME names, we cannot load using `from_pretrained`.</span>
<span class="c1"># Here is how to do it in this situation:</span>

<span class="c1"># Example for a Bert model</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">BertConfig</span><span class="o">.</span><span class="n">from_json_file</span><span class="p">(</span><span class="n">output_config_file</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BertForQuestionAnswering</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
<span class="n">state_dict</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">output_model_file</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="p">(</span><span class="n">output_vocab_file</span><span class="p">,</span> <span class="n">do_lower_case</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">do_lower_case</span><span class="p">)</span>

<span class="c1"># Example for a GPT model</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">OpenAIGPTConfig</span><span class="o">.</span><span class="n">from_json_file</span><span class="p">(</span><span class="n">output_config_file</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">OpenAIGPTDoubleHeadsModel</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
<span class="n">state_dict</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">output_model_file</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">OpenAIGPTTokenizer</span><span class="p">(</span><span class="n">output_vocab_file</span><span class="p">)</span>
</pre></div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="converting_tensorflow_models.html" class="btn btn-neutral float-right" title="Converting Tensorflow Checkpoints" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="notebooks.html" class="btn btn-neutral" title="Notebooks" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, huggingface

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
        <script type="text/javascript" src="_static/language_data.js"></script>
        <script type="text/javascript" src="_static/js/custom.js"></script>
    

  

  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>