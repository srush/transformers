

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Glossary &mdash; transformers 2.6.0 documentation</title>
  

  
  
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/huggingface.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/code-snippets.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="BERTology" href="bertology.html" />
    <link rel="prev" title="Migrating from pytorch-pretrained-bert" href="migration.html" /> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html" class="icon icon-home"> transformers
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="pretrained_models.html">Pretrained models</a></li>
<li class="toctree-l1"><a class="reference internal" href="usage.html">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_sharing.html">Model Sharing</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks.html">Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="serialization.html">Loading Google AI or OpenAI pre-trained weights or PyTorch dump</a></li>
<li class="toctree-l1"><a class="reference internal" href="serialization.html#serialization-best-practices">Serialization best-practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="converting_tensorflow_models.html">Converting Tensorflow Checkpoints</a></li>
<li class="toctree-l1"><a class="reference internal" href="torchscript.html">Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="multilingual.html">Multi-lingual models</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmarks.html">Benchmarks</a></li>
</ul>
<p class="caption"><span class="caption-text">Core Classes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="main_classes/configuration.html">Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/model.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/tokenizer.html">Tokenizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/pipelines.html">Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/optimizer_schedules.html">Optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/optimizer_schedules.html#schedules">Schedules</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/optimizer_schedules.html#gradient-strategies">Gradient Strategies</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/processors.html">Processors</a></li>
</ul>
<p class="caption"><span class="caption-text">Features</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="migration.html">Migrating from pytorch-pretrained-bert</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Glossary</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#input-ids">Input IDs</a></li>
<li class="toctree-l2"><a class="reference internal" href="#attention-mask">Attention mask</a></li>
<li class="toctree-l2"><a class="reference internal" href="#token-type-ids">Token Type IDs</a></li>
<li class="toctree-l2"><a class="reference internal" href="#position-ids">Position IDs</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="bertology.html">BERTology</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
</ul>
<p class="caption"><span class="caption-text">Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="model_doc/auto.html">AutoModels</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/bert.html">BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/gpt.html">OpenAI GPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/transformerxl.html">Transformer XL</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/gpt2.html">OpenAI GPT2</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/xlm.html">XLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/xlnet.html">XLNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/roberta.html">RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/distilbert.html">DistilBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/ctrl.html">CTRL</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/camembert.html">CamemBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/albert.html">ALBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/xlmroberta.html">XLM-RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/flaubert.html">FlauBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/bart.html">Bart</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/t5.html">T5</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">transformers</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Glossary</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/glossary.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="glossary">
<h1>Glossary<a class="headerlink" href="#glossary" title="Permalink to this headline">¶</a></h1>
<p>Every model is different yet bears similarities with the others. Therefore most models use the same inputs, which are
detailed here alongside usage examples.</p>
<div class="section" id="input-ids">
<h2>Input IDs<a class="headerlink" href="#input-ids" title="Permalink to this headline">¶</a></h2>
<p>The input ids are often the only required parameters to be passed to the model as input. <em>They are token indices,
numerical representations of tokens building the sequences that will be used as input by the model</em>.</p>
<p>Each tokenizer works differently but the underlying mechanism remains the same. Here’s an example using the BERT
tokenizer, which is a <a class="reference external" href="https://arxiv.org/pdf/1609.08144.pdf">WordPiece</a> tokenizer:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="k">import</span> <span class="n">BertTokenizer</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bert-base-cased&quot;</span><span class="p">)</span>

<span class="n">sequence</span> <span class="o">=</span> <span class="s2">&quot;A Titan RTX has 24GB of VRAM&quot;</span>
</pre></div>
</div>
<p>The tokenizer takes care of splitting the sequence into tokens available in the tokenizer vocabulary.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Continuation of the previous script</span>
<span class="n">tokenized_sequence</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">sequence</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">tokenized_sequence</span> <span class="o">==</span> <span class="p">[</span><span class="s1">&#39;A&#39;</span><span class="p">,</span> <span class="s1">&#39;Titan&#39;</span><span class="p">,</span> <span class="s1">&#39;R&#39;</span><span class="p">,</span> <span class="s1">&#39;##T&#39;</span><span class="p">,</span> <span class="s1">&#39;##X&#39;</span><span class="p">,</span> <span class="s1">&#39;has&#39;</span><span class="p">,</span> <span class="s1">&#39;24&#39;</span><span class="p">,</span> <span class="s1">&#39;##GB&#39;</span><span class="p">,</span> <span class="s1">&#39;of&#39;</span><span class="p">,</span> <span class="s1">&#39;V&#39;</span><span class="p">,</span> <span class="s1">&#39;##RA&#39;</span><span class="p">,</span> <span class="s1">&#39;##M&#39;</span><span class="p">]</span>
</pre></div>
</div>
<p>These tokens can then be converted into IDs which are understandable by the model. Several methods are available for
this, the recommended being <cite>encode</cite> or <cite>encode_plus</cite>, which leverage the Rust implementation of
<a class="reference external" href="https://github.com/huggingface/tokenizers">huggingface/tokenizers</a> for peak performance.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Continuation of the previous script</span>
<span class="n">encoded_sequence</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">sequence</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">encoded_sequence</span> <span class="o">==</span> <span class="p">[</span><span class="mi">101</span><span class="p">,</span> <span class="mi">138</span><span class="p">,</span> <span class="mi">18696</span><span class="p">,</span> <span class="mi">155</span><span class="p">,</span> <span class="mi">1942</span><span class="p">,</span> <span class="mi">3190</span><span class="p">,</span> <span class="mi">1144</span><span class="p">,</span> <span class="mi">1572</span><span class="p">,</span> <span class="mi">13745</span><span class="p">,</span> <span class="mi">1104</span><span class="p">,</span> <span class="mi">159</span><span class="p">,</span> <span class="mi">9664</span><span class="p">,</span> <span class="mi">2107</span><span class="p">,</span> <span class="mi">102</span><span class="p">]</span>
</pre></div>
</div>
<p>The <cite>encode</cite> and <cite>encode_plus</cite> methods automatically add “special tokens” which are special IDs the model uses.</p>
</div>
<div class="section" id="attention-mask">
<h2>Attention mask<a class="headerlink" href="#attention-mask" title="Permalink to this headline">¶</a></h2>
<p>The attention mask is an optional argument used when batching sequences together. This argument indicates to the
model which tokens should be attended to, and which should not.</p>
<p>For example, consider these two sequences:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="k">import</span> <span class="n">BertTokenizer</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bert-base-cased&quot;</span><span class="p">)</span>

<span class="n">sequence_a</span> <span class="o">=</span> <span class="s2">&quot;This is a short sequence.&quot;</span>
<span class="n">sequence_b</span> <span class="o">=</span> <span class="s2">&quot;This is a rather long sequence. It is at least longer than the sequence A.&quot;</span>

<span class="n">encoded_sequence_a</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">sequence_a</span><span class="p">)</span>
<span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoded_sequence_a</span><span class="p">)</span> <span class="o">==</span> <span class="mi">8</span>

<span class="n">encoded_sequence_b</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">sequence_b</span><span class="p">)</span>
<span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoded_sequence_b</span><span class="p">)</span> <span class="o">==</span> <span class="mi">19</span>
</pre></div>
</div>
<p>These two sequences have different lengths and therefore can’t be put together in a same tensor as-is. The first
sequence needs to be padded up to the length of the second one, or the second one needs to be truncated down to
the length of the first one.</p>
<p>In the first case, the list of IDs will be extended by the padding indices:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Continuation of the previous script</span>
<span class="n">padded_sequence_a</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">sequence_a</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">19</span><span class="p">,</span> <span class="n">pad_to_max_length</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">assert</span> <span class="n">padded_sequence_a</span> <span class="o">==</span> <span class="p">[</span><span class="mi">101</span><span class="p">,</span> <span class="mi">1188</span><span class="p">,</span> <span class="mi">1110</span><span class="p">,</span> <span class="mi">170</span><span class="p">,</span> <span class="mi">1603</span><span class="p">,</span> <span class="mi">4954</span><span class="p">,</span>  <span class="mi">119</span><span class="p">,</span> <span class="mi">102</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>   <span class="mi">0</span><span class="p">,</span>   <span class="mi">0</span><span class="p">,</span>   <span class="mi">0</span><span class="p">]</span>
<span class="k">assert</span> <span class="n">encoded_sequence_b</span> <span class="o">==</span> <span class="p">[</span><span class="mi">101</span><span class="p">,</span> <span class="mi">1188</span><span class="p">,</span> <span class="mi">1110</span><span class="p">,</span> <span class="mi">170</span><span class="p">,</span> <span class="mi">1897</span><span class="p">,</span> <span class="mi">1263</span><span class="p">,</span> <span class="mi">4954</span><span class="p">,</span> <span class="mi">119</span><span class="p">,</span> <span class="mi">1135</span><span class="p">,</span> <span class="mi">1110</span><span class="p">,</span> <span class="mi">1120</span><span class="p">,</span> <span class="mi">1655</span><span class="p">,</span> <span class="mi">2039</span><span class="p">,</span> <span class="mi">1190</span><span class="p">,</span> <span class="mi">1103</span><span class="p">,</span> <span class="mi">4954</span><span class="p">,</span> <span class="mi">138</span><span class="p">,</span> <span class="mi">119</span><span class="p">,</span> <span class="mi">102</span><span class="p">]</span>
</pre></div>
</div>
<p>These can then be converted into a tensor in PyTorch or TensorFlow. The attention mask is a binary tensor indicating
the position of the padded indices so that the model does not attend to them. For the
<a class="reference internal" href="model_doc/bert.html#transformers.BertTokenizer" title="transformers.BertTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertTokenizer</span></code></a>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">1</span></code> indicate a value that should be attended to while <code class="xref py py-obj docutils literal notranslate"><span class="pre">0</span></code> indicate
a padded value.</p>
<p>The method <a class="reference internal" href="main_classes/tokenizer.html#transformers.PreTrainedTokenizer.encode_plus" title="transformers.PreTrainedTokenizer.encode_plus"><code class="xref py py-func docutils literal notranslate"><span class="pre">encode_plus()</span></code></a> may be used to obtain the attention mask directly:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Continuation of the previous script</span>
<span class="n">sequence_a_dict</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode_plus</span><span class="p">(</span><span class="n">sequence_a</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">19</span><span class="p">,</span> <span class="n">pad_to_max_length</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">assert</span> <span class="n">sequence_a_dict</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="p">[</span><span class="mi">101</span><span class="p">,</span> <span class="mi">1188</span><span class="p">,</span> <span class="mi">1110</span><span class="p">,</span> <span class="mi">170</span><span class="p">,</span> <span class="mi">1603</span><span class="p">,</span> <span class="mi">4954</span><span class="p">,</span> <span class="mi">119</span><span class="p">,</span> <span class="mi">102</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="k">assert</span> <span class="n">sequence_a_dict</span><span class="p">[</span><span class="s1">&#39;attention_mask&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="section" id="token-type-ids">
<h2>Token Type IDs<a class="headerlink" href="#token-type-ids" title="Permalink to this headline">¶</a></h2>
<p>Some models’ purpose is to do sequence classification or question answering. These require two different sequences to
be encoded in the same input IDs. They are usually separated by special tokens, such as the classifier and separator
tokens. For example, the BERT model builds its two sequence input as such:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="k">import</span> <span class="n">BertTokenizer</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bert-base-cased&quot;</span><span class="p">)</span>

<span class="c1"># [CLS] SEQ_A [SEP] SEQ_B [SEP]</span>

<span class="n">sequence_a</span> <span class="o">=</span> <span class="s2">&quot;HuggingFace is based in NYC&quot;</span>
<span class="n">sequence_b</span> <span class="o">=</span> <span class="s2">&quot;Where is HuggingFace based?&quot;</span>

<span class="n">encoded_sequence</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">sequence_a</span><span class="p">,</span> <span class="n">sequence_b</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">encoded_sequence</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;[CLS] HuggingFace is based in NYC [SEP] Where is HuggingFace based? [SEP]&quot;</span>
</pre></div>
</div>
<p>This is enough for some models to understand where one sequence ends and where another begins. However, other models
such as BERT have an additional mechanism, which are the segment IDs. The Token Type IDs are a binary mask identifying
the different sequences in the model.</p>
<p>We can leverage <a class="reference internal" href="main_classes/tokenizer.html#transformers.PreTrainedTokenizer.encode_plus" title="transformers.PreTrainedTokenizer.encode_plus"><code class="xref py py-func docutils literal notranslate"><span class="pre">encode_plus()</span></code></a> to output the Token Type IDs for us:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Continuation of the previous script</span>
<span class="n">encoded_dict</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode_plus</span><span class="p">(</span><span class="n">sequence_a</span><span class="p">,</span> <span class="n">sequence_b</span><span class="p">)</span>

<span class="k">assert</span> <span class="n">encoded_dict</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="p">[</span><span class="mi">101</span><span class="p">,</span> <span class="mi">20164</span><span class="p">,</span> <span class="mi">10932</span><span class="p">,</span> <span class="mi">2271</span><span class="p">,</span> <span class="mi">7954</span><span class="p">,</span> <span class="mi">1110</span><span class="p">,</span> <span class="mi">1359</span><span class="p">,</span> <span class="mi">1107</span><span class="p">,</span> <span class="mi">17520</span><span class="p">,</span> <span class="mi">102</span><span class="p">,</span> <span class="mi">2777</span><span class="p">,</span> <span class="mi">1110</span><span class="p">,</span> <span class="mi">20164</span><span class="p">,</span> <span class="mi">10932</span><span class="p">,</span> <span class="mi">2271</span><span class="p">,</span> <span class="mi">7954</span><span class="p">,</span> <span class="mi">1359</span><span class="p">,</span> <span class="mi">136</span><span class="p">,</span> <span class="mi">102</span><span class="p">]</span>
<span class="k">assert</span> <span class="n">encoded_dict</span><span class="p">[</span><span class="s1">&#39;token_type_ids&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
<p>The first sequence, the “context” used for the question, has all its tokens represented by <code class="xref py py-obj docutils literal notranslate"><span class="pre">0</span></code>, whereas the
question has all its tokens represented by <code class="xref py py-obj docutils literal notranslate"><span class="pre">1</span></code>. Some models, like <a class="reference internal" href="model_doc/xlnet.html#transformers.XLNetModel" title="transformers.XLNetModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetModel</span></code></a> use an
additional token represented by a <code class="xref py py-obj docutils literal notranslate"><span class="pre">2</span></code>.</p>
</div>
<div class="section" id="position-ids">
<h2>Position IDs<a class="headerlink" href="#position-ids" title="Permalink to this headline">¶</a></h2>
<p>The position IDs are used by the model to identify which token is at which position. Contrary to RNNs that have the
position of each token embedded within them, transformers are unaware of the position of each token. The position
IDs are created for this purpose.</p>
<p>They are an optional parameter. If no position IDs are passed to the model, they are automatically created as absolute
positional embeddings.</p>
<p>Absolute positional embeddings are selected in the range <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">config.max_position_embeddings</span> <span class="pre">-</span> <span class="pre">1]</span></code>. Some models
use other types of positional embeddings, such as sinusoidal position embeddings or relative position embeddings.</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="bertology.html" class="btn btn-neutral float-right" title="BERTology" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="migration.html" class="btn btn-neutral" title="Migrating from pytorch-pretrained-bert" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, huggingface

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
        <script type="text/javascript" src="_static/language_data.js"></script>
        <script type="text/javascript" src="_static/js/custom.js"></script>
    

  

  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>