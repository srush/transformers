{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart\n",
    "\n",
    "The Transformer that has led to a renaissance in tools for natural\n",
    "language processing acheiving state-of-the-art results across almost\n",
    "all benchmarks.  The mission of `transformers` is to make it easy for\n",
    "anyone _to access, extend, and apply_ these models for their own tasks.\n",
    "\n",
    "This guide walks you through two broad categories of tasks.\n",
    "\n",
    "* *Text Understanding*, e.g. [sentiment classification](), [question answering](), [sentence labeling](), and [information extraction]().\n",
    "* *Text Generation*, e.g. [conditional generation](), [text summarization](), and [machine translation](). \n",
    "\n",
    "\n",
    "The library is written such a user does not need to be aware of the internals of the model. However,\n",
    "you do need to be aware of three key objects: \n",
    "\n",
    "* *Tokenizer*; Maps a sequence of words into a sequence of indices. \n",
    "* *Model*; Maps a sequence of indices into a sequence of feature embeddings. \n",
    "* *Head*; Maps a sequence of feature embeddings to a prediction.\n",
    "\n",
    "\n",
    "You can pair a most Model's with a task-specific Head. The Model is\n",
    " quite large and _pre-trained_ on a massive volume of text; whereas\n",
    " the Head is small and can be _fine-tuned_ on a tiny annotated\n",
    " dataset.\n",
    "\n",
    "For the quickstart, we will focus on using `transformers` with community [shared\n",
    "models](www.huggingface.co/models). For mode details on training,\n",
    "see [training]().\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Text Understanding\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<!-- Let's start by preparing a tokenized input (a list of token\n",
    "embeddings indices to be fed to Bert) from a text string using\n",
    "`BertTokenizer` -->\n",
    "\n",
    "This section demonstrates how to use  transformers for text\n",
    "understanding. We focus on sequence classification, i.e.\n",
    "predicting a class label based on a sentence. Specifically we try to\n",
    "predict if a movie review has negative or positive sentiment with a\n",
    "value between 1 and 5.\n",
    "\n",
    "We begin by specifying a specific model. We use\n",
    "a classic model in this family, known as [Bert]().\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Can use any model on huggingface.co. This model has been trained for sentiment.\n",
    "model_name = 'nlptown/bert-base-multilingual-uncased-sentiment'\n",
    "\n",
    "# Download pre-trained Tokenizer, Model, and Head.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Set for evaluation on GPU.\n",
    "model.eval().cuda()\n",
    "```\n",
    "\n",
    "Next, we utilize the tokenizer to convert an input sentence to a format that\n",
    "the model can use.\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "# For this model, text format start with CLS and splits sentences with [SEP]\n",
    "text = \"[CLS] Muppets Take Manhattan is a great movie . [SEP]\"\n",
    "\n",
    "# Tokenize input\n",
    "indexed_tokens = tokenizer.encode(text)\n",
    "\n",
    "# Convert to PyTorch tensor format. \n",
    "tokens_tensor = torch.tensor([indexed_tokens]).cuda()\n",
    "```\n",
    "\n",
    "Finally, we pass this tensor to our model and mead to predict its output.\n",
    "\n",
    "```\n",
    "# Predict the sentiment (1-5) for the input sentence. \n",
    "model_output = model(tokens_tensor)[0]\n",
    "print(model_output.argmax(-1)+1)\n",
    "```\n",
    "\n",
    "That's it. In 10 lines, we have quite powerful model for this task.\n",
    "Similar methods can be used for a range of other text understanding\n",
    "tasks. These include: \n",
    "\n",
    "* AutoModelForTokenClassification\n",
    "* AutoModelForSequenceClassification\n",
    "* AutoModelForQuestionAnswering\n",
    "* AutoModelForTokenClassification\n",
    "\n",
    "If you want access to the raw transformer outputs, you can also utilize.\n",
    "\n",
    "* AutoModel\n",
    "\n",
    "For more advanced usage skip ahead to\n",
    "\n",
    "* Tokenization\n",
    "* Training\n",
    "* Deployment\n",
    "* Model Sharing\n",
    "\n",
    "\n",
    "## Text Generation\n",
    "\n",
    "This section demonstrates how to use transformers for text\n",
    "generation. We focus on langauge modeling, i.e.  predicting the next\n",
    "word in a sequence based on the previous words. Specifically we try to\n",
    "complete a sentence given only its beginning.  We use\n",
    "a large model of this class, known as [GPT-2]().\n",
    "\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForLMHeadModel\n",
    "\n",
    "# Can use any model on huggingface.co. This model has been trained for language modeling.\n",
    "model_name = 'gpt2'\n",
    "\n",
    "# Download pre-trained Tokenizer, Model, and Head.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForLMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Set for evaluation on GPU.\n",
    "model.eval().cuda()\n",
    "```\n",
    "\n",
    "Next, we utilize the tokenizer to convert an input sentence to a format that\n",
    "the model can use.\n",
    "\n",
    "\n",
    "```python\n",
    "# Encode a text inputs\n",
    "text = \"Who was Jim Henson ? Jim Henson was a\"\n",
    "indexed_tokens = tokenizer.encode(text)\n",
    "\n",
    "# Convert indexed tokens in a PyTorch tensor\n",
    "tokens_tensor = torch.tensor([indexed_tokens]).cuda()\n",
    "```\n",
    "\n",
    "Finally, we pass this to our model and decoder it to the next word.\n",
    "\n",
    "```python\n",
    "\n",
    "# Run the model.\n",
    "outputs = model(tokens_tensor)[0]\n",
    "\n",
    "# Convert to a word. \n",
    "predicted_index = torch.argmax(predictions[0, -1, :]).item()\n",
    "predicted_text = tokenizer.decode(indexed_tokens + [predicted_index])\n",
    "assert predicted_text == 'Who was Jim Henson? Jim Henson was a man'\n",
    "```\n",
    "\n",
    "If we want to generate a sequence of output words we can continue running\n",
    "in a loop.\n",
    "\n",
    "```\n",
    "generated = tokenizer.encode(text)\n",
    "context = torch.tensor([generated])\n",
    "past = None\n",
    "\n",
    "for i in range(100):\n",
    "    output, past = model(context, past=past)\n",
    "    token = torch.argmax(output[..., -1, :])\n",
    "\n",
    "    generated += [token.tolist()]\n",
    "    context = token.unsqueeze(0)\n",
    "\n",
    "sequence = tokenizer.decode(generated)\n",
    "```\n",
    "\n",
    "This is the basic form of text generation but similar models and head\n",
    "can be used for a range of other text understanding tasks. These\n",
    "include:\n",
    "\n",
    "* AutoModelForMaskedLM\n",
    "* AutoModelForLMHeadModel\n",
    "\n",
    "For more advanced usage skip ahead to\n",
    "\n",
    "* Tokenization\n",
    "* Training\n",
    "* Deployment\n",
    "* Model Sharing\n",
    "* Sequential generation\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
